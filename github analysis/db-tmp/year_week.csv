,id,url,username,repo,path,content,year,week
1,1,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week1/R/tw1_plot.R,"library(here)
library(readxl)
library(tidyverse)
library(glue)
library(ggrepel)

tidy_data <- dir(here(""week1"", ""data""), full.names = TRUE, pattern = ""us_avg"") %>%
  read_excel() %>%
  gather(year, avg_tuition, -State) %>%
  rename(state = State)


nat_avg <- tidy_data %>%
  filter(year %in% c(""2005-06"", ""2015-16"")) %>%
  group_by(year) %>%
  summarize(avg_tuition = mean(avg_tuition)) %>%
  mutate(state = ""National Average"")


plot_data <- tidy_data %>%
  filter(year %in% c(""2005-06"", ""2015-16"")) %>%
  left_join(select(nat_avg, year, nat_avg = avg_tuition), by = ""year"") %>%
  bind_rows(nat_avg)

labels <- plot_data %>%
  group_by(state) %>%
  filter(all(avg_tuition > nat_avg)) %>%
  pull(state) %>%
  unique()

plot <- plot_data %>%
  ggplot(., aes(x = year, y = avg_tuition, group = state)) +
  geom_text_repel(data = filter(plot_data, state %in% labels, year == ""2015-16""), aes(label = state), direction = ""y"", nudge_x = 0.1, segment.size = 0.1, hjust = 0, family = ""Oxygen"", size = 3) +
  geom_path(color = ""grey50"", size = 0.5, alpha = 0.5) +
  geom_point(color = ""grey50"") +
  geom_path(data = nat_avg, color = ""red"", size = 1) +
  geom_point(data = nat_avg, color = ""red"") +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = NULL, y = NULL, title = ""Comparison of the average US tuition growth between 2005 and 2015"", subtitle = ""Eastern and Northeastern students consistently face tutition above the national average, indicated by the red line."", caption = ""\nData: http://trends.collegeboard.org/ | Graphic: @jakekaupp"") +
  theme_minimal(base_family = ""Oswald Light"") +
  theme(panel.grid.minor = element_blank())

ggsave(plot, filename = glue('{here(""week1"")}/tidyweek-{Sys.Date()}.png'), height = 8, width = 6, dpi = 300)

",2018,1
2,2,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week11/R/tw11_plot.R,"library(here)
library(tidyverse)
library(treemap)
library(sysfonts)
library(showtext)
library(grid)
library(nord)

raw_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week11_fifa_audience.csv"") %>% 
  select(-X1)

font_add_google(""Oswald"",""Oswald-Light"", regular.wt = 300)
font_add_google(""Scope One"",""Scope One"")

showtext_auto()

vplayout <- function(x, y) viewport(width=11/3, height=8.5, layout.pos.row = x, layout.pos.col = y)

build_treemap <- function(x, y, size)  {
  
  title <- set_names(c(""Population Share"", ""TV Audience Share"", ""GDP Weighted Share""), c(""population_share"",""tv_audience_share"", ""gdp_weighted_share""))
  
  treemap(raw_data,
          index = c(""confederation"",""country""),
          vSize = size,
          vColor = ""confederation"",
          type = ""categorical"",
          title = title[size],
          title.legend = """",
          fontfamily.title = ""Oswald-Light"",
          fontsize.labels = c(20, 10),
          fontfamily.labels = ""Oswald-Light"",
          fontcolor.labels = ""#f0f0f0"",
          lowerbound.cex.labels = 1,
          bg.labels = 0,
          inflate.labels = FALSE,
          border.col = ""white"",
          border.lwds = 1,
          position.legend = ""none"",
          palette = nord(""baie_mouton""),
          align.labels = list(c(""left"",""top""), c(""right"",""bottom"")),
          drop.unused.levels = TRUE,
          vp = vplayout(x,y))
  
  
  
}

fifa_maps <- function() {
  grid.newpage()
  pushViewport(viewport(layout = grid.layout(3, 3, heights = c(0.1, 0.8, 0.1))))
  par(mai=c(0,0,0,0))
  
  grid.text(""Comparing FIFA Share Differences by Confederation and Country"", x = 0.1, hjust = 0, vp = vplayout(1,1), gp = gpar(fontfamily = ""Oswald-Light"", fontsize = 30))
  build_treemap(2, 1, ""population_share"")
  build_treemap(2, 2, ""tv_audience_share"")
  build_treemap(2, 3, ""gdp_weighted_share"")
  grid.text(""Data: fivethirtyeight.com | Graphic: @jakekaupp"", x = 0.5, vp = vplayout(3,3), gp = gpar(fontfamily = ""Scope One"", fontsize = 10))
  
}

png(here(""week11"", ""Fifa Treemaps.png""), width = 11, height=8.5, units = ""in"", res = 100)
fifa_maps()
dev.off()

",2018,11
3,3,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week12/R/tw12_plot.R,"library(tidyverse)
library(lubridate)
library(jkmisc)
library(ggridges)
library(nord)
library(here)
library(showtext)

font_add_google(""Oswald"", ""Oswald"", regular.wt = 400)
font_add_google(""Scope One"", ""Scope One"")

trend_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_google_trends.csv"", skip = 2, col_names = TRUE) %>% 
  set_names(str_extract(names(.), ""(?<=Hurricane )(\\w+)|(Day)"")) %>% 
  rename(Date = Day) %>% 
  mutate(source = ""Google Trends"") %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

mediacloud_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_mediacloud_hurricanes.csv"", col_names = TRUE) %>% 
  mutate(source = ""Online News"") %>% 
  mutate(Date = mdy(Date)) %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

tv_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_tv_hurricanes.csv"") %>% 
  mutate(source = ""TV"") %>% 
  mutate(Date = mdy(Date)) %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

all_data <- bind_rows(trend_data, mediacloud_data, tv_data) %>%
  gather(hurricane, value, -Date, - source) %>% 
  set_names(tolower(names(.)))

showtext_auto()

plot <- ggplot(all_data, aes(x = date, y = source)) +
  geom_ridgeline(aes(height = value, fill = factor(hurricane)), size = 0.1, scale = 0.8, alpha = 0.8) +
  labs(title = ""On nearly every form of media, hurricanes that hit mainland US received more sustained coverage than Maria in Puerto Rico"",
       subtitle = ""Ridgeline plots of normalized media shares (TV, Online News and Google Trends)"",
       caption = ""Data: fivethirtyeight | Graphic: @jakekaupp"",
       y = NULL,
       x = NULL) +
  scale_x_date(expand = c(0,0)) +
  scale_fill_nord(name = ""Hurricane"", palette = ""lumina"") +
  theme_jk(plot_title_size = 28, subtitle_size = 24, base_size = 20, caption_size = 12,  grid = ""X"") +
  theme(axis.text.y = element_text(vjust = -2))

ggsave(plot, filename = here(""week12"", ""ROCK YOU LIKE A HURRICANE.png""), width = 6, height = 3)
",2018,12
4,4,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week18/R/tw18_plot.R,"library(here)
library(tidyverse)
library(treemap)
library(sysfonts)
library(showtext)
library(grid)
library(nord)
library(readxl)
library(ggalt)
library(jkmisc)


raw_data <- read_xlsx(here(""week18"", ""data"", ""week18_dallas_animals.xlsx""), sheet = 1)

data <- raw_data %>% 
  filter(animal_type %in% c(""CAT"",""DOG""), mo_year == ""2017"") %>% 
  count(animal_type, month, mo_year, outcome_type) %>% 
  group_by(animal_type, month, mo_year) %>% 
  mutate(percent = n/sum(n)) %>% 
  ungroup() %>% 
  select(-n) %>% 
  filter(outcome_type %in% c(""EUTHANIZED"",""ADOPTION"")) %>% 
  mutate_if(is.character, tolower) %>%
  spread(outcome_type, percent) %>% 
  mutate_if(is.character, tools::toTitleCase) %>% 
  mutate(month = ifelse(month == ""may"", ""May"", month)) %>% 
  arrange(month) %>% 
  complete(month = month.abb, mo_year, animal_type, fill = list(adoption = NA, euthanized = NA)) %>% 
  mutate(ratio = adoption/euthanized) %>% 
  mutate(month = factor(month, month.abb))



ggplot(data, aes(x = month, y = ratio, group = animal_type, color = animal_type)) +
  geom_hline(yintercept = 1.0, size = 0.1, color = ""firebrick"", linetype = ""dashed"") +
  geom_line(size = 0.5) +
  geom_text(data = filter(data, month == ""Sep""), aes(label = animal_type), nudge_x = 0.3, family = ""Oswald"") +
  theme_jk(grid = ""XY"") +
  scale_color_nord(""victory_bonds"") +
  labs(x = NULL, y = ""Ratio of Adopted/Euthanized"", title = ""2017 was a bad time to be a cat in a shelter"", subtitle = ""Cats in the shelters were euthanized more than adopted compared to dogs."") +
  theme(legend.position = ""none"")
  



",2018,18
5,5,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week2/R/tw2_plot.R,"library(here)
library(readxl)
library(tidyverse)
library(glue)
library(janitor)
library(rvest)
library(nord)
library(jkmisc)
library(viridis)

# Function to scrape the top avg cap salary by player ----
pull_position_data <- function(year, position) {
  
  Sys.sleep(5)
  
  url <- glue(""http://www.spotrac.com/nfl/positional/{year}/{position}"")
  
  read_html(url) %>% 
    html_nodes(""#main > div.teams > table:nth-child(6)"") %>% 
    html_table() %>%
    flatten_df() %>% 
    set_names(c(""rank"",""player"",""cap_dollars"", ""cap_percent""))
}


# Formatter for 538 year labels 
labels_538 <- function(labels) {
  labels_out <- sprintf(""20%s"", str_sub(labels, 3, 4))
  labels_out <- c(labels_out[1], glue(""'{str_sub(labels_out[-1], 3, 4)}""))
  return(labels_out)
}

# Create the data scaffold 
years <- 2011:2018
positions <- c(""quarterback"", ""running-back"", ""fullback"", ""guard"", ""center"", ""left-tackle"", ""right-tackle"", ""tight-end"",""wide-receiver"",""defensive-end"",""cornerback"",""defensive-tackle"", ""inside-linebacker"", ""outside-linebacker"", ""free-safety"", ""strong-safety"", ""kicker"",""punter"",""long-snapper"")

scaffold <- tibble(year = years,
                   position = list(positions)) %>% 
  unnest() 

# Populate the scaffold
if(!file.exists(here(""week2"", ""data"", ""position_cap_data_named.RDS""))) {
  
  scaffold <- scaffold %>% 
    mutate(data = map2(year, position, ~pull_position_data(.x, .y))) %>% 
    unnest() %>% 
    mutate_at(c(""cap_dollars"", ""cap_percent""), parse_number) %>% 
    mutate(side = case_when(position %in% c(""quarterback"", ""running-back"", ""fullback"", ""guard"", ""center"", ""left-tackle"", ""right-tackle"", ""tight-end"",""wide-receiver"") ~ ""Offense"",
                            position %in% c(""kicker"",""punter"",""long-snapper"") ~ ""Special Teams"",
                            TRUE ~ ""Defense""))
  
  
  # Save it to avoid re-scraping 
  saveRDS(scaffold, file = here(""week2"", ""data"", ""position_cap_data_named.RDS""))
} else {
  
  scaffold <- readRDS(here(""week2"", ""data"", ""position_cap_data_named.RDS""))
  
}


# Make data for the plot
plot_data <- scaffold %>% 
  group_by(year, position, side) %>% 
  top_n(16, cap_dollars) %>% 
  summarize(avg_pay = mean(cap_dollars))
  
# Make a heatmap!
ggplot(plot_data, aes(x = year, y = position, fill = avg_pay)) +
  geom_tile(color = ""white"", size = 0.1) +
  coord_equal() +
  labs(x = NULL, y = NULL, title = ""The Fullback Gets No Respect"", subtitle = ""Average cap value of the 16 highest payed players in all positions"", caption = ""Data: http://www.spotrac.com/ | Graphic: @jakekaupp"") +
  scale_x_continuous(labels = labels_538, breaks = 2011:2018) +
  scale_y_discrete(labels = function(x) str_to_title(gsub(""[[:punct:]]"", "" "", x))) +
  scale_fill_viridis(discrete = FALSE, labels = scales::dollar, name = ""Average Salary"") +
  theme_jk(grid = FALSE, base_size = 14)

ggsave(here(""week2"", ""tw2_heatmap.png""), width = 8, height = 8)
",2018,2
6,6,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week3/R/tw3_plot.R,"library(tidyverse)
library(here)
library(readxl)
library(jkmisc)

# Read in the data
mortality_data <- dir(here(""week3"",""data""), pattern = ""global"", full.names = TRUE) %>% 
  read_excel()


# Tidy up the data
tidy_mort <- mortality_data %>% 
  gather(cause_of_death, percentage, -country:-year) %>% 
  mutate(cause_of_death = trimws(str_remove(cause_of_death, ""\\(\\%\\)""))) %>% 
  mutate(percentage = ifelse(is.na(percentage), NA, percentage/100))

# Get just the data pertaining to suicides
suicide_data <- tidy_mort %>% 
  filter(cause_of_death == ""Suicide"", !is.na(country_code))

# Get the World percentage
global_rate <- suicide_data %>% 
  filter(country == ""World"") %>% 
  select(year, percentage)

# Get the top 40 problem countries, those with the suicide rate constantly over the world average (note the all statement in the filter)
problem_countries <- suicide_data %>% 
  filter(country != ""World"") %>% 
  left_join(global_rate, by = ""year"") %>% 
  group_by(country) %>% 
  filter(all(percentage.x > percentage.y)) %>% 
  summarize(percentage = mean(percentage.x, na.rm = TRUE)) %>% 
  top_n(40, percentage) %>% 
  arrange(desc(percentage)) %>% 
  pull(country)

# Create the data to make the plot, and arrange descending by the overall avg rate of suicide
plot_data <- suicide_data %>% 
  filter(country %in% problem_countries) %>% 
  mutate(country = factor(country, problem_countries))

# Create the sad plot
sad_plot <- ggplot(plot_data, aes(x = year, y = percentage)) +
  geom_segment(aes(x = min(year), xend = max(year), y = 0, yend = 0)) +
  geom_area(fill = ""steelblue4"") +
  geom_path(color = ""grey30"", size = 0.2) +
  geom_area(data = global_rate, fill = ""steelblue3"") +
  geom_path(data = global_rate, color = ""grey30"", size = 0.2) +
  facet_wrap(~country, nrow = 5) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = ""Countries Coping With the Tradgedy and Pain of Suicide"",
       subtitle = ""Dark blue indicates suicide rate by year, Light blue fill indicates the global average suicide rate by year."",
       x = NULL,
       y = NULL,
       caption = ""Data: ourworldindata.org | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Y"")

# Save the sad plot
ggsave(here(""week3"",""tw3_sad_plot.png""), sad_plot, width = 16, height = 10)
",2018,3
7,7,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week4/R/tw4_plot.R,"library(tidyverse)
library(here)
library(jkmisc)
library(scales)
library(ggiraph)
library(glue)
library(waffle)

make_tooltip <- function(occupation, female, male, income_gap, ...) {
 
  glue('<div class=""tipchart"">
      <h3>{occupation}</h3>
      <h4>Mens taxable income {ifelse(income_gap <= 1, percent(1-round(income_gap, 2)), percent(round(income_gap, 2)))} {ifelse(income_gap <= 1, ""less"", ""more"")} than womens</h4>
      <table>
      <tr class=""tiprow"">
      <td class=""tipheader"">Average Male Taxable Income:</td>
      <td class=""tiptext"">{dollar(male)}</td>
      </tr>
      <tr class=""tiprow"">
      <td class=""tipheader"">Average Female Taxable Income:</td>
      <td class=""tiptext"">{dollar(female)}</td>
      </tr>
      </table>
      </div>')
  
}


# Read in the data
income_data <- dir(here(""week4"",""data""), pattern = ""salary"", full.names = TRUE) %>% 
  read_csv(locale = locale(""en""))
 
# Clean occupation up a bit.  Some rouge unicodes in there.
tidy_gap <- income_data %>% 
  mutate(occupation = iconv(occupation, ""UTF-8"", ""UTF-8"",sub='')) %>% 
  spread(gender, average_taxable_income) %>%
  set_names(tolower(names(.))) %>% 
  group_by(occupation) %>% 
  summarize_at(c(""female"", ""male""), sum, na.rm = TRUE) %>% 
  filter(female != 0, male != 0) %>% 
  mutate(income_gap = male/female)

plot_data <- tidy_gap %>% 
 mutate(fill = if_else(income_gap >= 1, ""grey80"", ""#ffd700""),
         alpha = if_else(income_gap >= 1, 0.2, 1)) %>% 
  mutate(tooltip = pmap(., make_tooltip)) %>% 
  mutate(tooltip = gsub(""\\\n"", """", tooltip)) %>% 
  mutate(tooltip = gsub(""'"", """", tooltip)) %>% 
  mutate(idx = row_number())

tooltip_css <- ""background-color:white;padding:10px;border-radius:20px 20px 20px 20px;border-color:black;border-style:solid;border-width:1px""

plot <- ggplot(plot_data, aes(x = female, y = male, fill = fill)) +
  geom_segment(x = 0, xend = 600000, y = 0, yend = 600000, size = 0.05, color = ""grey80"") +
  geom_point_interactive(aes(alpha = alpha, tooltip = tooltip, data_id = idx), shape = 21, color = ""grey30"", size = 3) +
  scale_y_continuous(labels = dollar, limits = c(0, 600000)) +
  scale_x_continuous(labels = dollar, limits = c(0, 600000)) +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL, 
       title = ""Gender Differences in Taxble Income in Australia"",
       subtitle = str_wrap(""Average male taxable income plotted against average female taxable income by occupation. Yellow dots indicate occupations where women have more taxable income than their male counterparts, 
       line indicates income equality. Hover over points for occupation, % difference and detailed income."", 100),
       caption = ""Data: data.gov.au | Graphic: @jakekaupp"") +
  theme_jk()

ggiraph(ggobj = plot, width_svg = 9, width = 1, tooltip_extra_css = tooltip_css)

waffle_data <- tidy_gap %>% 
  ungroup() %>% 
  mutate(category = case_when(income_gap > 1 ~ ""Men have more income"",
                              income_gap < 1 ~ ""Women have more income""))%>% 
  count(category) %>% 
  pull(n) %>% 
  set_names(., c(""Men have more income"", ""Women have more income""))

waffle(waffle_data, 
       rows = 14,
       size = 1,
       colors = c(""dodgerblue3"", ""deeppink""), 
       legend_pos = ""bottom"", 
       title = ""Out of 1092 occupations on record, men have more taxable income than women in 1011 of them.  That's 92.5% of occupations for those counting at home."") + 
  theme_jk() +
  labs(caption = ""Data: data.gov.au | Graphic: @jakekaupp"") +
  theme(axis.text = element_blank(),
        legend.position = ""bottom"")
",2018,4
8,8,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week5/R/tw_5plot.R,"library(tidyverse)
library(here)
library(janitor)
library(albersusa)
library(sf)
library(sp)
library(rgeos)
library(maptools)
library(ggthemes)
library(viridis)
library(scales)
library(glue)
library(jkmisc)


census_data <- dir(here(""week5"", ""data""), full.names = TRUE) %>% 
  read_csv() %>% 
  clean_names()

# Lets look at commuting!
commuting_data <- census_data %>% 
  select(census_id, state, county, total_pop, drive:mean_commute)

# Despacito is 3:47 in length
despacito_length <- 3 + 47/60

# Mapping things
us <- counties_composite()
us_map <- fortify(us, region=""fips"") 

# Calculate the how many despacitos the average commute has
despacito_commute <- commuting_data %>% 
  mutate(despacitos = mean_commute/despacito_length,
         id = ifelse(str_length(as.character(census_id)) < 5, glue(""0{census_id}""), as.character(census_id))) %>% 
  right_join(us_map)


# Make the map!
map <- ggplot() +
 geom_map(data = us_map, map = us_map,
                    aes(x = long, y = lat, map_id = id),
                    color =""#2b2b2b"", size = 0.05, fill = NA) +
  geom_map(data = despacito_commute, map = us_map,
           aes(x = long, y = lat, map_id = id, fill = despacitos),
           color =""#2b2b2b"", size = 0.05) + 
  scale_fill_viridis(name=""How many despactios?"", na.value = viridis(5, option = ""cividis"")[3], option = 'cividis', breaks = seq(1,12,2)) +
  labs(title = ""Just how much do you like your commute?"",
       subtitle = str_wrap(""What if your commute was defined by hearing a song on repeat?  
                           What if that song was the most streamed song on the planet, Despacito? 
                           Illustrated below is the average number of times you'd hear it on your way home across the US."", 80),
       caption = ""Data: census.gov | Graphic: @jakekaupp"") +
  coord_map() +
  theme_map(base_family=""Scope One"", 
            base_size = 16) +
  theme(legend.title = element_text(size = 10),
        plot.title = element_text(family = ""Oswald""),
        legend.background = element_rect(fill = NA),
        plot.caption = element_text(size = 10),
        legend.position = c(0.9,0.1))

ggsave(here(""week5"", ""tw5_choropleth map.png""), width = 10, height = 6)


",2018,5
9,9,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week6/R/tw_6plot.R,"library(tidyverse)
library(readxl)
library(here)
library(janitor)
library(glue)
library(fuzzyjoin)
library(stringi)
library(ggalt)
library(jkmisc)
library(nord)



provinces <- set_names(c(""Alberta"", ""British Columbia"", ""Manitoba"", ""New Brunswick"", ""Newfoundland and Labrador"",
                         ""Nova Scotia"", ""Northwest Territories"", ""Nunavut"", ""Ontario"", ""Prince Edward Island"", ""Quebec"",
                         ""Saskatchewan"", ""Yukon""),
                       c(""AB"", ""BC"", ""MB"", ""NB"", ""NL"", ""NS"", ""NT"", ""NU"", ""ON"", ""PE"", ""QC"", ""SK"", ""YT""))

# Just get the Tims data just for Canada
tim_hortons <- dir(here(""week6"", ""data""), full.names = TRUE, pattern = ""xlsx"") %>% 
  read_excel(sheet = ""timhorton"") %>% 
  filter(country == ""ca"") %>% 
  rename(province = state) 

# Counts at the City/Province level
tims_city_prov <- tim_hortons %>% 
  count(city, province)

# Counts at the National level
tims_national <- tim_hortons %>% 
  count(province) %>% 
  mutate(color = ifelse(province == ""ON"", nord(""victory_bonds"", 1), ""grey50""))

national <- ggplot(tims_national, aes(x = reorder(province,n), y = n)) +
  geom_lollipop(aes(color = color)) +
  scale_color_identity() +
  scale_y_continuous(expand = c(0.01,0.05),  breaks = scales::pretty_breaks()) +
  scale_x_discrete(labels = function(x) provinces[x]) +
  coord_flip() +
  labs(x = NULL, y = NULL, title = ""Ontario, we have a problem...."", subtitle = ""The highest number of Tim Hortons per province goes to Ontario, a land where you can't even get an oat cake."",
       caption = ""\nData: timhortons.com | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Xx"")

ggsave(plot = national, here(""week6"", ""National Tims.png""), width = 10, height = 6)

census_2011 <- dir(here(""week6"", ""data""), full.names = TRUE, pattern = ""2011 census"") %>% 
  read_csv() %>% 
  clean_names() %>% 
  remove_empty(""rows"") %>% 
  select(city = geographic_name, population = population_2011) %>% 
  mutate(province = stri_extract_last_regex(city, ""\\(([A-Za-z\\.]+?)\\)""),
         city = stri_replace_all_regex(city, ""\\((.*?)\\)"", """"),
         province = gsub(""[[:punct:]]"", """", province)) %>% 
  mutate(province = case_when(province == ""Que"" ~ ""QC"",
                              province == ""Ont"" ~ ""ON"",
                              province == ""Man"" ~ ""MB"",
                              province == ""Sask"" ~ ""SK"",
                              province == ""Alta"" ~ ""AB"",
                              province == ""NWT"" ~ ""NT"",
                              province == ""Nvt"" ~ ""NU"",
                              province == ""PEI"" ~ ""PE"",
                              TRUE ~ province)) %>% 
  mutate_if(is.character, trimws)



tims_density <- regex_right_join(census_2011, tims_city_prov,  by = c(""city"", ""province""), ignore_case = TRUE) 


plot_data <- tims_density %>% 
  select(population, city = city.y, province = province.x, n) %>% 
  group_by(city, province) %>% 
  summarize_at(c(""n"", ""population""), sum, na.rm = TRUE) %>% 
  ungroup() %>% 
  filter(population != 0, n > 1, population > 10000) %>% 
  mutate(density = (n/(population/1000))) %>% 
  top_n(25, density) %>% 
  mutate(color = ifelse(density == max(density), nord(""victory_bonds"", 1), ""grey50""))


most_tims <- ggplot(plot_data, aes(x = reorder(city, density), y = density)) +
  geom_lollipop(aes(color = color)) +
  scale_color_identity() +
  scale_y_continuous(expand = c(0,0.01),  breaks = scales::pretty_breaks(), limits = c(0,1.2)) +
  coord_flip() +
  labs(y = ""Number of Tim Hortons stores per 1,000 people"", x = NULL, title = ""However, the title of most Tim Hortons per capita belongs to Cold Lake, Alberta"", 
       subtitle = ""When looking at towns/cities with population > 10,000 and with more than two Tim Hortons. \nMy hometown of Truro, Nova Scotia comes in a puzzling fourth."",
       caption = ""\nData: timhortons.com | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Xx"")

ggsave(plot = most_tims, here(""week6"", ""Most Tims.png""), width = 10, height = 6)

",2018,6
10,10,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2018/week7/R/tw_7plot.R,"library(tidyverse)
library(here)
library(janitor)
library(likert)
library(jkmisc)
library(nord)

star_wars <- dir(here(""week7"", ""data""), pattern = ""StarWars"", full.names = TRUE) %>% 
  read_csv() 

clean_names <- stringi::stri_trans_general(names(star_wars), ""latin-ascii"") %>% 
  gsub(""[^\\x{00}-\\x{7f}]"", """", ., perl = TRUE) %>% 
  clean_names()

star_wars <- set_names(star_wars, clean_names) 

headers <- slice(star_wars, 1) %>% 
  flatten_chr()

clean_names <- gsub(""X\\d+"", NA_character_, clean_names) %>% 
  enframe() %>% 
  fill(value) %>% 
  pull(value)


shiny_clean_names <- paste(clean_names, headers, sep = ""|"")

long_star_wars <- set_names(star_wars, c(""RespondentID"", shiny_clean_names[-1])) %>% 
  slice(-1) %>% 
  gather(item, value, -1) %>% 
  separate(item, c(""question"", ""category""), sep = ""\\|"") %>% 
  mutate(category = if_else(category == ""Response"", NA_character_, category)) %>% 
  mutate(index = group_indices(., question))


plot_data <- long_star_wars %>% 
  filter(index == 12) %>% 
  replace_na(list(value = ""Unfamiliar (N/A)"")) %>% 
  filter(value != ""Unfamiliar (N/A)"") %>% 
  spread(category, value) %>% 
  mutate_at(vars(-RespondentID, -question, -index), function(x)
    factor(x, 
            levels = c(""Very unfavorably"", ""Somewhat unfavorably"",""Neither favorably nor unfavorably (neutral)"", ""Somewhat favorably"", ""Very favorably""),
            labels = 1:5
    )) 
            
            
likert_data <- plot_data %>% 
  select(-RespondentID, -question, -index) %>%
  as.data.frame() %>% 
  likert()


ggplot2::update_geom_defaults(""text"", list(family = ""Scope One"", size = 4))
  
plot <- likert.bar.plot(likert_data) + 
  scale_fill_nord(""mountain_forms"", labels = c(""Very unfavorably"", ""Somewhat unfavorably"",""Neither favorably nor unfavorably (neutral)"", ""Somewhat favorably"", ""Very favorably""), name = ""Response"") +
  labs(title = ""The Favorability Rankings of Star Wars Characters"", subtitle = ""People look favourably upon the scruffy nerf herder, and would give a ride to the EVIL RAISIN THAT SHOOTS LIGHTNING FROM HIS HANDS before the goofy gungan."") +
  theme_jk(grid = ""XY"") +
  theme(plot.title = element_text(family = ""Oswald""))

ggsave(here(""week7"", ""tw7_likert.png""), width = 16, height = 10)
  
  
 
",2018,7
11,11,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week1/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)


# week-of-month function
wom <- function(date) { 
  first <- wday(as.Date(paste(year(date),month(date),1,sep=""-"")))
  return((mday(date)+(first-2)) %/% 7+1)
}

# Get the TidyTuesday Tweets Data
tt_tweet_data <- readRDS(here(""2019"", ""week1"", ""data"", ""tidytuesday_tweets.rds""))

# Get the R tweet data 
r_tweet_data <- readRDS(here(""2019"", ""week1"", ""data"", ""rstats_tweets.rds""))

# Most that tweet about R tweet about the r4ds tidy tuesday.
no_rstats <- anti_join(tt_tweet_data, r_tweet_data, by = ""screen_name"") %>% 
  mutate(rstats_tag = case_when(grepl(""rstats"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""r4ds"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""visualization"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""data"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""code"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""plot"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""chart"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""graph"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""drob"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""ggplot"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""rstudio"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""model"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""median"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""average"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""week \\d+"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""@thomas_mock"", text, ignore.case = TRUE) ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  mutate(rstats_tag = case_when(screen_name %in% c(""NosyOwl"", ""sebastianhwells"", ""JenniferCai7"", ""matthwong"",
                                                   ""scrite_jones"", ""jrosenblum123"", ""zlipp"") ~ TRUE,
                                TRUE ~ rstats_tag)) %>% 
  filter(rstats_tag == FALSE)
  

plot_data <- anti_join(tt_tweet_data, no_rstats, by = ""screen_name"") %>% 
  mutate(created_at = as_date(created_at)) %>% 
  mutate(day = wday(created_at, label = TRUE, abbr = FALSE),
         week = wom(created_at),
         iweek = isoweek(created_at),
         month = month(created_at, label =  TRUE, abbr = FALSE),
         year = year(created_at))


count(plot_data, day, iweek) %>% 
  complete(day, iweek = 1:52, fill = list(n = NA)) %>% 
  ggplot(aes(x = iweek, y = day, fill = n)) +
  geom_tile(color = ""white"", size = 0.1) +
  scale_fill_viridis_c(""Tweet Frequency"", option = ""cividis"", na.value = ""grey95"", labels = seq(0,25,5), breaks = seq(0,25,5), limits = c(0,25)) +
  coord_equal() +
  labs(title = ""Tidy Tuesday or Tardy Tuesday?"",
       subtitle = ""A glance at when the community decides to submit their work."",
       y = NULL,
       x = ""Week of the Year"",
       caption = ""Data: rtweet | Analysis: @jakekaupp"") +
  scale_x_continuous(limits = c(1, 53), breaks = c(1,10,20,30,40,50), expand = c(0, 0)) +
  theme_jk(grid = FALSE, ticks = FALSE) +
  theme(legend.position = c(0.5,-0.7),
        legend.direction = ""horizontal"",
        legend.title = element_text(family = ""Scope One"", vjust = 0.8))

ggsave(here(""2019"", ""week1"",""tidy_or_tardy.png""), width = 8, height = 4)
",2019,1
12,12,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week10/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(ggrepel)
library(here)

jobs_gender <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/jobs_gender.csv"")


plot_data <- jobs_gender %>% 
  select(-year) %>% 
  group_by(occupation, major_category, minor_category) %>% 
  summarize_all(mean, na.rm = TRUE) %>% 
  filter(str_detect(occupation, ""engineer""), str_detect(major_category, ""Engineering"")) %>% 
  mutate(diff = if_else((total_earnings_female - total_earnings_male) > 0, ""firebrick"", ""grey80"")) %>% 
  mutate(alpha = if_else(diff == ""firebrick"", 1, 0.2)) %>% 
  gather(variable, value, starts_with(""total_earnings_"")) %>% 
  mutate(variable = factor(variable, c(""total_earnings_male"", ""total_earnings_female""), c(""Men"", ""Women"")))
 
slope_data <- build_slopegraph(plot_data, ""variable"", ""value"", ""occupation"") %>% 
  left_join(distinct(plot_data, occupation, diff, alpha), by = c(""group"" = ""occupation"")) %>% 
  mutate(group = case_when(str_detect(group, ""Mining"") ~ ""Mining Engineers"",
                                str_detect(group, ""Computer"") ~ ""Computer Engineers"",
                                str_detect(group, ""Electrical"") ~ ""Electrical Engineers"",
                                str_detect(group, ""Marine"") ~ ""Marine Engineers"",
                                str_detect(group, ""Industrial"") ~ ""Industrial Engineers"",
                           TRUE ~ str_to_title(group))) %>% 
  mutate(group = str_replace(group, ""Engineers"", ""Engineering""))


labels <- pretty(slope_data$y, 9)
breaks <- pretty(slope_data$ypos, 5)


plot <- ggplot(slope_data, aes(x = x, y = ypos, group = group, color = diff)) +
  geom_point() +
  geom_line() +
  geom_label_repel(data = filter(slope_data, x == ""Women""), aes(label = group), direction = ""y"", hjust = 0, nudge_x = 1, segment.alpha = 0.3, family = ""Oswald"", label.size = 0, fill = ""white"") +
  theme_jk(grid = ""XY"") +
  expand_limits(x = c(0, 5)) +
  scale_color_identity() +
  scale_alpha_identity() +
  scale_y_continuous(labels = scales::dollar(labels), breaks = breaks, limits = range(breaks)) +
  theme(panel.grid.major.x = element_line(linetype = ""dashed"", color = ""black"")) +
  labs(x = NULL,
       y = NULL,
       title = ""The Unnecessary and Unethical Pay Disparity in Engineering."",
       subtitle = str_wrap(""A slopegraph presenting the average total earnings from 2014-2016 for men and women across engineering disciplines.  Mining Engineering is the only discipline with women earning more than men on average."", 70),
       caption = ""Data: Census Bureau | Graphic: @jakekaupp"")

ggsave(here(""2019"", ""week10"", ""tw10_plot.png""), plot, width = 6.5, height = 9, type = ""cairo"")
",2019,10
13,13,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week10/R/helpers.R,"# Functions from an old friend.
# https://github.com/jkeirstead/r-slopegraph/blob/master/slopegraph.r

build_slopegraph <- function(df, x, y, group, min.space=0.05) {
  
  ## First rename the columns for consistency
  ids <- match(c(x, y, group), names(df))
  
  df <- df[,ids]
  
  names(df) <- c(""x"", ""y"", ""group"")
  
  ## Expand grid to ensure every combination has a defined value
  tmp <- expand.grid(x=unique(df$x), group=unique(df$group))
  
  tmp <- merge(df, tmp, all.y=TRUE)
  
  df <- mutate(tmp, y=ifelse(is.na(y), 0, y))
  
  spaced_sort(df, min.space=min.space)
  
}



spaced_sort <- function(df, min.space=0.05) {
  ## Define a minimum spacing (5% of full data range)
  min.space <- min.space*diff(range(df$y))
  
  ## Transform the data
  
  df <- split(df, ""x"") %>% 
    map_df(~calc_spaced_offset(.x, min.space))
  
  return(df)
}

##' Calculates the vertical offset between successive data points
##' 
##' @param df a data frame representing a single year of data
##' @param min.space the minimum spacing between y values
##' @return a data frame
calc_spaced_offset <- function(df, min.space) {
  
  ## Sort by value
  ord <- order(df$y, decreasing=T)
  ## Calculate the difference between adjacent values
  delta <- -1*diff(df$y[ord])
  ## Adjust to ensure that minimum space requirement is met 
  offset <- (min.space - delta)
  offset <- replace(offset, offset<0, 0)
  ## Add a trailing zero for the lowest value
  offset <- c(offset, 0)
  ## Calculate the offset needed to be added to each point
  ## as a cumulative sum of previous values
  offset <- rev(cumsum(rev(offset)))
  ## Assemble and return the new data frame
  df.new <- data.frame(group=df$group[ord],
                       x=df$x[ord],
                       y=df$y[ord],
                       ypos=offset+df$y[ord])
  return(df.new)
}
",2019,10
14,14,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week11/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(here)

board_games <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv"")

prolific <- board_games %>% 
  separate_rows(designer, sep = "","") %>% 
  filter(!str_detect(designer, ""Uncredited""), !str_detect(designer, ""Jr|III""), year_published >= 1990) %>% 
  group_by(designer) %>% 
  filter(n() >= 10) %>% 
  group_by(designer, year_published) %>% 
  summarize(year_avg_rating = mean(average_rating, na.rm = TRUE),
            games_published = n()) 

top_rated <- prolific %>% 
  group_by(designer) %>% 
  summarize(total_games = sum(games_published),
            overall_rating = mean(year_avg_rating, na.rm = TRUE)) %>% 
  top_n(1, overall_rating) %>% 
  pull(designer)

top_publishing <- prolific %>% 
  group_by(designer) %>% 
  summarize(total_games = sum(games_published),
            overall_rating = mean(year_avg_rating, na.rm = TRUE)) %>% 
  top_n(1, total_games) %>% 
  pull(designer)

overall_avg <- prolific %>% 
  group_by(year_published) %>% 
  summarize(year_avg_rating = mean(year_avg_rating, na.rm = TRUE),
            designer = ""Overall"")


plot_data <- prolific %>%
  bind_rows(overall_avg) %>% 
  mutate(color = case_when(designer == top_rated ~ ""#eebd31"" ,
                           designer == ""Overall"" ~ ""firebrick"",
                           designer == top_publishing ~ ""dodgerblue"",
                           TRUE ~ ""black""),
         alpha = case_when(designer == top_rated ~ 1,
                           designer == ""Overall"" ~ 1,
                           designer == top_publishing ~ 1,
                           TRUE ~ 0.05),
         size = case_when(designer == top_rated ~ 0.5,
                           designer == ""Overall"" ~ 0.5,
                           designer == top_publishing ~ 0.5,
                           TRUE ~ 0.3),
         point_size = case_when(designer == top_rated ~ 2,
                          designer == ""Overall"" ~ 2,
                          designer == top_publishing ~ 2,
                          TRUE ~ 1),
         line = case_when(designer == top_rated ~ ""solid"",
                           designer == ""Overall"" ~ ""dashed"",
                           designer == top_publishing ~ ""solid"",
                           TRUE ~ ""solid""))

plot <- ggplot(plot_data, aes(x = year_published, y = year_avg_rating, group = designer)) +
  geom_path(aes(color = color, alpha = alpha, linetype = line, size = size)) +
  geom_point(aes(fill = color, alpha = alpha, size = point_size), color = ""white"", shape = 21) +
  annotate(""label"", x = 1989.8, y = 2, label = ""Most Prolific: Reiner Knizia with 229 published games."", family = ""Oswald"", label.size = 0, fill = ""white"", color = ""dodgerblue"", hjust = 0) +
  annotate(""segment"", x = 1990, xend = 1990, y = 2.3, yend = 5.5, arrow = arrow(type = ""closed"", length = unit(1, ""mm"")), color = ""dodgerblue"") +
  annotate(""label"", x = 2002, y = 9, label = ""Highest Average Rating: Mark H. Walker with a 7.70 rating."", family = ""Oswald"", label.size = 0, fill = ""white"", color = ""#eebd31"") +
  annotate(""segment"", x = 2002, xend = 2002.8, y = 8.8, yend = 7.7, arrow = arrow(type = ""closed"", length = unit(1, ""mm"")), color = ""#eebd31"") +
  scale_y_continuous(limits = c(0, 10), breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_identity() +
  scale_linetype_identity() +
  scale_size_identity() +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL,
       title = ""It's Not A Habit, It's Cool, I'm a Prolific Game Designer"",
       subtitle = str_wrap(""A comparison from 1990 to 2016 of the the top rated and top published designer amongst those with 10 or more published games. Red dashed line represents the overall average designer rating."", 150),
       caption = ""Data: Board Game Geek | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"")

ggsave(here(""2019"",""week11"", ""tw11_plot.png""), width = 12, height = 6)
",2019,11
15,15,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week13/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(fs)
library(janitor)
library(jkmisc)

pet_licenses <- here(""2019"", ""week13"", ""data"") %>% 
  dir_ls(regexp = ""Seattle"") %>% 
  read_csv() %>% 
  clean_names() %>% 
  mutate_at(""license_issue_date"", mdy)

burst_name <- function(df) { 
  
  df %>% 
    distinct(license_issue_date) %>% 
    pull(license_issue_date) %>% 
    kleinberg()
    
  }

out <- pet_licenses %>% 
  filter(!is.na(animals_name)) %>% 
  mutate(animals_name = str_to_lower(animals_name)) %>% 
  group_by(animals_name) %>% 
  filter(n() >= 100) %>% 
  nest() %>% 
  mutate(bursts = map(data, burst_name)) %>% 
  unnest(bursts) %>% 
  arrange(desc(animals_name), level) %>% 
  mutate(id = ntile(animals_name, 1)) %>%
  mutate(color = case_when(level == 1 ~ ""grey50"",
                           level == 2 ~ ""#6baed6"",
                           level == 3 ~ ""#3182bd"",
                           level == 4 ~ ""#08519c""),
         alpha = case_when(level == 1 ~ 0.5,
                           level == 2 ~ 1,
                           level == 3 ~ 1,
                           level == 4 ~ 1)) %>% 
  ungroup()

facet_labels <- out %>% 
  group_by(id) %>% 
  summarize(label = sprintf(""%s to %s"", last(str_sub(animals_name, 1, 1)), first(str_sub(animals_name, 1, 1)))) %>% 
  pull(label) %>% 
  set_names(., sort(unique(out$id)))

order <- out %>% 
  filter(level == 1) %>% 
  arrange(desc(start)) %>% 
  pull(animals_name)

plot <- ggplot(out) +
  geom_segment(aes(x = start, xend = end, y = factor(animals_name, order), yend = factor(animals_name, order), color = color, alpha = alpha), size = 4, lineend = ""square"") +
  scale_color_identity() +
  scale_alpha_identity() +
  scale_x_date(limits = c(ymd(""2006/01/01""),ymd(""2019/01/01"")),  date_breaks = ""1 year"", date_labels = ""%Y"", expand = c(0.02, 0)) +
  scale_y_discrete(position = ""right"") +
  theme_jk(grid = ""XY"") +
  labs(x = NULL,
       y = NULL,
       title = ""What is it, Lassie? 'Bark! Bark-bark-bark! Bark-bark!' What, Timmy's fallen in the well?"",
       subtitle = str_wrap(""Illustrated below is the recorded use of and bursts in popularity of registered pet names (frequency of use > 100) in Seattle from 2006 to 2019.  The grey bar indicates the duration the name is in use, and the blue segments indicate bursts of increased use of the name.  Darker blue segments represent repeated bursts indicating an increased intensity of use."", 100),
       caption = ""Data: seattle.gov | Graphic: @jakekaupp"")

ggsave(here(""2019"",""week13"",""tw13_plot.png""), plot, width = 8, height = 10, type = ""cairo-png"")
",2019,13
16,16,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week14/R/analysis.R,"library(tidyverse)
library(lubridate)
library(jkmisc)
library(here)
library(patchwork)

bike_traffic <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-02/bike_traffic.csv"")

clean_bikes <- bike_traffic %>% 
  mutate(date = mdy_hms(date),
         month = month(date),
         year = year(date)) %>% 
  filter(between(year, 2014, 2018))

by_year <- clean_bikes %>% 
  group_by(year, month, crossing) %>% 
  summarize(total_bikes = sum(bike_count, na.rm = TRUE)) 

glyph <- ggplot(by_year, aes(x = crossing, y = total_bikes, group = year)) +
  geom_path() +
  geom_point(size = 1, aes(color = crossing)) +
  facet_grid(year ~ month, labeller = labeller(.cols = set_names(month.abb, 1:12)), switch = ""y"") +
  scale_color_manual(values = tol7qualitative) +
  theme_jk(grid = FALSE, plot_title_size = 14) +
  labs(x = NULL, 
       y = NULL) +
  theme(axis.text = element_blank(),
        strip.text.y = element_text(angle = 180),
        legend.position = ""none"")

main <- by_year %>% 
  filter(year == 2015, month == 1) %>% 
  ggplot(aes(x = crossing, y = total_bikes, group = year)) +
  geom_path() +
  geom_point(size = 3, aes(color = crossing)) +
  scale_color_manual(""Crossing"", values = tol7qualitative) +
  theme_jk(grid = FALSE, plot_title_size = 14) +
   labs(x = NULL, 
       y = NULL) +
  theme(axis.text = element_blank(),
        strip.text.y = element_text(angle = 180))

plot <- wrap_plots(list(main, glyph), widths = c(0.25, 0.75)) +
  plot_annotation(title = ""Annual Patterns in Seatle Bicycle Traffic"", 
                  subtitle = str_wrap(""This chart is glyph plot using multiple parallel coordinates plots to illustrate the monthly bike traffic at Seattle crossings.  A  colored dot represents each crossing and vertical position represents the total number of riders counted each month.  You can observe the year over year trends, as well as see which crossings experience cyclical patterns and which remain stable."", 155),
                  theme = theme_jk())

ggsave(here(""2019"", ""week14"", ""tw14_plot.png""), plot, width = 12, height = 6)
",2019,14
17,17,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week15/R/analysis.R,"library(tidyverse)
library(lubridate)
library(ggbeeswarm)
library(here)
library(jkmisc)
library(nord)

player_dob <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/player_dob.csv"")

grand_slams <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/grand_slams.csv"")

grand_slam_timeline <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/grand_slam_timeline.csv"")


plot_data <- player_dob %>% 
  select(name, date_of_birth) %>% 
  left_join(grand_slams, by = ""name"") %>% 
  mutate(age = interval(date_of_birth, tournament_date)/years(1)) %>% 
  group_by(name) %>% 
  filter(n()>1)
  

order <- plot_data %>% 
  group_by(name) %>% 
  filter(rolling_win_count == max(rolling_win_count)) %>% 
  arrange(rolling_win_count) %>% 
  pull(name)
  

plot <- ggplot(plot_data, aes(x = age, y = factor(name, order), size = rolling_win_count, color = gender, alpha = rolling_win_count)) +
  geom_point(aes(group = name)) + 
  facet_wrap(~gender, scales = ""free_y"") +
  scale_color_manual(values = c(""#C01E65"",""#117AB3"")) +
  scale_size_area(""Rolling Win Count"") +
  guides(size = guide_legend(override.aes = list(shape = 21, color = ""black"")), color = FALSE, alpha = FALSE) +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") +
  labs(x = NULL,
       y = NULL,
       title = ""Bright Stardom or Fading Obscurity: Looking at Players Major Wins Across their Careers."",
       subtitle = str_wrap(""The chart plots cumulative major wins against player age. Size and transparency of each point are mapped to the cumulative number of majors won.  Looking at the data, we can see the hot streaks in individual players, as well as the dominance of certain champions."", 110),
       caption = ""Data: wikipedia | Graphic: @jakekaupp"")
  
ggsave(here(""2019"",""week15"",""tw15_plot.png""), type = ""cairo"", width = 10, height = 12)
",2019,15
18,18,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week16/R/analysis.R,"library(tidyverse)
library(here) 
library(jkmisc)
library(ggalt)
library(grid)
library(Cairo)

dogs <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-16/dogs.csv"")

png(file = here(""2019"", ""week16"", ""tw16_plot.png""), width = 4, height = 4, units = ""in"", res = 300, type = ""cairo"")
                       
ggplot(dogs, aes(x = avg_weight, y = avg_neck)) +
  geom_xspline(size = 1) +
  geom_point(shape = 21, fill = ""black"", color = ""white"", stroke = 0.5, size = 2) +
  geom_text(data = filter(dogs, year == min(year)), aes(label = year), hjust = 0, nudge_x = 0.1, nudge_y = 0.01, family = ""Oswald"", size = 3) +
  geom_text(data = filter(dogs, year == max(year)), aes(label = year), hjust = 1, nudge_x = -0.1, family = ""Oswald"", size = 3) +
  annotate(""segment"", arrow = arrow(length = unit(0.2, ""cm""), type = ""closed""), x = 20.48, xend = 20.2, y = 44.3, yend = 44.03) +
  scale_y_continuous(limits = c(42, 45), breaks = 42:45) +
  expand_limits(x = c(17.5, 21)) +
  labs(title = ""Fit as a butcher's dog"",
       subtitle = ""Characteristics of dogs registered with the UK's\nKennel Club, average when fully grown"",
       x = bquote(""Weight*, kg""),
       y = NULL,
       caption = ""Sources: Kennel Club;\n The Economist "") +
  theme_jk(grid = ""XY"") +
  theme(plot.caption = element_text(hjust = -0.1))

grid.text(expression(paste(Neck~size, "", "", cm^""\u2020"")), x = 0.1, y = 0.78, gp = gpar(fontfamily = ""Oswald"", cex = 0.8))
grid.text(bquote(""* Where at leat 50 are registered per year""), x = 0.98, y = 0.075, gp = gpar(fontfamily = ""Scope One"", cex = 0.8), hjust = 1)
grid.text(expression(""\u2020""~Where~at~least~100~are~registered~per~year), x = 0.98, y = 0.040, gp = gpar(fontfamily = ""Scope One"", cex = 0.8), hjust = 1)

dev.off()
",2019,16
19,19,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week17/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)

tidy_anime <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/tidy_anime.csv"")

plot_data <- tidy_anime %>% 
  mutate(title = coalesce(title_english, name)) %>% 
  mutate(end_date = if_else(is.na(end_date), as.Date(""2019-04-01""), end_date)) %>% 
  mutate(interval = interval(start_date, end_date)) %>% 
  filter(type != ""Unknown"") %>% 
  distinct(animeID, .keep_all = TRUE) 


scaffold <- tibble(year = rep(1917:2019, each = 6),
       type = rep(c(""Movie"", ""Music"", ""ONA"", ""OVA"", ""Special"", ""TV""), length(1917:2019))) 

timeline <- scaffold %>% 
  mutate(count = map2_dbl(year, type, ~nrow(filter(plot_data, ymd(sprintf(""%s/01/01"", .x)) %within% interval , type == .y))))

order <- timeline %>% 
  filter(year == last(year)) %>% 
  arrange(desc(count)) %>% 
  pull(type)

# Area ----
area <- timeline %>% 
  mutate(type = factor(type, order, order)) %>% 
  ggplot(aes(x = year, y = count)) +
  geom_area(aes(fill = type)) +
  scale_fill_manual(""Anime Type"", values = tol6qualitative) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Fourty Years of Growth: The Rapid Rise of Anime"",
       subtitle = str_wrap(""The area chart below presents the number of anime titles released from 1919 to the present by release type.  Anime releases have increased over 400% since the 1980s, to meet the increasing demand driven by the invention of the VCR, the internet and the rise of streaming media services."", 95),
       caption = ""Data: MyAnimeList | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") +
  guides(fill = guide_legend(nrow = 1, byrow = TRUE))

# Popularity vs Score Hexbin ----
hex <- ggplot(plot_data, aes(x = popularity, y = score)) +
  geom_hex() +
  facet_wrap(~type, nrow = 1) +
  scale_fill_viridis_c(option = ""plasma"") +
  scale_x_continuous(trans = ""reverse"", breaks = scales::pretty_breaks(), labels = c("""","""",""Higher\nPopularity"", """", ""Lower\nPopularity"", """")) +
  scale_y_continuous(breaks = scales::pretty_breaks(), limits = c(0,10)) +
  guides(fill = guide_colorbar(title = ""Titles per Hex""), alpha =""none"") +
  labs(x = NULL, 
       y = NULL,
       title = ""The Relationship Between Ratings and Popularity on MyAnimeList"",
       subtitle = str_wrap(""The chart below plots the ratings score (out of 10) against popularity (rank) for all anime titles and anime types.  The data were hexangonally binned to illustrate areas of high occurance. It appears that a relationship may exist between ratings and popularity, warranting further analysis."", 100),
       caption = ""Data: MyAnimeList | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") 

ggsave(here(""2019"",""week17"",""tw17_hex.png""), hex, width = 8, height = 6)

ggsave(here(""2019"",""week17"",""tw17_area.png""), area, width = 8, height = 6)

                         ",2019,17
20,20,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week18/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)

bird_collisions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/bird_collisions.csv"")


plot_data <- bird_collisions %>%
  filter(locality == ""CHI"") %>% 
  mutate(month = month(date),
         year = year(date)) %>% 
  unite(""binomial_name"", genus, species, sep = "" "") %>% 
  count(year, month, binomial_name) %>% 
  complete(nesting(year, binomial_name), month = 1:12, fill = list(n = 0)) %>% 
  group_by(year, binomial_name) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))


flower <- ggplot(plot_data, aes(x = month, y = percent, fill = binomial_name)) +
  geom_area(size = 0, position = position_dodge(), alpha = 0.2) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  guides(fill = guide_colorbar()) +
  coord_polar() +
  labs(x = NULL,
       y = NULL,
       title = ""Overall"") +
  theme_jk(dark = FALSE, grid = ""X"", strip_text_size = 10, plot_title_size = 14) +
  theme(axis.text = element_blank(),
        legend.position = ""none"")

petals <- flower +
  aes(group = year) +
  geom_path(aes(color = binomial_name), size = 0.2, show.legend = FALSE) +
  labs(title = ""By Species"") +
  facet_wrap(~binomial_name, labeller = label_wrap_gen(10), nrow = 7) 

legend <- plot_data %>% 
  filter(binomial_name == ""Setophaga fusca"") %>% 
  ggplot(aes(x = month, y = percent, fill = binomial_name, group = year)) +
  geom_area(size = 0, position = position_dodge(), alpha = 0.1) +
  geom_path(aes(color = binomial_name), size = 0.2, show.legend = FALSE) +
  annotate(""text"", x = 11, y = 0.8, label = ""One year of\ncollisions in October"", family = ""Scope One"", size = 3, hjust = 0) +
  annotate(""segment"", x = 10.8, y = 0.8, xend = 10, yend = 0.8, arrow = arrow(length = unit(0.2, ""cm""))) +
  annotate(""text"", x = 3.5, y = 0.8, label = ""Multiple years of\ncollisions in May"", family = ""Scope One"", size = 3) +
  annotate(""segment"", x = 3.8, y = 0.8, xend = 5, yend = 0.8, arrow = arrow(length = unit(0.2, ""cm""))) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  labs(x = NULL,
       y = NULL,
       title = ""How to Interpret This Chart"",
       subtitle = str_wrap(""A flower represents the recorded total collisions of each bird species with the individual petals representing the normalized events during each year (from 0-1).  The position of the petals indicates the month or months collisions occur, with overlaps indicating repeated year-over-year collisions."", 70)) +
  guides(fill = guide_colorbar()) +
  coord_polar(theta = ""x"", start = 0) +
  theme_jk(dark = FALSE, grid = ""XY"", plot_title_size = 14) +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

out <- wrap_plots(flower / legend, petals, ncol = 2, widths = c(1, 2)) +
  plot_annotation(title = ""Seasonality of Bird Collisions in Chicago"",
                  subtitle = str_wrap(""Presented below is a petal chart of of bird collisions, with instructions on how to interpret this chart in the lower left.  The upper left flower represents collisions recorded across all years and species, with individual species presented as small multiple flowers on the right."", 220),
                  caption = ""Data: Winger et al. (2019) Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Proceedings of the Royal Society B 286(1900): 20190364. https://doi.org/10.1098/rspb.2019.0364 | Graphic: @jakekaupp"",
      theme = theme_jk())

ggsave(here(""2019"",""week18"", ""tw18_plot.png""), out, width = 16, height = 10, type = ""cairo"")



",2019,18
21,21,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week18/R/experiment.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)


bird_collisions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/bird_collisions.csv"")


plot_data <- bird_collisions %>%
  filter(locality == ""CHI"") %>% 
  mutate(month = month(date),
         year = year(date)) %>% 
  unite(""binomial_name"", genus, species, sep = "" "") %>% 
  count(year, month, binomial_name) %>% 
  complete(nesting(year, binomial_name), month = 1:12, fill = list(n = 0)) %>% 
  group_by(year, binomial_name) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))

petals <- plot_data %>% 
  filter(n != 0) %>% 
  split(list(.$year, .$month, .$binomial_name), drop = TRUE) %>% 
  map(~complete(.x, year, binomial_name, month = 1:12, fill = list(n = 0, percent = 0))) %>% 
  map(~geom_area(data = .x, aes(color = binomial_name), size = 0.2, alpha = 0.1))


base_plot <- ggplot(plot_data, aes(x = month, y = percent, fill = binomial_name)) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  labs(x = NULL,
       y = NULL) +
  coord_polar(theta = ""x"", start = 0) +
  theme_jk(dark = FALSE, grid = ""XY"", plot_title_size = 14) +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

out <- base_plot + petals + facet_wrap(~binomial_name, labeller = label_wrap_gen(10), nrow = 7)


ggsave(here(""2019"",""week18"", ""test.png""), plot = out)

",2019,18
22,22,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week19/R/analysis.R,"library(tidyverse)
library(readxl)
library(here)
library(fs)
library(jkmisc)
library(janitor)
library(countrycode)
library(patchwork)

ratio_data <- here(""2019"", ""week19"", ""data"") %>% 
  dir_ls(regexp = ""student_teacher_ratio"") %>% 
  read_excel(na = c("".."")) %>% 
  set_names(tolower(names(.))) %>% 
  gather(year, ratio, -1:-2,convert = TRUE)



plot_region <- function(data) {
  
  ggplot(data, aes(x = year, y = ratio, group = region, color = continent)) +
    geom_line(size = 0.3, show.legend = FALSE) +
    geom_ribbon(aes(ymin = 0, ymax = ratio, fill = continent), alpha = 0.5, color = NA, show.legend = FALSE) +
    facet_wrap(~region, nrow = 1) +
    scale_color_manual(values = colors) +
    scale_fill_manual(values = colors) +
    scale_x_continuous(breaks = seq(1970, 2010, 10)) +
    scale_y_continuous(limits = c(0, 40)) +
    labs(x = NULL,
         y = NULL,
         title = unique(data$continent)) +
    theme_jk(grid = ""XY"") 
  
  }

plot_continent <- function(data) {
  
  ggplot(data, aes(x = year, y = ratio, color = continent)) +
    geom_line(size = 0.3, show.legend = FALSE) +
    geom_ribbon(aes(ymin = 0, ymax = ratio, fill = continent), alpha = 0.5, color = NA, show.legend = FALSE) +
    facet_wrap(~continent, ncol = 1) +
    scale_color_manual(values = colors) +
    scale_fill_manual(values = colors) +
    scale_x_continuous(breaks = seq(1970, 2010, 10)) +
    scale_y_continuous(limits = c(0, 40)) +
    scale_shape_identity() +
    labs(x = NULL,
         y = NULL,
         title = unique(data$continent)) +
    theme_jk(grid = ""XY"") +
    theme(strip.text = element_blank())
  
}

plot_data <- ratio_data %>% 
  filter(type == ""Countries"") %>% 
  mutate(country_code = countrycode(country, ""country.name"", ""iso3c"")) %>% 
  mutate(region = countrycode(country_code, ""iso3c"", ""region"")) %>% 
  mutate(continent = countrycode(country_code, ""iso3c"", ""continent"")) %>% 
  filter(!is.na(region))

colors <- set_names(c(""#171635"", ""#00225D"", ""#763262"", ""#CA7508"", ""#E9A621""), c(unique(plot_data$continent)))

individual <- plot_data %>% 
  group_by(year, region, continent) %>% 
  summarize(ratio = mean(ratio, na.rm = TRUE)) %>% 
  filter(!is.nan(ratio)) %>% 
  split(.$continent) %>% 
  map(plot_region) 

summary <- plot_data %>% 
  group_by(year, continent) %>% 
  summarize(ratio = mean(ratio, na.rm = TRUE)) %>% 
  filter(!is.nan(ratio)) %>% 
  split(.$continent) %>% 
  map(plot_continent) 
  
plots <- map2(summary, individual, ~wrap_plots(.x, .y, nrow = 1, widths = c(1, 1)))
  
out <- wrap_plots(plots, ncol = 1) +
  plot_annotation(title = ""Working to Two Sigma: Student Teacher Ratios Improving Since the 1970s"",
                  subtitle = str_wrap(""Illustrated below is the average student to teacher ratio across each continent (left column) and region (right column).  Continent and region assigned from iso3c coding of country name and are consistent with the World Bank Dvelopment Indicators."", 210),
                  caption = ""Data: UNESCO Institute of Statistics | Graphic: @jakekaupp"",
                  theme = theme_jk())

ggsave(here(""2019"",""week19"",""tw19_plot.png""), out, width = 16, height = 10)
",2019,19
23,23,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week2/R/analysis.R,"library(tidyverse)
library(ggraph)
library(tidygraph)
library(jkmisc)
library(lubridate)
library(here)
library(patchwork)
library(nord)

set.seed(42)

source(here(""2019"", ""week2"", ""R"", ""functions.R""))

# Read data from github repo
tv_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-08/IMDb_Economist_tv_ratings.csv"") %>% 
  rename(title_id = titleId,
         season_number = seasonNumber) %>% 
  mutate(year = year(date))

# Make this into a nodes tibble
list <- tv_data %>% 
  split(.$year) %>% 
  map(share_packed_circle)

out <- wrap_plots(list, ncol = 10, nrow = 3) +
  plot_annotation(title = ""The Evolution and Differentiation of Dramas Across the Golden Age of Television"",
                  subtitle = str_wrap(""This chart presents a time series of circle-packed network representations of the television dramas.  
                                      The larger dark blue circle represents the year, light blue represents the genre (Action, Comedy, etc.) and the pale pink represents the individual program. 
                                      The area of each circle (node) is porportional to the sum of the audience share of the smaller circles within (child nodes)."", 180),
                  caption = ""data: IMDb | graphic: @jakekaupp"",
                  theme = theme_jk(plot_title_size  = 22, subtitle_size = 14) %+replace% theme(plot.background = element_rect(fill =""#2E3440""),
                                                      text = element_text(color = ""white"")))
 
ggsave(here(""2019"",""week2"", ""tt_week2.png""), out, width = 16, height = 8)
",2019,2
24,24,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week2/R/functions.R,"share_packed_circle <- function(df) {
 
  nodes <- make_nodes(df)
  
  edges <- make_edges(df)
 
  mygraph <- tbl_graph(nodes = nodes, edges = edges)
  
  # Make the plot
  plot <- ggraph(mygraph, layout = 'circlepack', weight = ""size"") + 
    geom_node_circle(aes(fill = depth)) +
    theme_void() +
    labs(title = unique(df$year)) +
    coord_equal() +
    scale_fill_nord(""lumina"", discrete = FALSE, reverse = TRUE) +
    #scale_fill_viridis(option = ""plasma"") +
    theme(legend.position = ""none"", 
          plot.background = element_rect(fill = ""#4C566A""),
          plot.title = element_text(family = ""Oswald"", hjust = 0.5, color = ""white""),
          )
  
  return(plot)
}


make_nodes <- function(df) {
  
  size <- df %>% 
    group_by(title, genres, year) %>% 
    summarize(share = mean(share)) %>% 
    distinct(genres, title, share) %>% 
    rename(name = title, size = share)
  
  genre_size <- size %>% 
    group_by(genres) %>% 
    summarize(size = sum(size)) %>% 
    rename(name = genres)
  
  title_size <- size %>% 
    ungroup() %>% 
    distinct(name, size) %>% 
    mutate(size = size)
  
  total_size <- df %>% 
    distinct(title, share) %>% 
    summarize(name = as.character(unique(df$year)),
              size = sum(share))
  
  sizes <- bind_rows(genre_size, title_size, total_size)
  
  nodes <- df %>% 
    group_by(title, genres, year) %>% 
    summarize(share = mean(share)) %>% 
    gather(variable, name, title, genres, year) %>% 
    arrange(variable, name) %>% 
    distinct(name) %>% 
    left_join(sizes, by = ""name"") %>% 
    mutate(size = if_else(size == 0, 0.001, size)) %>% 
    arrange(size)
  
  return(nodes)
  
  
}

make_edges <- function(df) {
  
  base <- tibble(from = as.character(unique(df$year)), to = unique(df$genres))
  
  inner <- df %>% 
    select(from = genres, to = title) %>% 
    distinct() 
  
  edges <- bind_rows(base, inner) 
  
  return(edges)
  
}
",2019,2
25,25,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week20/R/analysis.R,"library(tidyverse)
library(here)
library(fs)
library(rcrossref)
library(ggbeeswarm)
library(jkmisc)


# Not re-downloading things, the citation count pulls take 2hrs.
if (length(dir_ls(here(""2019"", ""week20"", ""data""))) <= 0) {
 
  nobel_winners <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winners.csv"")
  nobel_winners_all_pubs <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winner_all_pubs.csv"")
  
  saveRDS(nobel_winners, here(""2019"", ""week20"", ""data"", ""nobel_winners.RDS""))
  
  saveRDS(nobel_winners_all_pubs, here(""2019"", ""week20"", ""data"", ""nobel_winners_all_pubs.RDS""))

  
} else {
  
  nobel_winners <- dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""nobel_winners.RDS"") %>% 
    readRDS()
  
  nobel_winners_all_pubs <- dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""nobel_winners_all_pubs.RDS"") %>% 
    readRDS()
 
}


if (length(dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""cite_count"")) <= 0) {

  dois <- nobel_winners_all_pubs$doi

  list  <- split(dois, rep(1:ceiling(length(dois)/50), each=50)[1:length(dois)])

wait_cr_citation_count <- function(doi, index, list_len) {
  
  print(sprintf(""%s complete"", scales::percent(index/list_len)))
  
  Sys.sleep(1)
  
  cr_citation_count(doi)
  
}

cite_count <- imap_dfr(list, ~wait_cr_citation_count(.x, .y, length = length(list)))

saveRDS(cite_count, here(""2019"", ""week20"", ""data"", ""cite_count.RDS"")) 

} else {
  
cite_count <- readRDS(here(""2019"", ""week20"", ""data"", ""cite_count.RDS""))
  
}

highlights <- c(""einstein, a"", ""hill, av"", ""heeger, a"")

plot_data <- nobel_winners_all_pubs %>%
  left_join(cite_count) %>%
  distinct(laureate_id, paper_id, .keep_all = TRUE) %>%
  select(pub_year, laureate_name, is_prize_winning_paper, count, category) %>% 
  replace_na(list(count = 0)) %>% 
  group_by(laureate_name, pub_year, category) %>%
  summarize(count = sum(count)) %>% 
  group_by(laureate_name) %>% 
  mutate(rolling_sum = cumsum(count)) %>% 
  mutate(color = if_else(laureate_name %in% highlights, ""#F24534"", ""#21344F""),
         alpha = if_else(laureate_name %in% highlights, 1, 0.2))


everyone <- filter(plot_data, laureate_name %notin% highlights)

focus <- filter(plot_data, laureate_name %in% highlights) %>% 
  ungroup() %>% 
  mutate(laureate_name = case_when(laureate_name == ""einstein, a"" ~ ""Einstein, A"",
                                   laureate_name == ""hill, av"" ~ ""Hill, AV"",
                                   laureate_name == ""heeger, a"" ~ ""Heeger, A"")) %>% 
  group_by(laureate_name)


plot <- ggplot(plot_data, aes(x = pub_year, y = rolling_sum, group = laureate_name)) +
  geom_step(aes(color = color, alpha = alpha)) +
  geom_step(data = focus, aes(color = color, alpha = alpha)) +
  geom_text(data = filter(focus, pub_year == last(pub_year)), aes(color = color, alpha = alpha, label = laureate_name), x = 2018, family = ""Oswald"", hjust = 0) +
  scale_x_continuous(limits = c(1900, 2100), breaks = c(1900, 1925, 1950, 1975, 2000, 2018)) +
  scale_y_continuous(breaks = scales::pretty_breaks(), labels = scales::number) +
  scale_color_identity() +
  scale_alpha_identity() +
  facet_wrap(~category, labeller = as_labeller(str_to_title)) +
  labs(x = NULL,
       y = NULL,
       title = ""Growth Patterns in How Often Nobel Prize Winning Researchers Are Cited"",
       subtitle = str_wrap(""Cummulative citation count by year (1900-2018).  Highlighted are A. Heeger (conductive polymers), A.V. Hill (heat and work in muscle) and A. Einstein (photoelecric effect). Each exhibit different citation patterns, likely attributed to the continued relevance and impact of their work."", 150),
       caption = ""Data: Li, Jichao; Yin, Yian; Fortunato, Santo; Wang Dashun, 2018, 'A dataset of publication records for Nobel laureates', https://doi.org/10.7910/DVN/6NJ5RN, Harvard Dataverse. | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Y"") +
  theme(legend.position = ""bottom"") 

ggsave(here(""2019"", ""week20"", ""tw20_plot.png""), plot, width = 12, height = 6)

",2019,20
26,26,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week21/R/analysis.R,"library(tidyverse)
library(here)
library(readxl)
library(fs)
library(janitor)
library(jkmisc)
library(patchwork)


# Plotting Function to make separate ordered stacked bars by group----
make_bars <- function(df, pals) {
  
  order <- df %>% 
    arrange(desc(other)) %>% 
    pull(country)
  
  labs <- c(""HIC"" = ""High Income Group"",
            ""UMI"" = ""Upper Middle Income Group"",
            ""LMI"" = ""Lower Middle Income Group"",
            ""LI"" = ""Low Income Group"")
  
  
  df %>% 
    gather(type, value, c(""other"", ""plastic_waste"", ""inadequate_waste"", ""littered_waste"")) %>% 
    mutate(type = factor(type, c(""other"", ""inadequate_waste"", ""plastic_waste"", ""littered_waste""), c(""Other"", ""Inadequately Managed Waste"", ""Plastic Waste"", ""Littered Waste""))) %>% 
    mutate(country = factor(country, order)) %>% 
    mutate(alpha = if_else(type == 'Other', 0.5, 0.8)) %>% 
    ggplot() +
    geom_col(aes(x = country, y = value, fill = type, alpha = alpha), width = 0.90, size = 0.1) +
    coord_flip() +
    scale_fill_manual("""", values = pals) +
    scale_alpha_identity() +
    scale_y_continuous(expand = c(0,0.05), labels = scales::percent) +
    labs(x = NULL, y = NULL) +
    facet_wrap(~economic_status, scales = ""free_y"", labeller = as_labeller(labs)) +
    theme_jk(grid = FALSE) +
    theme(legend.direction = ""horizontal"")
  
}


# Function to extract ggplot legends ----
extract_legend <- function(ggp){
  
  tmp <- ggplot_gtable(ggplot_build(ggp))
  
  leg <- which(map_lgl(tmp$grobs, function(x) x$name == ""guide-box""))
  
  legend <- tmp$grobs[[leg]]
  
  return(legend)}


# Read in Coastal Waste Data----
# Plastic waste inputs from land into the ocean
# BY JENNA R. JAMBECK, ROLAND GEYER, CHRIS WILCOX, THEODORE R. SIEGLER, MIRIAM PERRYMAN, ANTHONY ANDRADY, RAMANI NARAYAN, KARA LAVENDER LAW
# 
# SCIENCE13 FEB 2015 : 768-771

coastal_waste <- here(""2019"", ""week21"", ""data"") %>% 
  dir_ls(regexp = ""xlsx"") %>% 
  read_excel() %>% 
  clean_names() %>% 
  set_names(str_remove(names(.), ""_*[0-9]$"")) %>% 
  mutate(country = str_remove(country, ""[0-9]"")) %>% 
  mutate(country = case_when(str_detect(country, ""Palestine"") ~ ""Palestine"",
                             str_detect(country, ""Korea, South"") ~ ""South Korea"",
                             str_detect(country, ""Korea, North"") ~ ""North Korea"",
                             str_detect(country, ""Congo"") ~ ""Congo"",
                             TRUE ~ country)) %>% 
  filter(!grepl(""Burma"", country)) %>% 
  filter(complete.cases(.)) %>% 
  mutate(other = 100 - (percent_plastic_in_waste_stream + percent_inadequately_managed_waste + percent_littered_waste)) %>% 
  rename(plastic_waste = percent_plastic_in_waste_stream, inadequate_waste  = percent_inadequately_managed_waste, littered_waste = percent_littered_waste) %>% 
  mutate_at(c(""other"", ""plastic_waste"", ""inadequate_waste"", ""littered_waste""), function(x) x/100) %>% 
  mutate(other = if_else(other < 0, 0, other)) %>% 
  mutate(total_waste = waste_generation_kg_day * 365/1000,
         total_plastic_waste = total_waste * plastic_waste,
         total_inadequate_waste = total_waste * inadequate_waste,
         total_littered =  total_waste * littered_waste,
         other_waste = total_waste * other)


# Palette for plot----
pal <- c(""#F5F0F6"", ""#629460"", ""#385F71"", ""#2B4162"")

avg <- coastal_waste %>% 
  summarize(total_waste = mean(total_waste, na.rm = TRUE),
            other_waste  = mean(other_waste, na.rm = TRUE),
            total_plastic_waste  = mean(total_plastic_waste, na.rm = TRUE),
            total_inadequate_waste = mean(total_inadequate_waste, na.rm = TRUE),
            total_littered = mean(total_littered, na.rm = TRUE)) %>% 
  mutate(country = ""Global Average"")


order <- coastal_waste %>% 
  top_n(50, total_inadequate_waste) %>% 
  bind_rows(avg) %>% 
  top_n(50, total_inadequate_waste) %>% 
  arrange(desc(total_inadequate_waste)) %>% 
  pull(country) 



overall_mismanaged <- coastal_waste %>% 
  top_n(50, total_inadequate_waste) %>% 
  bind_rows(avg) %>% 
  top_n(50, total_inadequate_waste) %>% 
  arrange(desc(total_inadequate_waste)) %>% 
  gather(type, value, c(""other_waste"", ""total_plastic_waste"", ""total_inadequate_waste"", ""total_littered"")) %>% 
  mutate(type = factor(type,  c(""other_waste"", ""total_plastic_waste"", ""total_inadequate_waste"", ""total_littered""), c(""Other"", ""Inadequately Managed Waste"", ""Plastic Waste"", ""Littered Waste""))) %>% 
  mutate(country = factor(country, rev(order))) %>% 
  mutate(alpha = if_else(type == 'Other', 0.5, 0.8)) %>% 
  mutate(strip = ""Top 50 Producers & Global Average of Total Indequately Managed Waste (kg)"") %>% 
  ggplot() +
  geom_col(aes(x = country, y = value, fill = type, alpha = alpha), width = 0.90, size = 0.1) +
  coord_flip() +
  scale_fill_manual("""", values = pal) +
  scale_alpha_identity() +
  scale_y_continuous(expand = c(0,0.05), labels = scales::comma) +
  labs(x = NULL, y = NULL) +
  facet_wrap(~strip) +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""none"")


# Make plots----
list <- coastal_waste %>% 
  split(.$economic_status) %>% 
  map(make_bars, pal)

# Extract legend----
legend <- extract_legend(list[[1]])

# Remove legend from list of plots----
list <- map(list, ~.x + theme(legend.position = ""none""))
  
# Finish plot----
out <- (overall_mismanaged + wrap_plots(list[c(""HIC"", ""UMI"", ""LMI"", ""LI"")], nrow = 1) + plot_layout(widths = c(0.3, 0.7))) / legend + plot_layout(heights = c(0.95, 0.05)) +
  plot_annotation(title = ""The Relationship Between World Bank Income Classification and Mismanaged Waste"",
                  subtitle = str_wrap(""Illustrated below is the percentage of waste by category for each country by World Bank income classification.  The lower the classification, the higher the mismanaged waste.  Much of this mismanaged waste (especially plastics) ends up in waterways that ultimately lead to our oceans, suggesting that global income inequality plays a role in ocean pollution by hampering the implementation of effective waste management strategies."", 240),
                  caption = ""Data: Jambeck, Jenna R., et al. 'Plastic waste inputs from land into the ocean.' Science 347.6223 (2015): 768-771. | Graphic: @jakekaupp"",
                  theme = theme_jk())

ggsave(here(""2019"", ""week21"", ""tw21_plot.png""), out, width = 19, height = 12)



",2019,21
27,27,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week22/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(lubridate)
library(here)
library(ggridges)
library(tidytext)
library(countrycode)
library(ggwordcloud)
library(patchwork)

source(here(""2019"", ""week22"", ""R"", ""packed_bars.R""))

wine_ratings <- here(""2019"", ""week22"", ""data"", ""winemag-data-130k-v2.csv"") %>% 
  read_csv()

wine_counts <- wine_ratings %>% 
  count(country) %>% 
  mutate(max_rel_val = n/sum(n)) %>% 
  filter(!is.na(country))
 
summary_ratings <- wine_ratings %>% 
  group_by(country) %>% 
  summarize_at(c(""points"",""price""), mean, na.rm = TRUE) %>% 
  filter(!is.na(country))

summary_data <- left_join(wine_counts, summary_ratings)

plot_data <- pack_bars(summary_data, number_rows = 4, max_rel_val)

packed_bar <- ggplot(plot_data) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = ""white"") +
  geom_text(data = filter(plot_data, fill == ""#4B384C""), aes(x = xmin, y = (ymin + ymax)/2, label = country), family = ""Oswald"", color = ""white"", nudge_x = 0.01, hjust = 0) +
  scale_fill_identity() +
  scale_color_identity() +
  scale_x_continuous(labels = scales::percent) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

word_counts <- wine_ratings %>%
  select(country, description) %>%
  group_by(country) %>% 
  filter(n() > 2) %>% 
  filter(!is.na(country)) %>% 
  unnest_tokens(word, description) %>% 
  anti_join(stop_words) %>% 
  count(country, word) %>% 
  mutate(word = trimws(word)) %>% 
  filter(!str_detect(word, ""[0-9]""), !str_detect(word, ""aroma|wine|note|nose|notes|aromas|drink|drinks|feel|feels|finish"")) %>% 
  group_by(country) %>% 
  top_n(300, n) 

clouds <- word_counts %>% 
  ungroup() %>% 
  mutate(iso2 = tolower(countrycode(country, ""country.name"", ""iso2c"")),
         iso2 = if_else(country == ""England"", ""gb"", iso2)) %>% 
  filter(country %in%  c(""US"", ""France"", ""Italy"", ""Spain"")) %>% 
  mutate(country = factor(country, levels = c(""US"", ""France"", ""Italy"", ""Spain"")),
         iso2 = factor(iso2, levels = c(""us"",""fr"", ""it"", ""es""))) %>% 
  group_by(iso2) %>% 
  nest() %>% 
  arrange(iso2) %>% 
  mutate(clouds =  map2(iso2, data, create_wc))

word_clouds <- wrap_plots(clouds$clouds, ncol = 1) 

out <- packed_bar + word_clouds +
  plot_annotation(title = ""Wine-ing: The Top 4 Countries and What Reviewers Say About Their Wines"",
                  subtitle = str_wrap(""On the left, a packed bar chart showing the % of reviewed wines by country.  On the right, wordclouds of the top 300 most frequent terms used in reviews."", 100),
                  caption = ""Data: Kaggle via WineEnthusiast | Graphic: @jakekaupp"",
                  theme = theme_jk()
                  )

ggsave(here('2019', ""week22"", ""tw22_plot.png""), out, width = 8, height = 12)

ggsave(here('2019', ""week22"", ""packed_bar.png""), packed_bar + labs(title = ""Top 4 Countries Reviewed as Packed Bar Chart"",
                                                                   subtitle = str_wrap(""The visualizion below is a packed bar chart, developed by Xan Gregg.  It combines the ordered nature of a bar chart with the total view and condensed nature of a treemap.  Colour denotes the focus, while the each gray sections represents each other reviwed country. This gives a sense of how many secondary categories there are, their magnitude and distribution. Additionally, since they are on the same scale of the focused bars we can even estimate some of the values from the length they span on the axis."", 100)), width = 8, height = 6)
",2019,22
28,28,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week22/R/packed_bars.R,"
pack_bars <- function(data, number_rows, value_column, fill_color = ""#4B384C"", border_color = ""white"") {

value_column <- ensym(value_column)  
  
color_bar_data <- summary_data %>% 
  top_n(number_rows, !!value_column) %>% 
  arrange(desc(!!value_column))

# calc row height based on num rows
bar_h = 1/number_rows

color_bars <- color_bar_data %>% 
  mutate(fill = fill_color,
         color = border_color,
         xmin = 0,
         xmax = !!value_column,
         ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1)),
         ymax = map_dbl(1:number_rows, ~1 - bar_h*.x))

gray_bar_data <- summary_data %>% 
  anti_join(color_bar_data) %>% 
  arrange(desc(!!value_column))

#get max x level for each bar level
row_sums <- pull(color_bar_data, !!value_column)

#gen gray ramp function
gray_gen <- colorRampPalette(c(""#E8E8E8"", ""#cccccc""))

#gen gray ramp
grays <- gray_gen(105)
low_grays <- grays[1:50]
hi_grays  <- grays[56:105]

last_gray <- sample(c(low_grays, hi_grays), number_rows, replace = TRUE)

gray_bar_list <- vector('list', nrow(gray_bar_data))


for (i in 1:nrow(gray_bar_data)) {
  
  row <- gray_bar_data[i,]
  
  # Determine placing of each block by looking at the minium starting values of colored bars
  # adding on the new block and setting value to represent the new block length
  vert_pos <- which.min(row_sums + pull(row, !!value_column))
  
  # Assign alternating random grays to fill
  if (i == 1) {
    
    gray_fill <- sample(low_grays, 1)
    
  } else {
    
    last_gray <- last_gray[vert_pos]
    
    gray_fill <- ifelse(last_gray %in% low_grays, sample(hi_grays, 1), sample(low_grays, 1))
  }
  
  last_gray[vert_pos] <- gray_fill
  
  # Generate aes for geom_rect
  gray_bar_list[[i]]  <- mutate(row, 
                                fill = gray_fill,
                                color = border_color,
                                xmin = row_sums[[vert_pos]],
                                xmax = row_sums[[vert_pos]] + !!value_column,
                                ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1))[[vert_pos]],
                                ymax = map_dbl(1:number_rows, ~1 - bar_h*.x)[[vert_pos]]
  )
  
  # Assign the new color_bar + rectangle as the max value for that row 
  row_sums[[vert_pos]] <- gray_bar_list[[i]]$xmax
  
}

gray_bars <- bind_rows(gray_bar_list)

bind_rows(color_bars, gray_bars)

}


create_wc <- function(iso2, data) {
  
  cntry_mask <- png::readPNG(here(""2019"", ""week22"", ""data"", ""png maps"", iso2, ""1024.png""))
  
  ggplot(data, aes(label = word, size = n)) +
    geom_text_wordcloud(family = ""Oswald"", mask = cntry_mask, rm_outside = TRUE) +
    scale_radius(range = c(0, 40)) +
    theme_jk() 
  
  
}
",2019,22
29,29,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week23/R/analysis.R,"library(tidyverse)
library(glue)
library(rvest)
library(xml2)
library(lubridate)
library(here)
library(jkmisc)



#Scraping functions----

get_urls <- function(sitemap_url) {
  
  read_xml(sitemap_url) %>%
    xml_children() %>% 
    xml_children() %>% 
    xml_text() %>% 
    keep(~str_detect(.x, ""https://www.theramenrater.com/[0-9]{4}/[0-9]{2}/[0-9]{1,}/\\w+""))
  
}

get_post_title <- function(url, idx, rows) {
  
  print(sprintf(""Progress: %s/%s"", idx, rows))
  
  read_html(url) %>% 
    html_node("".entry-title"") %>% 
    html_text()
  
  
}

slowly_get_post_title <- slowly(~ get_post_title(.x, .y, rows), rate = rate_delay(pause = 0.5), quiet = TRUE)

#Week of month
wom <- function(date) { # week-of-month
  first <- wday(as.Date(paste(year(date), month(date), 1, sep=""-"")))
  return((mday(date) + (first - 2)) %/% 7 + 1)
}

#Plotting functions----
month_outline <- function(df) {
  
  top1 <- with(df, tibble(x = min(wmonth) - 0.5,
                          xend = wday[day == min(day)] - 0.5,
                          y = wmonth[day == min(day)] + 0.5,
                          yend = wmonth[day == min(day)] + 0.5,
                          line = ""top1"")) 
  
  top2 <- with(df, tibble(x = wday[day == min(day)] - 0.5,
                          xend = max(wday) + 0.5,
                          y = min(wmonth) - 0.5,
                          yend = min(wmonth) - 0.5,
                          line = ""top2"")) 
  
  left1 <- with(df, tibble(x = wday[day == min(day)] - 0.5,
                           xend = wday[day == min(day)] - 0.5,
                           y = wmonth[day == min(day)] + 0.5,
                           yend = min(wmonth) - 0.5,
                           line = ""left1""))
  
  left2 <- with(df, tibble(x = min(wmonth) - 0.5,
                           xend = min(wmonth) - 0.5,
                           y = wmonth[day == min(day)] + 0.5,
                           yend = wmonth[day == max(day)] + 0.5,
                           line = ""left2""))
  

  right1 <- with(df, tibble(x = max(wday) + 0.5,
                            xend = max(wday) + 0.5,
                            y = min(wmonth) - 0.5,
                            yend = wmonth[day == max(day)] - 0.5,
                            line = ""right1""))
  
  right2 <- with(df, tibble(x = wday[day == max(day)] + 0.5,
                            xend = wday[day == max(day)] + 0.5,
                            y = wmonth[day == max(day)] - 0.5,
                            yend = wmonth[day == max(day)] + 0.5,
                            line = ""right2""))

  
  bottom1 <- with(df, tibble(x = min(wmonth) - 0.5,
                             xend = wday[day == max(day)] + 0.5,
                             y = wmonth[day == max(day)] + 0.5,
                             yend = wmonth[day == max(day)] + 0.5,
                             line = ""bottom1""))
  
  bottom2 <- with(df, tibble(x = wday[day == max(day)] + 0.5,
                             xend = max(wday) + 0.5,
                             y = wmonth[day == max(day)] - 0.5,
                             yend = wmonth[day == max(day)] -0.5,
                             line = ""bottom2""))
  
  top <- bind_rows(top1, top2)
  left <- bind_rows(left1, left2)
  bottom <- bind_rows(bottom1, bottom2) 
  right <- bind_rows(right1, right2) 
    
    bind_rows(top, left, right, bottom) %>% 
      mutate(year = unique(df$year),
             month = unique(df$month)) 
    
  
}


if(!file.exists(here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))) {
  
  # Add Recent ratings #3181-319
  recent_ratings <- tibble(review_number = c(3181:3189, 1676, 2745, 2991),
                           stars = c(3.75, 3.25, 4.0, 3.25, 2.0, 3.75, 3.75, 3.5, 2.25, 4.25, 5, 3.25),
                           brand = c(""Nissin Yakisoba"", ""Maruchan"", ""Uni-President"", ""Maruchan"", ""Sakruai Foods"", ""Nissin Mago"", ""Big Bon"", ""Sapporo Ichiban"", ""Canton"", ""A1"", ""Nissin"", ""Big Bon""),
                           variety = c(""Instant Panict Savory Beef Flavour"", ""Maruchan Ramen Noodle Soup Roast Beef Flavour"", ""Imperial Big Meal Super Hot Pot Beef Flavour"",
                                       ""Ramen Noodle Soup Pork Beef Flavour"", ""Vegetarian Stir Fry Noodles"", ""Nissin Lamen Light Legumes "", ""Spice Mix Piquant"", ""Momosan Ramen Tokyo Chicken"", ""Instant Noodles Spicy Tomato"",
                                       ""Emperor Herbs Chicken Noodle"", ""U.F.O. Big Wasabi-Mayo Yakisoba"", ""Chicken & Salsa Sauce Instant Noodles""),
                           country = c(""Phillipines"", ""United States"", ""Taiwan"", ""United States"", ""Japan"", ""Brazil"", ""Russia"" , ""United States"", ""India"", ""Malaysia"", ""Japan"", ""Russia""),
                           style = c(""Cup"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack""))
  
  # Read tidytesday data----
  ramen_ratings <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv"") %>% 
    bind_rows(recent_ratings)
  
  out <- tibble(sitemap_url = glue(""https://www.theramenrater.com/post-sitemap{1:5}.xml""),
                contents = map(sitemap_url, get_urls)) %>% 
    unnest() 
  
  rows <- nrow(out)
  
  # Scrapin der web purges----
  out <-  out %>% 
    mutate(title = imap(contents, ~slowly_get_post_title(.x, .y, rows))) %>% 
    mutate(date = parse_date(str_extract(contents, ""[0-9]{4}/[0-9]{2}/[0-9]{1,}""), ""%Y/%m/%d""),
           review_number = as.numeric(str_extract(title, ""(?!#)[0-9]{1,4}(?=\\:)""))) %>% 
    filter(!is.na(review_number)) %>% 
    left_join(ramen_ratings) %>% 
    filter(review_number < 4000) %>% 
    select(-sitemap_url)
  
  # Saving the new dataset----
  saveRDS(out, here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))
  
} else {
  
  ramen_data <- readRDS(here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))
  
}

#Make months
all_dates <- tibble(date = seq.Date(from = ymd(""2009/01/01""), to =ymd(""2019/12/31""), by =""day"")) %>% 
  mutate(day = day(date),
         month = month(date),
         year = year(date))

plot_data <- ramen_data %>%
  mutate(day = day(date),
         month = month(date),
         year = year(date)) %>% 
  group_by(year, month, day) %>% 
  summarize(brands = toString(sprintf(""%s: %s"", brand, variety)),
            count = n(),
            avg_stars = mean(stars)) %>% 
  right_join(all_dates) %>% 
  ungroup() %>% 
  mutate(wday = wday(date, label = TRUE, week_start = 7),
         month = month(date,label = TRUE),
         wmonth = wom(date),
         week = week(date))
  
outlines <- all_dates %>% 
  mutate(wday_label = wday(date, label = TRUE),
         wday = wday(date),
         month = month(date,label = TRUE),
         wmonth = wom(date),
         week = week(date)) %>% 
  split(list(.$year, .$month), drop = TRUE) %>% 
  map_df(month_outline)



plot <- ggplot(data = plot_data, aes(x = wday, y = wmonth, fill = avg_stars)) +
  geom_tile(color = ""grey80"", size = 0.1) +
  geom_segment(data = outlines, aes(x = x, xend = xend, y = y, yend = yend, group = line), color = ""grey30"", inherit.aes = FALSE) +
  scale_y_continuous(trans = ""reverse"", labels = NULL) +
  scale_x_discrete(labels = NULL) +
  scale_fill_gradientn(""Average Stars"", colors = rev(parula(100)), na.value = ""grey95"") +
  facet_grid(month ~ year, switch = ""y"") +
  labs(x = NULL,
       y = NULL,
       title = ""The Prolfic Nature of the Ramen Rater and a Birds-Eye View of Ramen Quality"",
       subtitle = str_wrap(""Below is a heatmap calendar of the all the Ramen Raters ramen ratings by the published date of the review.  In the early days, multiple reviews were posted in a single day, until reaching the usual pattern of a single review per day.  However, there are still some reviews that get posted en masse."", 100),
       caption = ""Data: The Ramen Rater | Graphic: @jakekaupp"") +
  theme_jk(grid = FALSE) +
  theme(strip.text.y = element_text(angle = 180),
        panel.spacing.y = unit(-0.2, ""lines""),
        legend.position = ""bottom"")

ggsave(here(""2019"", ""week23"", ""tw23_plot.png""), height = 11, width = 8.5, type = ""cairo"")





  
",2019,23
30,30,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week24/R/analysis.R,"library(nord)
library(tidyverse)
library(ggmap)
library(here)
library(countrycode)
library(jkmisc)
library(patchwork)

source(here(""2019"", ""week24"", ""R"", ""packed_bars.R""))

if (!file.exists(here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))) {
  
  slow_revgeocode <- slowly(~revgeocode(.x, output = ""address""), rate = rate_delay(0.03), quiet = TRUE)
  
  reverse_geocoded <- meteorites %>% 
    distinct(long, lat) %>% 
    mutate(location = map2_chr(long, lat, ~slow_revgeocode(c(.x, .y))))
  
  saveRDS(reverse_geocoded, here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))
  
  
} else {
  
  meteorite_locations <- readRDS(here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))
  
  
}

meteorites <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv"")

meteorites <- left_join(meteorites, meteorite_locations) %>% 
  mutate(country_code = countrycode(location, ""country.name"", ""iso3c"")) %>% 
  mutate(country_code = case_when(str_detect(location, ""UK"") ~ ""GBR"",
                                  str_detect(location, ""USA"") ~ ""USA"",
                                  str_detect(location, ""China"") ~ ""CHN"",
                                  str_detect(location, ""Philippines"") ~ ""PHL"",
                                  str_detect(location, ""Serbia"") ~ ""RUS"",
                                  str_detect(location, ""Australia"") ~ ""AUS"",
                                  str_detect(location, ""Chile"") ~ ""CHL"",
                                  str_detect(location, ""Shopian"") ~ ""IND"",
                                  str_detect(location, ""Argentina"") ~ ""ARG"",
                                  str_detect(location, ""Bass Strait"") ~ ""AUS"",
                                  TRUE ~ country_code)) %>% 
  mutate(country_code = case_when(str_detect(name, ""Indarch"") ~ ""AZE"",
                                  str_detect(name, ""Oum Dreyga"") ~ ""ESH"",
                                  str_detect(name, ""Zag"") ~ ""ESH"",
                                  str_detect(name, ""Al Haggounia"") ~ ""ESH"",
                                  str_detect(name, ""Bou Kra"") ~ ""ESH"",
                                  TRUE ~ country_code)) %>% 
  rename(iso3c = country_code) %>% 
  filter(!is.na(iso3c)) 


world_tile_grid <- read_csv(""https://gist.githubusercontent.com/maartenzam/787498bbc07ae06b637447dbd430ea0a/raw/9a9dafafb44d8990f85243a9c7ca349acd3a0d07/worldtilegrid.csv"")

meteorite_wtg <- meteorites %>% 
  group_by(iso3c) %>% 
  summarize(n = n(),
            mass = sum(mass, na.rm = TRUE)/1000) %>%
  mutate(per_meteorite = mass/n) %>% 
  right_join(world_tile_grid, by = c(""iso3c"" = ""alpha.3"")) %>% 
  mutate(text_color = if_else(per_meteorite < 1, ""white"", ""black"")) %>% 
  replace_na(list('alpha.2' = ""NA"",
                  ""text_color"" = ""black"")) 


meteorite_map <- ggplot(meteorite_wtg, aes(x, y, fill = odds, group = iso3c)) +
  geom_tile(color = ""grey30"", size = 0.1) +
  geom_text(aes(label = alpha.2, color = text_color), family = ""Oswald"") +
  labs(x = NULL,
       y = NULL) +
  scale_y_reverse() +
  scale_fill_viridis_c(name = ""Average Metorite Mass (kg, log scale)"",option = ""cividis"", na.value = ""white"", breaks = c(1, 10, 100, 1000, 10000, 100000), guide = guide_colourbar(title.position = ""top"", title.hjust = 0)) +
  scale_color_identity() +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank(),
        legend.direction = ""horizontal"",
        legend.key.width = unit(2, ""lines""),
        legend.position = c(0.2, 0.05))


plot_data <- meteorite_wtg %>%
  select(alpha.2, n) %>% 
  mutate(n = log10(n)) %>% 
  replace_na(list(n = 0)) %>% 
  pack_bars(10, value_column = n, fill_color = last(nord(""lumina"", 5)))


packed_bars <- ggplot(plot_data) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = ""white"", size = 0.1) +
  geom_text(data = filter(plot_data, (xmax - xmin) > 0.1), aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = alpha.2), family = ""Oswald"", color = ""white"") +
  scale_fill_identity() +
  scale_color_identity() +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())


out <- packed_bars + meteorite_map  + plot_annotation(title = ""You May Need More Than An Umbrella in Russia:  Where the Most, and Heaviest, Meteorites fall"",
                                                subtitle = str_wrap(""On the left is a packed bar chart showing the top 10 regions struck by the most meteorites, while the tile map on the right shows the average meteorite mass across all regions.  Both measures have been scaled logathrimically to aid in comparability."", 180),
                                                caption = ""Data: NASA | Graphic: @jakekaupp"",
                                              theme = theme_jk())

ggsave(here(""2019"", ""week24"", ""tw24_plot.png""), out, width = 14, height = 7)
",2019,24
31,31,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week24/R/packed_bars.R,"
pack_bars <- function(data, number_rows, value_column, fill_color = ""#4B384C"", border_color = ""white"") {

value_column <- ensym(value_column)  
  
color_bar_data <- data %>% 
  top_n(number_rows, !!value_column) %>% 
  arrange(desc(!!value_column))

# calc row height based on num rows
bar_h = 1/number_rows

color_bars <- color_bar_data %>% 
  mutate(fill = fill_color,
         color = border_color,
         xmin = 0,
         xmax = !!value_column,
         ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1)),
         ymax = map_dbl(1:number_rows, ~1 - bar_h*.x))

gray_bar_data <- data %>% 
  anti_join(color_bar_data) %>% 
  arrange(desc(!!value_column))

#get max x level for each bar level
row_sums <- pull(color_bar_data, !!value_column)

#gen gray ramp function
gray_gen <- colorRampPalette(c(""#E8E8E8"", ""#cccccc""))

#gen gray ramp
grays <- gray_gen(105)
low_grays <- grays[1:50]
hi_grays  <- grays[56:105]

last_gray <- sample(c(low_grays, hi_grays), number_rows, replace = TRUE)

gray_bar_list <- vector('list', nrow(gray_bar_data))


for (i in 1:nrow(gray_bar_data)) {
  
  row <- gray_bar_data[i,]
  
  # Determine placing of each block by looking at the minium starting values of colored bars
  # adding on the new block and setting value to represent the new block length
  vert_pos <- which.min(row_sums + pull(row, !!value_column))
  
  # Assign alternating random grays to fill
  if (i == 1) {
    
    gray_fill <- sample(low_grays, 1)
    
  } else {
    
    last_gray <- last_gray[vert_pos]
    
    gray_fill <- ifelse(last_gray %in% low_grays, sample(hi_grays, 1), sample(low_grays, 1))
  }
  
  last_gray[vert_pos] <- gray_fill
  
  # Generate aes for geom_rect
  gray_bar_list[[i]]  <- mutate(row, 
                                fill = gray_fill,
                                color = border_color,
                                xmin = row_sums[[vert_pos]],
                                xmax = row_sums[[vert_pos]] + !!value_column,
                                ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1))[[vert_pos]],
                                ymax = map_dbl(1:number_rows, ~1 - bar_h*.x)[[vert_pos]]
  )
  
  # Assign the new color_bar + rectangle as the max value for that row 
  row_sums[[vert_pos]] <- gray_bar_list[[i]]$xmax
  
}

gray_bars <- bind_rows(gray_bar_list)

bind_rows(color_bars, gray_bars)

}

",2019,24
32,32,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week25/R/analysis.R,"library(tidyverse)
library(waffle)
library(jkmisc)
library(here)

bird_counts <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-18/bird_counts.csv"")

top_10 <- bird_counts %>% 
  separate(species_latin, c(""genus"", ""family""), sep = "" "") %>% 
  group_by(genus) %>% 
  summarize(total = sum(how_many_counted_by_hour, na.rm = TRUE)) %>% 
  top_n(10, total) %>% 
  pull(genus)

by_genus <- filter(bird_counts, year >= 2000) %>% 
  separate(species_latin, c(""genus"", ""family""), sep = "" "") %>% 
  filter(genus %in% top_10) %>% 
  group_by(year, genus) %>% 
  summarize(counts_by_hour = sum(how_many_counted_by_hour, na.rm = TRUE))

colors <- set_names(gray.colors(10), top_10)

colors[""Anas""] <- ""#ffd45c""

ducks <- ggplot(by_genus, aes(values = counts_by_hour, fill = genus)) +
  geom_waffle(color = ""white"", size = .25, n_rows = 10, flip = TRUE, show.legend = FALSE) +
  facet_wrap(~year, nrow = 1, strip.position = ""bottom"") +
  coord_equal() +
  labs(title = str_to_title(""The Duck is one of the most noble, agile and intelligent creatures in the animal kingdom.""),
       subtitle = str_wrap(""Total counts per hour, of the top 10 genera from since 2000.  Duck counts (genus Anas) are highlighted in yellow, because if it looks like a duck, and quacks like a duck, we have at least to consider the possibility that we have a small aquatic bird of the family anatidae on our hands."", 120),
       caption = ""Data: www.birdscanada.org/ | Graphic: @jakekaupp"") +
  scale_x_discrete() + 
  scale_y_continuous(labels = function(x) x * 10,
                     expand = c(0,0)) +
  scale_fill_manual(values = colors) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(strip.text = element_text(size = rel(0.8)))

cobra_chicken <- bird_counts %>% 
  filter(year >=1950) %>% 
  group_split(species) %>% 
  map_dfr(~mutate(.x, color = if_else(species == ""Canada Goose"", ""#CB181D"", sample(gray.colors(255), 1)),
                  alpha = if_else(species == ""Canada Goose"", 1, 0.25))) %>%   
  ggplot(aes(x = year, y = how_many_counted_by_hour, group = fct_relevel(species, ""Canada Goose"", after = Inf), fill = color, alpha = alpha), color = ""grey30"") +
  geom_area() +
  scale_y_continuous(expand = c(0.01, 0.1), breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = c(seq(1950, 2010, 10), 2017)) +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL, 
       title = ""Rise Of The Cobra Chicken, the Scourge of the Hamilton Waterfront"",
       subtitle = str_wrap(""The Canada Goose, hilarious and aptly referred to as a 'Cobra Chicken' has been a threat to the delicate ecosystem of Hamilton's Harbor."", 120),
       caption = ""Data: www.birdscanada.org | Graphic: @jakekaupp"") +
  theme_jk(grid = FALSE, ticks = TRUE)
  

ggsave(here(""2019"", ""week25"", ""tw25_ducks.png""), ducks, width = 10, height = 4)
ggsave(here(""2019"", ""week25"", ""tw25_canada_goose.png""), cobra_chicken, width = 10, height = 4)
",2019,25
33,33,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week26/R/analysis.R,"library(sf)
library(albersusa)
library(here)
library(jsonlite)
library(RCurl)
library(janitor)
library(jkmisc)
library(cowplot)
library(tidyverse)



# Get ufo  & pop. density data----
ufo_sightings <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv"")

pop_density <- read_csv(here(""2019"", ""week26"", ""data"", ""pop_density.csv""), skip = 1) %>% 
  clean_names() %>% 
  filter(target_geo_id2 > 1000) %>% 
  mutate(fips = str_pad(target_geo_id2, 5, side = ""left"", pad = ""0"")) %>% 
  select(fips, density = contains(""density""))

# Get alberusa us county sf object----
us_counties <- counties_sf()

# Conver the us ufo sightings to an sf object and join with the alberusa to the the county fips----
usa_sightings_fips <- ufo_sightings %>% 
  filter(country == 'us') %>% 
  st_as_sf(crs = 4326, coords = c(""longitude"", ""latitude"")) 

cont_usa_sightings <- st_join(us_counties, usa_sightings_fips) 



# Some are missing!----
missing <- st_join(usa_sightings_fips, us_counties)  %>% 
  filter(is.na(fips)) %>% 
  semi_join(ufo_sightings,.)

# Make a function to call to the fcc census block API----
geocode_fips <- function(latitude, longitude, index) {
  
  url <- sprintf(""https://geo.fcc.gov/api/census/block/find?latitude=%f&longitude=%f&format=json"",  latitude, longitude)
  
  response <- getURL(url)
  
  json <- fromJSON(response)
  
  print(index)
  
  as.character(json$County['FIPS'])
}

# Make this work insistently----
insistent_geocode <- insistently(~geocode_fips(..1, ..2, ..3), rate = rate_backoff())

# Make it return NA if it fails ----
poss_insistent_geocode <- possibly(~insistent_geocode(..1, ..2, ..3), otherwise = NA_character_)

# Get the missing fips ----

if(!file.exists(here(""2019"", ""week26"", ""data"", ""missing_fips.RDS""))) {
  missing_fips <- missing %>% 
    distinct(latitude, longitude) %>% 
    mutate(index = row_number()) %>% 
    mutate(fips = pmap_chr(list(latitude, longitude, index), poss_insistent_geocode)) } else {
      
      missing_fips <- readRDS(here(""2019"", ""week26"", ""data"", ""missing_fips.RDS""))
      
    }

# Join it back to missing to fill in fips ----
missing <- left_join(missing, missing_fips) %>% 
  dplyr::select(-index) 

# Bind rows back to cont_usa_sightings for full_usa data ----
full_usa <- cont_usa_sightings %>% 
  left_join(missing, by = c(names(ufo_sightings)[c(1:2,4:9)], ""state.x"" = ""state"")) %>% 
  mutate_at(vars(contains(""fips"")), as.character) %>% 
  mutate(fips = coalesce(`fips.x`, `fips.y`)) %>% 
  select(-fips.x, -fips.y, -state_fips, -county_fips, -latitude, -longitude)

# Summarize sightings, create a ratio and add in population densities----
plot_data <- full_usa %>% 
  group_by_at(.vars = vars(fips, name, lsad, census_area, state.y, iso_3166_2)) %>% 
  summarize(sightings = n()) %>% 
  ungroup() %>% 
  mutate(sightings_ratio = 100*sightings/sum(sightings)) %>% 
  left_join(pop_density)


# create 3 buckets for variables ---
quantiles_sightings <- plot_data %>%
  pull(sightings_ratio) %>%
  quantile(probs = seq(0, 1, length.out = 4))

quantiles_density <- plot_data %>%
  pull(density) %>%
  quantile(probs = seq(0, 1, length.out = 4))

# create color scale that encodes two variables
# red for sightings and blue for population density
bivariate_color_scale <- tibble(
  ""3 - 3"" = ""#3F2949"", # high sightings, high density
  ""2 - 3"" = ""#435786"",
  ""1 - 3"" = ""#4885C1"", # low sightings, high density
  ""3 - 2"" = ""#77324C"",
  ""2 - 2"" = ""#806A8A"", # medium sightings, medium density
  ""1 - 2"" = ""#89A1C8"",
  ""3 - 1"" = ""#AE3A4E"", # high sightings, low density
  ""2 - 1"" = ""#BC7C8F"",
  ""1 - 1"" = ""#CABED0"" # low sightings, low density
) %>%
  gather(""group"", ""fill"")


# Assign each fips area to their correct group and assign the fill from the bivariate scale ----
plot_data <- plot_data %>%
  mutate(sightings_quantiles = cut(sightings_ratio,
                              breaks = quantiles_sightings,
                              include.lowest = TRUE),
    density_quantiles = cut(density,
                            breaks = quantiles_density,
                            include.lowest = TRUE),
    group = paste(as.numeric(sightings_quantiles), ""-"", as.numeric(density_quantiles))) %>%
  left_join(bivariate_color_scale, by = ""group"")


# Making ze plot ----
plot <- ggplot(plot_data) +
  geom_sf(aes(fill = fill), size = 0.05, color = ""#2b2b2b"") +
  scale_fill_identity() +
  labs(title = ""If A UFO Flew Over The Desert And No One Was Around To See It, Would Senators Be Briefed?"",
       subtitle = str_wrap(""Below is a bivariate choropleth map by county illustrating the relationship between the UFO sightings (% of recorded sightings since 1911) and population density (people per sq. mile circa 2010).  Densely populated coastal and lakeside areas along with the sparsely populated southwest have the highest sightings, whereas the less populous midwest and Alaska have lower percentages of sightings."", 110),
       caption = ""Data: NUFORC & 2010 US Census | Graphic: @jakekaupp"",
       x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank()) +
  coord_sf(clip = ""off"")

# Making ze legend ---
bivariate_legend <- bivariate_color_scale %>% 
  separate(group, into = c(""sightings"", ""density""), sep = "" - "") %>%
  mutate_at(c(""sightings"", ""density""), as.integer)

legend <- ggplot(bivariate_legend) +
  geom_tile( aes(x = sightings, y = density, fill = fill)) +
  scale_fill_identity() +
  labs(x = expression(paste(""More Sightings "", symbol('\256'))),
       y = expression(paste(""More People "", symbol('\256')))) +
  theme_jk(grid = FALSE) +
  theme(axis.title = element_text(size = 6),
        axis.text = element_blank()) +
  coord_fixed(clip = ""off"")

finished_plot <- ggdraw() +
  draw_plot(plot, 0, 0, 1, 1) +
  draw_plot(legend, 0.75, 0.075, 0.2, 0.2)


ggsave(here(""2019"", ""week26"", ""tw26_plot.png""), plot = finished_plot, width = 10, height = 6)
",2019,26
34,34,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week28/R/analysis.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(glue)


squads <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/squads.csv"")


data <- squads %>% 
  mutate(idx = goals/caps) %>% 
  filter(pos != ""GK"", caps > 0, goals > 0) %>% 
  mutate(pos = case_when(pos == ""DF"" ~ ""Defense"",
                         pos == ""FW"" ~ ""Forward"",
                         pos == ""MF"" ~ ""Mid-Field"")) %>% 
  mutate(desc = glue(""{club}\n{caps} Matches, {goals} Goals"")) %>% 
  mutate(pos = factor(pos, c(""Defense"", ""Mid-Field"", ""Forward"")))

means <- data %>% 
  group_by(pos) %>% 
  summarize(idx = mean(idx))

plot <- ggplot(data, aes(x = age, y = idx)) +
  geom_point(color = ""grey20"") +
  geom_mark_circle(aes(label = player, description = desc, filter = player == ""Khadija Shaw""), expand = unit(4, ""mm"")) +
  geom_mark_circle(aes(label = player, description = desc, filter = player  == ""Lea Schller""), expand = unit(4, ""mm"")) +
  geom_mark_circle(aes(label = player, description = desc, filter = player  == ""Ainon Phancha""), expand = unit(4, ""mm"")) +
  geom_hline(data = means, aes(yintercept = idx), color = ""firebrick"") +
  theme_jk() +
  facet_wrap(~pos, nrow = 1) +
  labs(y = NULL,
       x = ""Age"",
       title = ""Efficient Scorers Competing in the Womens World Cup by Position and Age"",
       subtitle = str_wrap(""Goals per games played in international play by player age.  Red line illustrates the average goals per game at each position.  The highly efficient players at each position are a mix of newcomers and seasoned veterans, illustrating consistency in some players through their career."", 120),
       caption = ""Data: data.world | Graphic : @jakekaupp"")

ggsave(here(""2019"", ""week28"", ""tw28_plot.png""), plot = plot, width = 10, height = 6)

",2019,28
35,35,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week29/R/analysis.R,"library(tidyverse)
library(tricolore)
library(ggtern)
library(here)
library(jkmisc)
library(magick)

r4ds_members <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-16/r4ds_members.csv"")


tern_plot <- Tricolore(r4ds_members, ""percent_of_messages_public_channels"",
          ""percent_of_messages_private_channels"",
          ""percent_of_messages_d_ms"", breaks = 5, show_data = FALSE)

legend <- tern_plot$key +
  labs(title = ""Color Legend"",
       x       = ""Public\nChannels"",
       y       = ""Private\nChannels"",
       z       = ""Direct\nMessages"") +
  theme_hidetitles() +
  theme_hidelabels() +
  theme_hideticks() +
  theme(plot.title = element_text(hjust = 0.5, family = ""Scope One"", size = 40),
        axis.text = element_text(family = ""Scope One""),
        axis.title = element_text(family = ""Scope One""))

png(here(""2019"", ""week29"", ""legend.png"")) 
legend
dev.off()

legend <- image_read(here(""2019"", ""week29"", ""legend.png""))

plot <- r4ds_members %>% 
  mutate(color = tern_plot$rgb,
         year = lubridate::year(date)) %>% 
  ggtern(aes(x = percent_of_messages_public_channels, y = percent_of_messages_private_channels, z = percent_of_messages_d_ms, color = color)) +
  geom_point(size = 3) +
  scale_color_identity() +
  labs(title = str_to_title(""The Dialogue in the R4DS Slack indicates an Open and Inclusive Learning Community""),
       subtitle = str_wrap(""Below is a ternary digram presenting the message composition in public channels, private channels and direct messages as a percentage.  Each day is represented by a point with the composition represented by position relative to each axes.  Composition is additionally encoded by color as illustrated on the inset legend."", 100),
       x       = ""Public\nChannels"",
       xarrow  = ""More Public Channel Messages"",
       y       = ""Private\nChannels"",
       yarrow  = ""More Private Channel Messages"",
       z       = ""Direct\nMessages"",
       zarrow  = ""More Direct Messages"",
       caption = ""Data: R4DS Community | Graphic: @jakekaupp"") +
  theme(panel.background = element_rect(fill = ""#2E3440""),
        panel.grid = element_line(color = ""#ffffff"", size = 0.1),
        panel.grid.minor = element_blank(),
        text = element_text(family = ""Oswald""),
        plot.subtitle = element_text(family = ""Scope One""),
        axis.text = element_text(family = ""Scope One""),
        axis.title = element_text(family = ""Scope One"")) +
  theme_showarrows() +
  theme_arrowlong() 


png(here(""2019"", ""week29"", ""tw29_plot.png""), width = 10, height = 8, units = ""in"", res = 200)
grid::grid.newpage()
plot
grid::grid.raster(legend, width = 0.18, height = 0.2, x = 0.75, y = 0.7)
dev.off()
",2019,29
36,36,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week3/R/analysis.R,"library(tidyverse)
library(here)
library(nord)
library(jkmisc)
library(ggbeeswarm)
library(ggrepel)

agencies <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-15/agencies.csv"")

launches <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-15/launches.csv"")


us_launch_data <- launches %>% 
  filter(agency == ""US"" | state_code == ""US"") %>% 
  mutate(type = gsub(""Zenit-"", ""Zenit "", type),
         type = gsub(""/"", "" "", type),
         type = gsub(""Minotaur-"", ""Minotaur "", type)) %>% 
  separate(type, ""type"", sep = "" "", extra = ""drop"") %>% 
  mutate(type = if_else(type == ""Space"", ""Space Shuttle"", sprintf(""%s Program"",type))) %>% 
  mutate(label = if_else(type == ""Space Shuttle"" & category == ""F"", ""Challenger Disaster"", NA_character_)) %>% 
  group_by(type) %>% 
  filter(n() > 10)

plot <- ggplot(us_launch_data, aes(x = launch_year, y = type), size = 4) +
  geom_quasirandom(data = filter(us_launch_data, category == ""O""), alpha = 0.2, fill = nord(""polarnight"", 2)[2], shape = 21, groupOnX = FALSE) +
  geom_quasirandom(data = filter(us_launch_data, category == ""F""), fill = nord(""victory_bonds"", 5)[1], shape = 21, groupOnX = FALSE, color = ""grey30"", stroke = 0.2) +
  theme_jk(grid = ""XY"", dark = FALSE) +
  labs(x = NULL,
       y = NULL,
       title = ""From the Space Race to Space-X: 1548 Successes and 101 Failures of US Launch Vehicles from 1958-2018."",
       subtitle = str_wrap(""A beeswarm plot illustrating the success or failure of a launch vehicle program over time. Red dots indicate failed launches, grey dots indicate success.  Deeper grey colors indicate a higher frequency of success in a given year due to multiple launches. Only includes programs with more than 10 launches"", 120),
       caption = ""Data: JSR Launch Vehicle Database | Analysis: @jakekaupp"")

plot <- plot + annotate(""segment"", x = 1987, xend = 1986.2, y = 8.7, yend = 8.2, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 1986, y = 9, label = ""Challenger Disaster"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1]) +
  annotate(""segment"", x = 1959, xend = 1958.2, y = 1.7, yend = 1.2, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 1959, y = 2.5, label = ""First Communication\nSatellite Protoype"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1], hjust = 0) +
  annotate(""segment"", x = 2015, xend = 2015, y = 5.5, yend = 3.1, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 2013, y = 6.5, label = ""SpaceX Falcon 9\nStrut Failure"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1], hjust = 0)

ggsave(here(""2019"", ""week3"", ""tt_week3.png""), plot, width = 11, height = 5)

  ",2019,3
37,37,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week30/R/analysis.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(geofacet)
library(patchwork)
library(glue)
library(cowplot)

wildlife_impacts <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-23/wildlife_impacts.csv"")


# Petal charts ----
plot_data <- wildlife_impacts %>% 
  #mutate(airport_type = if_else(str_detect(airport, ""INTL""), ""INT"", ""DOM"")) %>% 
  filter(state %in% state.abb) %>% 
  count(state,  incident_month, incident_year) %>% 
  complete(incident_year = 1990:2018, state,  incident_month = 1:12, fill = list(n = 0)) %>% 
  group_by(incident_year, state) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))


state_flower_grid <- plot_data %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_area(position = position_identity(), alpha = 0.5, size = 0.1) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_c(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  labs(x = NULL, 
       y = NULL) +
  coord_polar() +
  facet_geo(~ state, grid = ""us_state_grid2"") +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines""))

flower_legend <- plot_data %>% 
  filter(state == ""ME"") %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_area(position = position_identity(), alpha = 0.5, size = 0.1) +
  geom_mark_circle(aes(label = glue(""{month.name[incident_month]}""), description = ""Single colour petal represents a single collison event during this month"", filter = incident_year == 1991 & incident_month == 3), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm"")) +
  geom_mark_circle(aes(label = glue(""{month.name[incident_month]}""), description = ""Multiple coloured petals represent repeated annual incidents during this month"", filter = incident_year == 1996 & incident_month == 11), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm"")) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  scale_y_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  scale_fill_viridis_c(""Year"", option = ""plasma"", direction = 1, breaks = c(seq(1990, 2020, by = 5))) +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  guides(fill = guide_colorbar(), color = ""none"") +
  labs(x = NULL, y = NULL) +
  coord_polar(clip = ""off"") +
  theme_jk(grid = ""XY"") +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

color_legend <- tibble(year = 1990:2018,
                       y = 1) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = y, fill = year), show.legend = FALSE, color = ""white"", size = 0.1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  scale_x_continuous(breaks = c(1990, 2000, 2010, 2018)) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  labs(x = NULL, y = NULL) +
  theme(axis.text.y = element_blank())

finished_legend <- ggdraw() +
  draw_plot(flower_legend, 0, 0, 1, 1) +
  draw_plot(color_legend, 0.3, -0.17, 0.4, 0.4)

out <- wrap_plots(finished_legend, state_flower_grid,  nrow = 1, widths = c(0.85, 1.2)) +
   plot_annotation(title = ""Seasonality of Wildlife-Aircraft Collisions by State"",
                   subtitle = str_wrap(""Presented below is a petal chart of of wildlife collisions with aircraft, with an inset legend showing assisting interpretation.  Wildlife collisions by state are presented as small multiples, geographically arranged.  Smaller compact flowers illustrate states with collisions occuring year round, while the bigger flowers tend to see single or concentrated spikes of collision activity.  Flowers with diverse colours indicate repeated annual collisons while the single-hued flowers illustrate more sparse or isolated annual events."", 210),
                   caption = ""Data: FAA Wildlife Strike Database | Graphic: @jakekaupp"",
                   theme = theme_jk())


ggsave(here(""2019"",""week30"", ""tw30_plot.png""), out, width = 16, height = 10, type = ""cairo"")


",2019,30
38,38,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week30/R/experiment.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(geofacet)
library(patchwork)
library(glue)
library(cowplot)

wildlife_impacts <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-23/wildlife_impacts.csv"")


# Petal charts ----
plot_data <- wildlife_impacts %>% 
  filter(state %in% state.abb) %>% 
  count(state,  incident_month, incident_year) %>% 
  group_by(incident_year, state) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent)) %>% 
  mutate(angle = 90 - (incident_month-1)*30,
         angle = ifelse(angle < 0, 360 + angle, angle),
         radians = angle*pi/180,
         x0 = percent * cos(radians),
         y0 = percent * sin(radians))
         
         
big_flower <- ggplot(plot_data) +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = percent, b = percent/3, angle = radians, fill = incident_year), alpha = 0.2, size = 0, color = ""white"") +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  theme_jk(grid = FALSE, plot_title_size = 12) +
  labs(x = NULL, y = NULL, title = ""National"") +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines""),
        plot.title = element_text(hjust = 0.5)) +
  coord_equal() 

state_flower <- big_flower +
  facet_geo(~ state, grid = ""us_state_grid2"")


flower_axes_lines <- tibble(idx = 1:12,
                            angle = 90 - (idx-1)*30,
                            angle2 = ifelse(angle < 0, 360 + angle, angle),
                            radians = angle2*pi/180)

axes_lines <- function(radius) {
  
  tibble(segment = 1:6,
                     x = c(0, radius*cos(pi/3), radius*cos(pi/6), radius, radius*cos(pi/6), radius*cos(pi/3)),
                     xend = c(0, -radius*cos(pi/3), -radius*cos(pi/6), -radius, -radius*cos(pi/6), -radius*cos(pi/3)),
                     y = c(radius, radius*sin(pi/3), radius*sin(pi/6), 0, -radius*sin(pi/6), -radius*sin(pi/3)),
                     yend = c(-radius, -radius*sin(pi/3), -radius*sin(pi/6), 0, radius*sin(pi/6), radius*sin(pi/3))) 
  }

axes_labels <- function(radius) {
  tibble(month = 1:12,
                      label = month.abb[month],
                      x = c(axes_lines(radius)$x, axes_lines(radius)$xend),
                      y = c(axes_lines(radius)$y, axes_lines(radius)$yend))  }


flower_legend <- plot_data %>% 
  filter(state == ""ME"") %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_segment(data = axes_lines(2), aes(x = x, xend = xend, y = y , yend = yend), size = 0.1, color = ""#cccccc"", inherit.aes = FALSE) +
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), inherit.aes = FALSE, size = 0.1, color = ""#cccccc"") +
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE, size = 0.1, color = ""#cccccc"") +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = percent, b = percent/3, angle = radians, fill = incident_year), alpha = 0.5, size = 0.1, color = ""white"") +
  geom_mark_circle(aes(x = 2*x0, y = 2*y0, label = glue(""{month.name[incident_month]}, {incident_year}""), description = ""Single colour long petal represents 100% of collison event during this month and year"", filter = incident_year == 1991 & incident_month == 3), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm""), inherit.aes = FALSE) +
  geom_mark_circle(aes(x = x0, y = y0, label = glue(""{month.name[incident_month]}, Multiple years""), description = ""Multiple coloured petals represent repeated annual incidents during this month"", filter = incident_year == 1996 & incident_month == 11), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm""), inherit.aes = FALSE) +
  geom_text(data = filter(axes_labels(2.15), label != ""Feb""), aes(x = x, y = y, label = label), inherit.aes = FALSE, family = ""Oswald"") +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  labs(x = NULL, y = NULL) +
  theme_jk(grid = FALSE) +
  coord_fixed(clip = ""off"") +
  theme(axis.text = element_blank(),
        legend.position = ""none"")

color_legend <- tibble(year = 1990:2018,
                       y = 1) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = y, fill = year), show.legend = FALSE, color = ""white"", size = 0.1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  scale_x_continuous(breaks = c(1990, 2000, 2010, 2018)) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  labs(x = NULL, y = NULL) +
  theme(axis.text.y = element_blank())


finished_legend <- ggdraw() +
  draw_plot(flower_legend, 0, 0, 1, 1) +
  draw_plot(color_legend, 0.3, -0.175, 0.4, 0.4)

state_flower_grid <- ggdraw() +
  draw_plot(state_flower, 0, 0, 1, 1) + 
  draw_plot(big_flower, 0.75, 0.15, 0.25, 0.25)

out <- wrap_plots(finished_legend, state_flower_grid, nrow = 1, widths = c(0.85, 1.2)) +
  plot_annotation(title = ""Seasonality of Wildlife-Aircraft Collisions by State"",
                  subtitle = str_wrap(""Presented below is a petal chart of of wildlife collisions with aircraft across the US from 1990-2018. Below this is an inset legend showing assisting interpretation of the plots.  On the right are wildlife-aircraft collisions by state presented as small multiples, geographically arranged, with an inset flower representing the National data. Petal length is the annual proportion of collisions in a given month.  Smaller compact flowers illustrate states with collisions occuring year round, while the bigger flowers tend to see single or concentrated spikes of collision activity.  Flowers with diverse colours indicate repeated annual collisons while the single-hued flowers illustrate more sparse or isolated annual events."", 210),
                  caption = ""Data: FAA Wildlife Strike Database | Graphic: @jakekaupp"",
                  theme = theme_jk())


ggsave(here(""2019"",""week30"", ""tw30_plot_remix.png""), out, width = 16, height = 10, type = ""cairo"")
",2019,30
39,39,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week31/R/analysis.R,"library(tidyverse)
library(here)
library(ggbeeswarm)
library(jkmisc)

video_games <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-30/video_games.csv"")  
 
all_games <- video_games %>% 
  filter(!is.na(game), !is.na(metascore))  %>% 
  mutate(developer = tolower(developer),
         idx = row_number())

plot_data <- tibble(facet = c(""bioware"", ""valve"", ""ubisoft"", ""rockstar"", ""square enix""),
       data = list(all_games)) %>% 
  mutate(filtered = map2(data, facet, ~mutate(.x, option = case_when(str_detect(tolower(developer), .y) ~ ""selected"", 
                                                                     TRUE ~ ""other"")))) %>% 
  unnest(filtered) %>% 
  mutate(facet = case_when(facet == ""bioware"" ~ ""BioWare"",
                           facet == ""valve"" ~ ""Valve"",
                           facet == ""ubisoft"" ~ ""Ubisoft"",
                           facet == ""rockstar"" ~ ""Rockstar"",
                           facet == ""square enix"" ~ ""Square Enix"")) %>% 
  mutate(option = factor(option, c(""other"", ""selected""))) %>% 
  arrange(facet, option)



mean <- all_games %>% 
  summarize(metascore = mean(metascore, na.rm = TRUE)) %>% 
  pull(metascore)

min_labels <- plot_data %>% 
  filter(option == ""selected"", !is.na(metascore)) %>% 
  group_by(facet) %>% 
  filter(metascore == min(metascore))

max_labels <- plot_data %>% 
  filter(option == ""selected"", !is.na(metascore)) %>% 
  group_by(facet) %>% 
  filter(metascore == max(metascore)) %>% 
  mutate(game = if_else(str_detect(""FINAL FANTASY"", game), ""Final Fantasy IX"", game)) %>% 
  slice(1)

plot <- ggplot(plot_data) +
  geom_quasirandom(aes(y = metascore, x = 0, alpha = option, fill = option, size = option), shape = 21, method = ""tukey"", show.legend = FALSE) +
  geom_label(data = min_labels, aes(x = 0, y = metascore, label = game), family = ""Oswald"", nudge_y = -2, fill = ""#E5E4E2"", label.r = unit(0, ""lines""), alpha = 0.5) +
  geom_label(data = max_labels, aes(x = 0, y = metascore, label = game), family = ""Oswald"", nudge_y = +2, fill = ""#E5E4E2"", label.r = unit(0, ""lines""), alpha = 0.5) +
  geom_hline(yintercept = mean, color = ""firebrick"", size = 0.5, linetype = ""dashed"") +
  labs(x = NULL, 
       y = NULL,
       title = ""How Do The Big Developers Score Against The Competition on Steam?"",
       subtitle = str_wrap(""Presented below is a jittered strip plot of metascore by developer.  Titles worked on by that developer are highlighted in yellow, the average metascore (72) is shown as a dashed red line. Annotations show the top an bottom rated titles for each developer.  Sqaure and Ubisoft have the most titles with less than average reviews amongst the large developers."", 180),
       caption = ""Data: SteamSpy | Graphic: @jakekaupp"") +
  scale_size_manual(values = c(""other"" = 2, ""selected"" = 2)) +
  scale_y_continuous(limits = c(20, 100), breaks = seq(20, 100, 20)) +
  scale_fill_manual(values = c(""other"" = ""#E5E4E2"", ""selected"" = ""#ffd644"")) +
  scale_alpha_manual(values = c(""other"" = 0.05, ""selected"" = 1)) +
  theme_jk(grid = ""Y"", dark = TRUE) +
  facet_wrap(~facet, nrow = 1) +
  theme(strip.text = element_text(color = ""white""),
      axis.text.x = element_blank())

ggsave(here(""2019"", ""week31"", ""tw31_plot.png""), plot, width = 14, height = 8)
",2019,31
40,40,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week32/R/analysis.R,"library(tidyverse)
library(ggforce)
library(here)
library(jkmisc)
library(patchwork)

bob_ross_paintings <- here(""2019"", ""week32"", ""data"", ""tidytuesday_201932_bob_ross_paintings.csv"")

data <- read_csv(bob_ross_paintings, col_names = c('episode', 'title', 'color', 'color_name')) %>% 
  mutate(season = parse_number(str_extract(episode, ""S\\d+"")),
         color = if_else(color == ""#FFFFFF"", ""grey80"", color))


plot_data <- data %>% 
  count(season, title, color_name, color) %>% 
  group_by(season, title) %>% 
  mutate(percent = n/sum(n)) %>% 
  ungroup() %>% 
  mutate(color_number = as.numeric(factor(color_name))) %>% 
  mutate(angle = (color_number-1)*(360/15),
         angle = ifelse(angle < 0, 360 + angle, angle),
         radians = angle*pi/180,
         x0 = percent * cos(radians),
         y0 = percent * sin(radians))
  

plot_spiros <- function(data) {
  
season <- sprintf(""Season %s"",unique(data$season))
  
ggplot(data) +
  geom_spiro(aes(R = ifelse(percent == 1, 0.1, 1 - percent), r = percent, d = radians, color = color, group = color), size = 0.1) +
  scale_color_identity() +
  theme_jk(grid = FALSE, plot_title_size = 8, strip_text_size = 8) +
  facet_wrap(~ title, ncol = 1, labeller = label_wrap_gen(15)) +
  labs(x = NULL, y = NULL, title = season) +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines"")) +
  coord_equal()  }


plots <- plot_data %>% 
  split(.$season) %>% 
  map(plot_spiros)


all_seasons <- wrap_plots(plots, nrow = 1) + plot_annotation(title = ""Happy Accidents with 1960s Toys: Sprirographs of Palette Colors of Bob Ross Paintings for 31 Seaons"",
                                                             subtitle = ""Illustrated below is a spirograph tracing of the 15 distinct un-mixed palette colours used in each of Bob Ross' paintings.  The more colours used in a painting, the larger the spirograph and it appears similar to china pattern while those paintings with a more minimalist palette show up as smaller sparse rings."",
                                                             caption = ""Data: c/o @geokaramanis | Graphic: @jakekaupp"",
                                                             theme = theme_jk())


ggsave(filename = here(""2019"", ""week32"", ""tw32_plot.png""), plot = all_seasons, width = 30, height = 15, type = ""cairo"")

twitter <- map(plots, ~.x + theme(strip.text = element_blank()))


all_seasons_twitter <- wrap_plots(twitter, nrow = 1) + plot_annotation(title = ""Happy Accidents with 1960s Toys: Sprirographs of Palette Colors of Bob Ross Paintings for 31 Seaons"",
                                                             subtitle = str_wrap(""Illustrated below is a spirograph tracing of the 15 distinct un-mixed palette colours used in each of Bob Ross' paintings.  The more colours used in a painting, the larger the spirograph and it appears similar to china pattern while those paintings with a more minimalist palette show up as smaller sparse rings."", 265),
                                                             caption = ""Data: c/o @geokaramanis | Graphic: @jakekaupp"",
                                                             theme = theme_jk())


ggsave(filename = here(""2019"", ""week32"", ""tw32_plot_for_twitter.png""), plot = all_seasons_twitter, width = 20, height = 7)


",2019,32
41,41,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week33/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(lubridate)
library(here)
library(patchwork)


emperors <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv"") 

ad_births <- c(""Augustus"", ""Tiberius"", ""Claudius"", ""Galba"")

emp_numeric_years <- emperors %>% 
  mutate_if(is.Date, list(year = year)) %>% 
  mutate(birth_year = if_else(name %in% ad_births, -birth_year, birth_year),
         reign_start_year = if_else(name == ""Augustus"", -reign_start_year, reign_start_year))


missing_birth_estimates <- emp_numeric_years %>% 
  filter(is.na(birth_year)) %>% 
  mutate(birth_year = case_when(name == ""Florian"" ~ 202,
                                name == ""Numerian"" ~ 248,
                                name == ""Carinus"" ~ 245,
                                name == ""Severus II"" ~ 260,
                                name == ""Vetranio"" ~ 325))


plot_data <- emp_numeric_years %>% 
  filter(!is.na(birth_year)) %>% 
  bind_rows(missing_birth_estimates)


dynasties <- plot_data %>% 
  group_by(dynasty) %>% 
  summarize(reign_start_year = min(reign_start_year),
               reign_end_year = max(reign_end_year))

roman_palette <- set_names(colorRampPalette(c(""#191970"", ""#FF7F50""))(8), unique(plot_data$dynasty))


overall <- ggplot(plot_data, aes(y = 0)) +
  geom_segment(aes(x = reign_start_year, xend = reign_end_year, yend = 0, color = dynasty), size = 4) +
  scale_color_manual(""Dynasty"", values = roman_palette, breaks = names(roman_palette)) +
  scale_x_continuous(breaks = c(-62, 0, 100, 200, 300, 400), labels = c(""62 BC"", ""1 AD"", ""100 AD"", ""200 AD"", ""300 AD"", ""400 AD"")) +
  expand_limits(x = c(-62, 450)) +
  labs(x = NULL, y = NULL,
       caption = ""Data: Wikipedia via @geokaramanis | Graphic: @jakekaupp"") +
  theme_jk(grid = ""X"") +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

bars <- ggplot(plot_data, aes(y = reorder(name, reign_start_year))) +
  geom_segment(aes(x = birth_year, xend = death_year, yend = name), size = 2, color = ""grey90"") +
  geom_segment(aes(x = reign_start_year, xend = reign_end_year, yend = name, color = dynasty), size = 2) +
  geom_segment(data = filter(plot_data, reign_start_year == reign_end_year), aes(x = reign_start_year - 0.5, xend = reign_start_year + 0.5, y = name, yend = name, color = dynasty), size = 2) +
  geom_text(aes(x = death_year, label = name), hjust = 0, family = ""Scope One"", size = 2, nudge_x = 3) +
  scale_color_manual(""Dynasty"", values = roman_palette, breaks = names(roman_palette)) +
  scale_x_continuous(breaks = c(-62, 0, 100, 200, 300, 400), labels = c(""62 BC"", ""1 AD"", ""100 AD"", ""200 AD"", ""300 AD"", ""400 AD"")) +
  expand_limits(x = c(-62, 450)) +
  labs(x = NULL, 
       y = NULL,
       title = str_to_title(""When in Rome: The Game of Imperial Thrones. You Win or You Die.""),
       subtitle = str_wrap(""Illustrated below is a timeline of the life and reigns of Roman Emperors from 62 BC to 395 AD.  The light grey bar depicts the liftime of the emperor, the colored bar (by dynasty) indicates the duration of their reign. An overall timeline by dynasty is shown near the horizontal axis. Unsurprisingly, the majority of emperors reign ending also coincides with the end of their life."", 100)) +
  theme_jk(grid = ""X"") +
  theme(axis.text = element_blank(),
        legend.position = c(0.2, 0.7),
        legend.background = element_rect(fill = ""white"", size = 0))


out <- wrap_plots(bars, overall, ncol = 1, heights = c(0.9, 0.1))

ggsave(here(""2019"", ""week33"", ""tw33_plot.png""), out, width = 7.5, height = 8)
",2019,33
42,42,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week34/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(ggtext)
library(jkmisc)
library(waffle)
library(ggforce)
library(glue)
library(ragg)

nuclear_explosions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-20/nuclear_explosions.csv"")




plot_data <- nuclear_explosions %>% 
  mutate(date_long = ymd(date_long)) %>% 
  group_by(country, date_long) %>% 
  summarize(n = n(),
            total_yield = sum(yield_upper, na.rm = TRUE)) %>% 
  group_by(country) %>% 
  mutate(c_sum = cumsum(n),
         c_yield = cumsum(total_yield)/1000) %>% 
  filter(country %in% c(""USSR"", ""USA""))

items <- nuclear_explosions %>% 
  mutate(date_long = ymd(date_long)) %>% 
  filter(country %in% c(""USSR"", ""USA"")) %>% 
  group_by(country) %>% 
  top_n(1, yield_upper) %>% 
  slice(1) %>%
  semi_join(plot_data, .) %>% 
  mutate(date_long = ymd(date_long) - 1) %>% 
  mutate(name = if_else(country == ""USA"", ""March 1954: Castle Bravo"", ""October 1961: Tsar Bomba""),
         description  = if_else(country == ""USA"", ""2nd most powerful nuclear test explosion, 3 times over the predicted 5 MT yield."", ""Most powerful nuclear test explosion, twice the predicted yield of 25 MT.""))

explosions <- ggplot(plot_data, aes(x = c_sum, y = c_yield, color = country)) +
  geom_step(linetype = ""solid"", size = 1, direction = ""hv"") +
  geom_point(data = filter(plot_data, date_long %in% range(date_long))) +
  geom_text(data = filter(plot_data, date_long == last(date_long)), aes(label = year(ymd(date_long))), family = ""Oswald"", hjust = -0.5) +
  geom_text(data = filter(plot_data, date_long == first(date_long)), aes(label = year(ymd(date_long))), family = ""Oswald"", nudge_y = c(10, -10)) +
  geom_mark_circle(data = items, aes(color = country, label = name, description = description), expand = unit(3, ""mm""), label.margin = margin(5, 5, 5, 5, ""mm""), con.colour = c(""#0052A5"", ""#FF2400""), label.family = c(""Oswald"",""Scope One""), label.fill = NA, label.minwidth = unit(50, ""mm""), label.fontsize = 10, con.type = ""straight"") +
  scale_color_manual(values = c('USSR' = ""#FF2400"", ""USA"" = ""#0052A5"")) +
  scale_fill_manual(values = c('USSR' = ""#FF2400"", ""USA"" = ""#0052A5"")) +
  scale_y_continuous(labels = function(x) scales::comma(x, suffix = "" MT"")) +
  labs(x = 'Cumulative Number of Explosions',
       y = ""Cumulative Yield (MT)"",
       title = ""Nuclear Weapons Research Race During And After The Cold War"",
       subtitle = ""Illustrated below is a step chart showing the number and yield of nuclear explosions for weapons research for <span style='color:#0052A5'>**USA**</span> and <span style='color:#FF2400'>**USSR**</span>.  During this race nearly<br>500 MT of nuclear explosions and accompanying fallout blanketed the world. The effects are still being dealt with to this date."",
       caption = ""Data: Our World in Data | Graphic: @jakekaupp"") +
  theme_jk(subtitle_family = ""PT Serif"",
           caption_family = ""PT Serif"") +
  theme(plot.title = element_markdown(), 
        plot.subtitle = element_markdown(),
        legend.position = ""none"")
  
ggsave(here(""2019"", ""week34"", ""tw34_plot_2.png""), plot = explosions, width = 12, height = 6, device = agg_png())


# Waffle ----

waffle_data <- nuclear_explosions %>% 
  mutate(purpose = case_when(grepl(""WR"", purpose) ~ ""WR"",
                             grepl(""WE"", purpose) ~ ""WE"",
                             grepl(""PNE"", purpose) ~ ""PNE"",
                             TRUE ~ purpose)) %>% 
  filter(country %in% c(""USSR"", ""USA"")) %>% 
  count(year, purpose) %>% 
  group_by(purpose) %>% 
  mutate(c_sum = cumsum(n))


pal <- set_names(sample(grey.colors(50),10), unique(waffle_data$purpose))

pal[""WR""] <- ""#8A0303""

waffle <- ggplot(waffle_data, aes(fill = purpose, values = c_sum)) +
  geom_waffle(color = ""white"", size = .25, n_rows = 10, flip = TRUE, show.legend = FALSE) +
  facet_wrap(~year, strip.position = ""bottom"", nrow = 1) +
  scale_x_discrete() + 
  scale_y_continuous(labels = function(x) x * 10,
                     expand = c(0,0)) +
  coord_equal() +
  scale_fill_manual(values = pal) +
  labs(y = ""Cumulative Number of Explosions"",
       title = ""Nuclear Weapons Research Testing During the Cold War Was the Primary Driver of Controlled  Nuclear Explosions"",
       subtitle = ""Illustrated below is a timeline of waffle charts showing the distribution of cumulative explosions from <span style='color:#8A0303'>**weapons research**</span> or <span style='color:#4D4D4D'>**other purposes**</span>. Widescale Nuclear testing ceased in the mid-'90's. The only active country<br>conducting nuclear testing in this era is North Korea, weathering the disapproval and ire of the global community."",
       caption = ""Data: Our World in Data | Graphic: @jakekaupp"") +
  theme_jk(strip_text_size = 10,
           subtitle_family = ""PT Serif"",
           caption_family = ""PT Serif"") +
  theme(panel.grid = element_blank(), 
        axis.ticks.y = element_line(),
        strip.text = element_text(hjust = 0.5),
        plot.title = element_markdown(), 
        plot.subtitle = element_markdown())
  
ggsave(here(""2019"", ""week34"", ""tw34_plot.png""), plot = waffle, width = 18, height = 6, device = agg_png())
",2019,34
43,43,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week35/R/analysis.R,"library(tidyverse)
library(tidygraph)
library(ggraph)
library(colorspace)
library(glue)
library(jkmisc)
library(ggtext)
library(ragg)
library(here)


html_string <- glue(""Shown below is a co-occurance network of guest stars in The Simpsons, best explained as a 'Who co-stars together?'. {subtitle_names} are the most frequent guest stars in the series."")

str_break <- function (html_string, width = 80, indent = 0, exdent = 0) {

tags <- str_extract_all(html_string, ""<.*?>"") %>% 
  flatten_chr() 

index <- sprintf(""tag_%s"", seq_along(tags))

string <- str_replace_all(html_string, set_names(index, tags))

  if (width <= 0) 
    width <- 1
  
  out <- stringi::stri_wrap(string, width = width, indent = indent, 
                   exdent = exdent, simplify = FALSE)
  
  broken <- vapply(out, str_c, collapse = ""<br>"", character(1))
  
  str_replace_all(broken, set_names(tags, index))
  
}

text_bc <- function(text, color) {
  
  glue(""<span style = color:{color}>**{text}**</span>"")
  
}


simpsons <- read_delim(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-27/simpsons-guests.csv"", delim = ""|"", quote = """")

nodes <- count(simpsons, guest_star) 

top5 <- top_n(nodes, 5, n) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column(var = ""color"") %>% 
  select(-n)

nodes <- nodes %>% 
  left_join(top5) %>% 
  replace_na(list(color = 7)) %>% 
  mutate(guest_star = str_remove(guest_star, ""'"")) %>% 
  mutate(alpha = if_else(color < 7, 1, 0.5)) 


edges <- simpsons %>% 
  mutate(guest_star = str_remove(guest_star, ""'"")) %>% 
  group_by(production_code) %>% 
  mutate(co_stars = map(guest_star, ~str_subset(guest_star, .x, negate = TRUE))) %>%
  ungroup() %>% 
  mutate(co_stars = map(co_stars, ~ifelse(length(.x) == 0, NA_character_,.x))) %>% 
  unnest(co_stars) %>% 
  count(guest_star, co_stars) %>% 
  filter(!is.na(n), !is.na(co_stars)) %>% 
  set_names(c(""from"", ""to"", ""n"")) %>% 
  left_join(top5, by = c(""from"" = ""guest_star"")) %>% 
  left_join(top5, by = c(""to"" = ""guest_star"")) %>% 
  mutate(color = coalesce(color.x, color.y)) %>% 
  replace_na(list(color = ""grey80"")) 

  
colors <- set_names(tol6qualitative, top5$guest_star)  

subtitle_names <- imap(colors[1:5], ~text_bc(.y, .x)) %>% 
  glue_collapse(sep = ', ') %>% 
  glue("" and {imap(colors[6], ~text_bc(.y, .x))}"")

co_star_graph <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)

co_star_plot <- co_star_graph %>% 
  activate(nodes) %>% 
  arrange(n) %>% 
  mutate(degree = centrality_degree()) %>% 
  filter(degree > 1) %>%  
  ggraph(layout = ""fr"") + 
  geom_edge_arc(edge_width = 0.5, curvature = 0.2, aes(alpha = stat(index), edge_colour = color)) +
  geom_node_point(aes(size = n, color = color, alpha = alpha, fill = color), shape = 21) +
  scale_color_manual(values = c(darken(tol6qualitative), ""grey80"")) + 
  scale_alpha_identity() +
  scale_edge_color_manual(values = c(tol6qualitative, ""grey85"")) +
  scale_fill_manual(values = c(tol6qualitative, ""grey80"")) + 
  scale_size(range = c(2,6)) +
  labs(x = NULL,
       y = NULL,
       title = ""The Guest Star Backbone Of A Simpsons Co-Star Network"",
       subtitle = glue(""Shown below is a co-occurance network of guest stars in The Simpsons, best explained as a 'Who co-stars together?'.<br> {subtitle_names} <br>are the most frequent guest stars in the series.""),
       caption = ""**Data**: Wikipedia via @datawookie | **Graphic**: @jakekaupp"") +
  theme_jk(grid = FALSE,
           subtitle_family = ""Lora"",
           caption_family = ""Lora"",
           markdown = TRUE) +
  theme(legend.position = ""none"",
        axis.text = element_blank())

ggsave(here(""2019"", ""week35"", ""tw35_plot.png""), plot = co_star_plot, device = agg_png(), width = 9, height = 8)

ggplot(mtcars, aes(x = mpg, y = disp)) +
  geom_point() +
  labs(title = paste0(highlight_text(""This is bold"", style = ""bi""), ""This isn't""),
       subtitle = paste0(highlight_text(""This is bold"", style = ""bi""), ""This isn't"")) +
  theme_jk() +
  theme(plot.title = ggtext::element_markdown(),
        plot.subtitle = ggtext::element_markdown())
",2019,35
44,44,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week36/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(nord)
library(glue)
library(here)
library(ragg)

cpu <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/cpu.csv"")

gpu <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/gpu.csv"")

ram <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/ram.csv"")


plot_data <- list(cpu = cpu, gpu = gpu) %>% 
  imap_dfr(~select(.x, date_of_introduction, transistor_count, area, process) %>% 
             mutate(type = .y)) %>% 
  group_by(date_of_introduction, type) %>% 
  summarize_at(vars(transistor_count, area, process), mean, na.rm = TRUE) %>% 
  arrange(date_of_introduction, type)

strip_labels <- tibble(type = c(""cpu"", ""gpu""))

plot <- ggplot(plot_data, aes(x = area, y = transistor_count)) +
  geom_text(data = strip_labels, aes(label = toupper(type)), x = 400, y = 4, family = ""Oswald Bold"", size = 18, color = ""grey90"") +
  geom_smooth(method = ""auto"", formula = y ~ log10(x), se = FALSE, size = 0.5,  color = nth(nord_palettes$victory_bonds, 3)) +
  geom_hline(aes(yintercept = 10^10), linetype = ""dotted"", color = first(nord_palettes$victory_bonds)) +
  geom_point(aes(color = log10(process)), size = 3) +
  scale_color_nord(name = ""Process Size"",
                        discrete = FALSE,
                        palette = ""lumina"",
                        reverse = TRUE,
                        labels = function(x) glue(""{scales::comma(10^x)} nm""),
                        breaks = c(1, 2, 3, 4)) +
  scale_y_log10(breaks = c(1, 10^4, 10^6, 10^8, 10^10),
                labels = c(""1"", ""10K"", ""1M"", ""100M"", ""10B"")) +
  scale_x_continuous(labels = function(x) glue(""{x} {expression(mm^2)}"")) +
  facet_wrap(~type) +
  labs(x = NULL, 
       y = NULL,
       title = ""Moore's Law May Be Dead, Killed By The Tension Between Manufacturing and Transistor Density"",
       subtitle = ""*Moore's law*, the observation that the **number of transistors** on integrated circuits **doubles every two years**<br>
       hasn't held.  Transistor density is reaching a plateau, requiring manufacturing changes of an increase in available<br>
       chip size or a decrease in process size."",
       caption = ""**Data:** Wikipedia | **Graphic:** @jakekaupp"") +
  theme_jk(grid = ""XY"",
          markdown = TRUE) +
  theme(strip.text = element_blank())

ggsave(here(""2019"", ""week36"", ""tw36_plot.png""), plot = plot, device = agg_png(), width = 10, height = 6)
",2019,36
45,45,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week37/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(tidygraph)
library(ggraph)
library(ggforce)
library(janitor)
library(jkmisc)
library(glue)
library(ggtext)
library(colorspace)
library(ragg)

legacy_data <- here(""2019"", ""week37"", ""data"", ""Saferparks-dataset-legacy.csv"") %>% 
  read_csv() %>% 
  mutate(year = year(mdy(acc_date))) %>% 
  filter(between(year, 1999, 2007))

device_type <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, device_type, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ lighten(""#251351"", .75),
                           industry_sector == ""recreation"" ~ lighten(""#7d2e68"", .75),
                           industry_sector == ""water park"" ~ lighten(""#41658a"", .75))) %>% 
  select(name = device_type, size, color)

device_category <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ lighten(""#251351"", .25),
                           industry_sector == ""recreation"" ~ lighten(""#7d2e68"", .25),
                           industry_sector == ""water park"" ~ lighten(""#41658a"", .25))) %>% 
  select(name = device_category, size, color) 

sector <-  legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ ""#251351"",
                           industry_sector == ""recreation"" ~ ""#7d2e68"",
                           industry_sector == ""water park"" ~""#41658a"")) %>% 
  select(name = industry_sector, size, color)

nodes <- bind_rows(sector, device_category, device_type)

edge_one <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>%
  select(industry_sector, device_category) %>% 
  mutate_all(~as.numeric(factor(., nodes$name))) %>% 
  set_names(c(""from"", ""to""))

edge_two <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>%
  select(device_category, device_type) %>% 
  mutate_all(~as.numeric(factor(., nodes$name))) %>% 
  set_names(c(""from"", ""to""))

edges <- bind_rows(edge_one, edge_two)

graph <- tbl_graph(nodes = nodes, edges = edges) 

labels <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, device_type, name = ""size"") %>% 
  group_by(industry_sector) %>% 
  top_n(1 , size) %>% 
  filter(size > 1) %>% 
  pull(device_type) 
 
text_bc <- function(text, color) {
  
  glue(""<span style = color:{color}>**{text}**</span>"")
  
}


plot <- ggraph(graph, 'circlepack', weight = size) + 
  geom_node_circle(aes(fill = color)) + 
  geom_node_text(aes(label = glue(""{str_remove(name, ' - undefined')}:\n{size}""), filter = name %in% labels, family = ""Oswald"")) +
  scale_fill_identity() +
  labs(x = NULL,
       y = NULL,
       title = ""Attractions With The Most Reported Injuries from 1999-2007"",
       caption = ""Data: **SaferParks** | Graphic: **@jakekaupp**"",
       subtitle = glue(""Shown below is a packed circle representation of reported accidents in the SaferParks database from 1999-2007.<br>Circles are organized by {highlight_text('Amusement rides', '#251351', 'b')}, {highlight_text('Recreation', '#7d2e68', 'b')} and {highlight_text('Water Park', '#41658a', 'b')}. Device category and device type are the<br>middle and lightest hues, respectively."")) +
  theme_jk(grid = FALSE) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        plot.subtitle = element_markdown(),
        plot.caption = element_markdown()) 


ggsave(here(""2019"", ""week37"", ""tw_37plot.png""), plot, width = 9, height = 10, dev = agg_png())


  ",2019,37
46,46,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week38/R/analysis.R,"library(tidyverse)
library(rvest)
library(janitor)
library(here)
library(fuzzyjoin)
library(jkmisc)
library(ragg)

# Get park fees
fees_page <- ""https://www.nps.gov/aboutus/entrance-fee-prices.htm""

parks <- read_html(fees_page) %>% 
  html_nodes(""h3"") %>% 
  html_text() %>% 
  .[-1:-2]

park_fees <- read_html(fees_page) %>% 
  html_nodes("".table-wrapper > table"") %>% 
  html_table() %>% 
  map(~set_names(.x, c(""date"", ""park_specific_annual_pass"", ""per_vehicle"", ""per_person"", 
                       ""per_motorcycle""))) %>% 
  map2(parks, ~mutate(.x, park = .y)) %>% 
  bind_rows() %>% 
  filter(date == ""Current"") %>% 
  rename(park_name = park) %>% 
  mutate(park_name = stringi::stri_trans_general(park_name, id = ""Latin-ASCII""),
         park_name = str_replace(park_name, ""Hawai'i"", ""Hawaii""))



#udpated data
summary_report <- here(""2019"", ""week38"", ""data"", ""annual_summary_report.csv"") %>% 
  read_csv() %>% 
  clean_names()

plot_data <- summary_report %>% 
  filter(year == 2018) %>% 
  mutate(visitors = recreation_visitors + non_recreation_visitors) %>% 
  select(year, park_name, visitors) %>% 
  mutate(park_name = str_remove(park_name, ""[A-Z]{2,}""),
         park_name = str_remove(park_name, ""& PRES""),
         park_name = trimws(park_name)) %>% 
  regex_left_join(park_fees, ., ignore_case = TRUE) %>% 
  distinct(year, park_name.x, .keep_all = TRUE) %>% 
  filter(str_detect(park_name.x, ""Park""), !str_detect(park_name.x, ""Great Falls"")) %>% 
  mutate(revenue = visitors * parse_number(per_person)) %>% 
  rename(park_name = park_name.x) %>% 
  select(-park_name.y)
  

plot <- ggplot(plot_data, aes(x = fct_reorder(park_name, revenue), y = revenue)) +
  geom_col(fill = ""#5e81ac"", size = 0.1) +
  coord_flip() +
  scale_y_continuous(labels = scales::dollar, expand = c(0.01,0)) +
  labs(title = ""Estimated National Park Revenue from Fees for 2018"",
       subtitle = str_wrap(""Illustrated below is a bar chart of fee revenue from US National Parks in 2018.  Estimated Revenue calculated using per person admittance rates and total park visitors."", 95),
       caption = ""Data: www.nps.gov | Graphic: @jakekaupp"",
       x = NULL,
       y = NULL) +
  theme_jk(grid = ""X"") +
  theme(plot.background = element_rect(fill = ""#2e3440""),
        text = element_text(color = ""#eceff4""),
        panel.grid = element_line(color = ""#e5e9f0""),
        axis.text.x = element_text(color = ""#eceff4""),
        axis.text.y = element_text(color = ""#eceff4""))

ggsave(here(""2019"", ""week38"", ""tw_38plot.png""), plot, width = 10, height = 8, device = agg_png())

",2019,38
47,47,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week39/R/analysis.R,"library(tidyverse)
library(janitor)
library(tidycensus)
library(glue)
library(here)
library(sf)
library(tigris)
library(jkmisc)
library(ggtext)

school_diversity <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-24/school_diversity.csv"")

## Getting the ACS Survey data ----
acs_var <- load_variables(2017, ""acs1"", cache = TRUE)

race_vars <- filter(acs_var, concept == ""RACE"") %>% 
  select(name, label) %>% 
  separate(label, c(""estimate"", ""total"", ""type""), sep = ""!!"") %>% 
  mutate(type = coalesce(type, total)) %>% 
  select(name, label = type)

if (!file.exists(here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))) {
  
  acs_race <- map_df(state.abb, ~get_acs(geography = ""school district (unified)"", 
                                         variables = race_vars$name,
                                         state = .x))
  
  saveRDS(acs_race, here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))
  
} else {
  
  acs_race <- readRDS(here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))
}





# Recoding ACS and aggregating data, sadly not easy to determine Hispanic origin ----
# Following methodology from WaPo repo, recoding Native Hawaiian and Pacifici Islander into Asian.
diversity_data <- acs_race %>% 
  left_join(race_vars, by = c(""variable"" = ""name"")) %>% 
  mutate(label = case_when(label == ""White alone"" ~ ""White"",
                           label == ""Black or African American alone"" ~ ""Black"",
                           label == ""American Indian and Alaska Native alone"" ~ ""AIAN"",
                           label == ""Native Hawaiian and Other Pacific Islander alone"" ~ ""Asian"",
                           label == ""Asian alone"" ~ ""Asian"",
                           label == ""Two or more races"" ~ ""Multi"",
                           label == ""Some other race alone"" ~ ""Other"",
                           TRUE ~ label)) %>% 
  group_by(GEOID, NAME, label) %>% 
  summarize_at(vars(estimate), sum) 


# Using Simpson's Diversity Index instead of max race metrics for diversity----
totals <- diversity_data %>% 
  summarize(total = sum(estimate)*(sum(estimate)-1))

dvs_score <- diversity_data %>% 
  filter(label != ""Total"") %>% 
  mutate(es_minus = estimate-1) %>% 
  summarize(numerator = sum(estimate*es_minus)) %>% 
  left_join(totals) %>% 
  mutate(diversity = 1 - numerator/total) %>% 
  select(GEOID, NAME, diversity) 
  
acs_diversity <- diversity_data %>% 
  spread(label, estimate) %>% 
  select(-Total) %>% 
  left_join(dvs_score) %>% 
  rename(acs_diversity = diversity) %>% 
  ungroup() %>% 
  mutate(NAME = tolower(NAME),
         NAME = str_remove(NAME, ""\\(.+\\)""),
         NAME = str_replace_all(NAME, "";"", "","")) %>% 
  separate(NAME, c(""NAME"", ""state""), sep = "","") %>% 
  mutate(NAME = str_remove_all(NAME, ""school district*+"")) %>% 
  select(GEOID, acs_diversity)

# Use WaPo data and calculate Simpson's Diversity Index
upd_school <- school_diversity %>% 
  filter(SCHOOL_YEAR == ""2016-2017"") %>% 
  select(LEAID, LEA_NAME, ST, SCHOOL_YEAR, AIAN:Total) %>% 
  pivot_longer(AIAN:Multi, ""race"", ""value"") %>% 
  mutate(n = floor(Total * value))

school_totals <- upd_school %>% 
  group_by(LEAID, LEA_NAME, ST, SCHOOL_YEAR) %>% 
  summarize(total = sum(n)*(sum(n)-1))
  
upd_school_dvs <- upd_school %>% 
  group_by(LEAID, LEA_NAME, ST, SCHOOL_YEAR) %>% 
  mutate(n_minus = n-1) %>% 
  summarize(numerator = sum(n*n_minus)) %>% 
  left_join(school_totals) %>% 
  mutate(diversity = 1 - numerator/total) %>% 
  select(GEOID = LEAID, NAME = LEA_NAME, ST, school_diversity = diversity) 

# Environment Cleanup---
remove(list = ls()[str_which(ls(), ""upd_school_dvs|acs_diversity"", negate = TRUE)])

# Comparing the two diversity measures and creating the ratio ----
overall_diversity <- upd_school_dvs %>% 
  left_join(acs_diversity, by = ""GEOID"") %>% 
  mutate(ratio = school_diversity/acs_diversity) %>% 
  filter(!is.na(acs_diversity))

# Get School District maps ----

if (!file.exists(here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))) {
  
  district_maps <- map(state.abb, ~school_districts(.x, class = ""sf""))
  
  saveRDS(district_maps, here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))
  
} else {
  
  district_maps <- readRDS(here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))
}



# One do.call to keep them all, and in the shallows bind them----
maps <- do.call(sf:::rbind.sf, district_maps)

plot <- overall_diversity %>%
  right_join(maps, ., by = ""GEOID"") %>%
  filter(!ST %in% c(""HI"", ""AK"")) %>%
  ggplot() +
  geom_sf(aes(fill = ratio), color = 'white', size = 0.01) +
  scale_fill_viridis_c(""Alignment Ratio"", option = ""cividis"", limits = c(0, 1), labels = scales::percent, na.value = ""white"") +
  coord_sf(crs = 26915) +
  labs(title = ""Is Diversity In School Districts Reflected In The Diversity Of The General Population?"",
       subtitle = glue(""Shown below is a choropleth map illustrating the ratio between the Diversity Index of a School Population and the Diversity Index of the General Population in that School District in 2017.<br>
       The more {highlight_text('yellow', '#FFEA46', 'b')} an area, the greater alignment between diversity indices.  The more {highlight_text('blue', '#00204D', 'b')} an area, the greater the difference between the diversity of the school and the general populace.<br> 
       This analysis focused on unified school districts and available data on race from the ACS Survey.  Diversity was calculated using Simpson's Diversity Index.""),
       caption = ""Data: **Washington Post via @dataKateR & American Community Survey** | Graphic: **@jakekaupp**"") +
  theme_jk(grid = FALSE,
          markdown = TRUE) +
  theme(axis.text = element_blank())

ggsave(here(""2019"", ""week39"", ""tw39_plot.png""), width = 16, height = 10, device = ragg::agg_png())
",2019,39
48,48,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week4/R/analysis.R,"library(tidyverse)
library(readxl)
library(here)
library(janitor)
library(albersusa)
library(sf)
library(sp)
library(rgeos)
library(maptools)
library(ggthemes)
library(viridis)
library(scales)
library(glue)
library(jkmisc)



incar_data <- here(""2019"",""week4"",""data"") %>% 
  dir(full.names = TRUE, pattern = ""incarceration"") %>% 
  read_excel()

fix_null <- function(x) if_else(is.nan(x), NA_real_, x)

# Mapping things
us <- counties_composite()
us_map <- fortify(us, region=""fips"") %>% 
  mutate_at(""id"", as.numeric)

ratio_data <- incar_data %>% 
  group_by(year, fips, state, county_name) %>% 
  transmute(black_pop_ratio = black_pop_15to64/total_pop_15to64,
         black_prison_ratio = black_prison_pop/total_prison_pop,
         asian_pop_ratio = asian_pop_15to64/total_pop_15to64,
         asian_prison_ratio = asian_prison_pop/total_prison_pop,
         latino_pop_ratio = latino_pop_15to64/total_pop_15to64,
         latino_prison_ratio = latino_prison_pop/total_prison_pop,
         native_pop_ratio = native_pop_15to64/total_pop_15to64,
         native_prison_ratio = native_prison_pop/total_prison_pop,
         white_pop_ratio = white_pop_15to64/total_pop_15to64,
         white_prison_ratio = white_prison_pop/total_prison_pop) %>% 
  group_by(fips, state, county_name) %>% 
  summarize_at(vars(contains(""ratio"")), mean, na.rm = TRUE) %>% 
  mutate_at(vars(contains(""ratio"")), fix_null)


map_data <- left_join(us_map, ratio_data, by = c(""id"" = ""fips""))  %>% 
  ungroup() %>% 
  gather(""variable"",""percentage"", contains(""ratio"")) %>% 
  separate(variable, c(""ethnicity"", ""category""), sep = ""_"")

plot <- ggplot() +
  geom_map(data = us_map, map = us_map,
           aes(x = long, y = lat, map_id = id),
           color =""#2b2b2b"", size = 0.05, fill = NA) +
  geom_map(data = map_data, map = us_map,
           aes(x = long, y = lat, map_id = id, fill = percentage),
           color =""#2b2b2b"", size = 0.05) + 
  scale_fill_viridis_c("""", na.value = ""white"", option = 'cividis', labels = scales::percent) +
  coord_map() +
  labs(title = ""Differences between the General and Prison Population by County and Ethnic Group from 1970 to 2016"",
       subtitle = str_wrap(""Non-white and non-Asian ethnic groups in the South-Eastern United States have a higher average representation in prison than in the overall population.  Missing data indicated by no fill color."",  90),
       caption = ""Data: Vera Institute of Justice | Graphic: @jakekaupp"") +
  facet_grid(category ~ ethnicity , labeller = labeller(category = c(""pop"" = ""Total\nPopulation"", ""prison"" = ""Prison\nPopulation""),
                                                       ethnicity = str_to_title)) +
  theme_map(base_family = ""Scope One"", 
            base_size = 16) +
  theme(plot.caption = element_text(size = 10),
        plot.title = element_text(family = ""Oswald""),
        legend.background = element_rect(fill = NA),
        legend.text = element_text(size = 10),
        legend.title = element_text(size = 12),
        legend.position = ""bottom"",
        legend.justification = ""center"",
        legend.direction = ""horizontal"",
        legend.key.height = unit(0.2, ""cm""),
        legend.key.width = unit(1, ""cm""),
        strip.background = element_blank(),
        strip.text.y = element_text(angle = 0))

ggsave(here(""2019"",""week4"",""tw4_choro.png""), width = 11, height = 5)

",2019,4
49,49,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week40/R/analysis.R,"library(tidyverse)
library(sf)
library(tigris)
library(glue)
library(colorspace)
library(jkmisc)
library(ggforce)
library(ragg)
library(here)

# Get TidyTuesday data
pizza_datafiniti <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv"") %>% 
  filter(province == ""NY"") %>% 
  distinct(name, latitude, longitude)

pizza_barstool <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv"") %>% 
  distinct(name, latitude, longitude) %>% 
  filter(!is.na(latitude))

# Get all New York County road maps
counties <- c(""New York County"", ""Kings County"", ""Bronx County"", ""Richmond County"",  ""Queens County"")

roads_data <- map(counties, ~roads(""NY"", .x, class = ""sf"")) %>% 
  do.call(sf:::rbind.sf, .)

# Build plot colors as a named vector and as a tibble
plotcolors <- c('Other' = '#cccccc',
                'Ave' = '#59c8e5',
                'St' = '#fed032',
                'Tunl' = '#fed032',
                'Brg' = '#fed032',
                'N' = '#fed032',
                'S' = '#fed032',
                'E' = '#fed032',
                'W' = '#fed032',
                'Rd' = '#4cb580',
                'Dr' = '#0a7abf', 
                'Hwy' = '#ff9223', 
                'Plz' = '#ff9223',
                'Viaduct' = '#ff9223', 
                'Expy' = '#ff9223', 
                'Pkwy' = '#ff9223',
                'Thruway' = '#ff9223',
                'State Hwy' = '#ff9223',
                'State' = '#ff9223',
                'US Hwy' = '#ff9223',
                'Blvd'= '#2e968c')

pc_tibble <- tibble(street_type = names(plotcolors),
                    color = plotcolors)

# Assign street types to roads
roads <- roads_data %>% 
  filter(!is.na(RTTYP)) %>% 
  mutate(street_type = map_chr(FULLNAME, ~first(names(plotcolors)[str_which(.x, glue(""{names(plotcolors)}\\b""))]))) %>% 
  mutate(street_type = if_else(str_detect(FULLNAME, ""I-""), 'I-', street_type)) %>% 
  mutate(street_type = case_when(is.na(street_type) & MTFCC == ""S1100"" ~ 'Expy',
                                 is.na(street_type) & MTFCC == ""S1200"" ~ 'St',
                                 is.na(street_type) & !MTFCC %in% c(""S1100"", ""S1200"") ~ ""Other"",
                                 TRUE ~ street_type)) %>% 
  left_join(pc_tibble, by = ""street_type"")

# Get Counties shapefiles to determine which pizza places are in the areas I want
counties_sf <- counties(""NY"", class = ""sf"") %>% 
  filter(NAMELSAD %in% counties)

# Use st_intersects and filter to remove out of bounds pizza places
pizza_sf_df <- st_as_sf(pizza_datafiniti, coords = c(""longitude"", ""latitude""), crs = st_crs(roads)) 
pizza_sf_bs <- st_as_sf(pizza_barstool, coords = c(""longitude"", ""latitude""), crs = st_crs(roads)) 
  
  
# Filter to pizza places in the five boroughs
ny_pizza_df <-  filter(pizza_sf_df, map_lgl(st_intersects(pizza_sf_df, counties_sf), ~!is_empty(.x)))

ny_pizza_bs <-  filter(pizza_sf_bs, map_lgl(st_intersects(pizza_sf_bs, counties_sf), ~!is_empty(.x)))

ny_pizza <- sf:::rbind.sf(ny_pizza_bs, ny_pizza_df) %>% 
  distinct(geometry)

# Construct the color legend
legend <- pc_tibble %>% 
  filter(street_type %in% c(""Other"",""Ave"",""St"", ""Rd"", ""Dr"", ""Hwy"", ""Blvd"")) %>% 
  mutate(street_type = factor(street_type, levels = c(""Other"", ""Ave"", ""Dr"", ""Rd"", ""Blvd"", ""St"", ""Hwy""), labels = c(""Other"", ""Avenue"", ""Drive"", ""Road"", ""Boulevard "", ""Street"", ""Highway""))) %>%
  arrange(street_type) %>% 
  mutate(x0 = seq(3, by = 4.5, length.out = 7),
         r = 1.75,
         y0 = 0) %>% 
  ggplot(aes(x0 = x0, y0 = y0, r = r)) +
  geom_circle(aes(fill = color, color = darken(color))) +
  geom_text(aes(label = street_type, x = x0, y = 0), family = ""Lora"", size = 3) +
  annotate(""text"", family = ""Oswald"", x = -2, y = 0, label = ""Legend"", size = 6) +
  scale_fill_identity() +
  scale_color_identity() +
  expand_limits(y = c(-0.5, 4),
                x = c(-4, 24)) +
  labs(x = NULL,
       y = NULL) +
  coord_equal(clip = ""off"") +
  theme_jk(grid = FALSE, plot_title_size = 30) +
  theme(panel.grid.major = element_line(colour = ""transparent""),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()) 

legend_grob <- ggplotGrob(legend)

subtitle <- ""Shown on this map are the roads and the pizza places of the Five Boroughs of New York City.  
Pizza places are distinct locations almagamated from the DataFiniti and Barstool datasets, and a represented by purple dots.  Manhattan is the most represented borough in the dataset, unsurprising given the relative population, and it being the home of the Teenage Mutant Ninja Turtles.
The map style of plotting the colored roads were inspired by Erin Davis (erdavis1 on github), and her series of circular maps of World Cities.""

caption <- ""Data: DataFiniti, Barstool, US Census Shapefiles\nGraphic: @jakekaupp""


# Plot the Street maps and Pizza place data
pizza_map <- ggplot() +
  geom_sf(data = filter(roads, street_type != ""Other""), aes(color = color), size = 0.25) + 
  geom_sf(data = filter(roads, street_type == ""Other""), aes(color = color), size = 0.35) + 
  geom_sf(data = ny_pizza, color = darken(""#963484""), fill = ""#963484"", shape = 21, size = 2, alpha = 0.5) +
  annotate(""text"", label = ""Pizza Places of the Five Boroughs"", family = ""Oswald"", x = -74.3, y = 40.91, size = 6, hjust = 0) +
  annotate(""text"", family = ""Lato"", label = str_wrap(subtitle, 60), x = -74.3, y = 40.89, hjust = 0, vjust = 1) +
  annotate(""text"", family = ""Lato"", label = caption, x = -74.3, y = 40.78, hjust = 0, vjust = 1) +
  annotation_custom(legend_grob, xmin = -74.2, xmax = Inf, ymin = 40.49, ymax = 40.55) +
  scale_color_identity() +
  scale_size_identity() +
  coord_sf(clip = ""off"") +
  labs(x = NULL, y = NULL) +
  theme_jk(grid = FALSE) +
  theme(panel.grid.major = element_line(colour = ""transparent""),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()) 


ggsave(here('2019', 'week40', 'tw40_plot.png'), width = 12, height = 9, dev = agg_png())",2019,40
50,50,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week41/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(janitor)
library(jkmisc)
library(ggforce)
library(ggtext)
library(nord)
library(glue)
library(ragg)

# It's 250mb, can't put it into git, you're going to have to go get it
# from https://openpowerlifting.org/data and stick it in data.
pl_data <- here(""2019"", ""week41"", ""data"") %>% 
  dir(pattern = ""openpowerlifting"", full.names = TRUE) %>% 
  read_csv() %>% 
  clean_names()

plot_data <- pl_data %>%
  filter_at(vars(starts_with(""best"")), all_vars(. > 0)) %>% 
  filter(str_detect(place, ""1"")) %>% 
  mutate(year = year(date)) %>%
  select(-date) %>%
  mutate_at(vars(starts_with(""best"")), ~./bodyweight_kg) %>% 
  pivot_longer(starts_with(""best""), names_to = ""lift"") %>% 
  group_by(year, sex, lift) %>%
  filter(value == max(value, na.rm = TRUE)) %>% 
  arrange(lift, sex, year)

labels <- tibble(label = c(""Bench Press"", ""Deadlift"", ""Squat""),
                 lift = c(""best3bench_kg"", ""best3deadlift_kg"", ""best3squat_kg""),
                 year = 2020,
                 value = 1)

annotations <- plot_data %>% 
  group_by(sex, lift) %>% 
  filter(value == max(value, na.rm = TRUE)) %>% 
  mutate(description = glue(""Weight: {bodyweight_kg} kg\nLifted: {bodyweight_kg*value} kg\n{federation}: {meet_name}""),
         name = str_remove(name, ""\\#[0-9]""))

plot <- ggplot(plot_data, aes(x = year, y = value)) +
  geom_path(aes(color = sex)) +
  geom_point(aes(fill = sex), shape = 21, color = ""#2E3440"") +
  geom_text(data = labels, aes(label = label), color = ""#E5E9F0"", family = ""Oswald"", fontface = ""bold"", size = 10, hjust = 1) +
  geom_mark_circle(data = filter(annotations, sex == ""M""), aes(color = sex, label = name, description = description), expand = unit(2, ""mm""), label.family = c(""Oswald"", ""Lato""), label.fill = ""#4C566A"", label.colour = ""#E5E9F0"", con.colour = ""#D8DEE9"", label.margin = margin(2, 3, 2, 3, ""mm"")) +
  geom_mark_circle(data = filter(annotations, sex == ""F""), aes(color = sex, label = name, description = description), expand = unit(2, ""mm""), label.family = c(""Oswald"", ""Lato""), label.fill = ""#4C566A"", label.colour = ""#E5E9F0"", con.colour = ""#D8DEE9"", label.margin = margin(2, 3, 2, 3, ""mm"")) +
  facet_wrap(~lift) +
  scale_color_manual(values = set_names(c(""#314cb6"",""#DD2A7B""), c(""M"",""F""))) +
  scale_fill_manual(values = set_names(c(""#314cb6"",""#DD2A7B""), c(""M"",""F""))) +
  scale_x_continuous(breaks = seq(1970, 2020, 10)) +
  theme_jk(dark = TRUE, 
           grid = ""XY"",
           markdown = TRUE) +
  labs(x = NULL,
       y = NULL,
       title = ""Evolution of Power: How the Ratio of Bodyweight to Lifted Weight Has Progressed"",
       subtitle = glue(""Illustrated below is the maximum of the ratio of bodyweight to lifted weight for winning lifts in each year and event for both {highlight_text('Men', '#314cb6', 'b')} and {highlight_text('Women', '#DD2A7B', 'b')} for all meets recorded by Open Powerlifting.""),
       caption = ""Data: **openpowerlifting.org** | Graphic: **@jakekaupp**"") +
  theme(legend.position = ""none"",
        panel.grid.major = element_line(size = 0.01),
        strip.text = element_blank())

ggsave(here('2019', 'week41', 'tw41_plot.png'), plot, width = 15, height = 8, device = agg_png())",2019,41
51,51,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week42/R/analysis.R,"library(tidyverse)
library(here)
library(janitor)
library(jkmisc)
library(ggalt)
library(ggtext)
library(glue)
library(ragg)

big_epa_cars <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-15/big_epa_cars.csv"")

top <- big_epa_cars %>% 
  clean_names() %>% 
  count(make, year) %>% 
  count(make) %>% 
  filter(n == 37)

plot_data <- big_epa_cars %>% 
  clean_names() %>% 
  select(make, v_class, year, you_save_spend) %>% 
  semi_join(top) %>% 
  group_by(year, make) %>% 
  summarize(total_save_spend = mean(you_save_spend)) %>%
  group_by(year) %>% 
  mutate(rank = min_rank(desc(total_save_spend))) %>% 
  ungroup() %>% 
  mutate(size = if_else(make == ""Ford"", 1, 0.5),
         make = factor(make, pull(top, make)),
         make = fct_relevel(make, ""Ford"", after = Inf),
         make = fct_recode(make, ""**Ford**"" = ""Ford""))


grid <- tibble(rank = 1:22)

colors <- set_names(grey.colors(22), pull(top, make) %>%
                      factor() %>%
                      fct_recode(""**Ford**"" = ""Ford""))

colors[[""**Ford**""]] <- ""#DD2A7B""


plot <- ggplot(plot_data, aes(x = year, y = rank)) +
  geom_segment(data = grid, aes(x = 1983, xend = 2021, y = rank, yend = rank), color = ""#cccccc"", alpha = 0.5, size = 0.1) +
  geom_xspline(aes(color = make, size = size), show.legend = FALSE) +
  geom_point(aes(fill = make), shape = 21, color = ""white"", show.legend = FALSE) +
  geom_richtext(data = filter(plot_data, year == 2020), aes(label = as.character(make), x = 2021, color = make), hjust = 0, family = ""Lora"", size = 4, show.legend = FALSE,  fill = NA, label.color = NA, 
                label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_text(data = filter(plot_data, year == 1984), aes(label = rank, x = 1983), hjust = 1, family = ""Oswald"", size = 4) +
  labs(x = NULL,
       y = NULL,
       title = ""From Chugging to Sipping: Fuel Cost Savings of Major Automakers since 1984"",
       subtitle = glue(""Shown below is a rankings chart of average fuel cost savings, measured over 5 years, from 1984 to 2020.  {highlight_text('Ford','#DD2A7B', 'b')} has had quite the journey, battling from the bottom<br>of the list to the second-best North American manufacturer."")) +
  scale_color_manual(values = colors) +
  scale_fill_manual(values = colors) +
  scale_size_identity() +
  scale_x_continuous(breaks = 1984:2020) +
  scale_y_continuous(trans = ""reverse"", breaks = NULL) +
  expand_limits(x = 2025) +
  theme_jk(grid = ""X"", 
           markdown = TRUE)

ggsave(here(""2019"", ""week42"", ""tw42_plot.png""), plot = plot, width = 13, height = 6)
",2019,42
52,52,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week43/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(ggraph)
library(tidygraph)
library(glue)
library(jkmisc)
library(colorspace)
library(ggforce)
library(ggtext)

horror_movies <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv"")

movie_cast <- distinct(horror_movies, title, release_date, review_rating, .keep_all = TRUE) %>% 
  mutate(year = str_extract(title, ""\\((\\d{4})\\)""),
         year = parse_number(year),
         title = str_remove(title, ""(\\s\\(\\d{4}\\))""),
         date = dmy(release_date)) %>% 
  arrange(title) %>% 
  separate_rows(cast, sep = ""\\|"") %>% 
  mutate(cast = trimws(cast)) %>% 
  select(title, year, review_rating, cast)

cast_df <- left_join(movie_cast, movie_cast, by = c(""title"", ""year"", ""review_rating"")) %>% 
  rename(from = cast.x,
         to = cast.y) %>% 
  filter(from != to) 

nodes <- cast_df %>% 
  group_by(from) %>% 
  summarize(node_size = n_distinct(title)) %>% 
  distinct(from, .keep_all = TRUE) 

focus <- ""Eric Roberts""

edges <- cast_df %>% 
  count(from, to, sort = TRUE, name = ""edge_size"") %>% 
  distinct(from, to, .keep_all = TRUE) %>% 
  mutate(color = if_else(from == focus | to == focus, ""#bb0a1e"", ""#373e40""),
         alpha = if_else(from == focus | to == focus, 1, 0.2),
         size = if_else(from == focus | to == focus, 1, 0.1))

connected <- filter(edges, from == focus) %>% 
  distinct() %>% 
  pull(to) 

cast_network <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE) %>% 
  activate(nodes) %>% 
  mutate(degree = centrality_eigen(),
         alpha = if_else(from %in% c(focus, connected),  1, 0.2)) %>% 
  top_n(500, degree) %>% 
  mutate(fill = if_else(from %in% c(focus, connected), ""#bb0a1e"", ""#373e40""),
         color = if_else(from %in% c(focus, connected), darken(""#bb0a1e""), darken(""#373e40"")))

plot <- ggraph(cast_network, layout = ""graphopt"") + 
  geom_edge_link(aes(alpha = stat(index), edge_colour = color, edge_width = size), show.legend = FALSE) + 
  geom_node_point(aes(size = node_size, fill = fill, color = color), shape = 21, show.legend = FALSE) +
  #geom_mark_circle(aes(x, y, filter = from == focus, label = from, description = ""Legendary B-Movie Actor""), expand = unit(0, ""mm""), label.family = c(""Oswald"", ""Lora"")) +
  scale_edge_color_identity() +
  scale_alpha_identity() +
  scale_fill_identity() +
  scale_edge_width_identity() +
  scale_color_identity() +
  labs(x = NULL,
       y = NULL,
       title = ""Horror Movie Co-Star Networks of the Top 500 Prolific Performers"",
       subtitle = glue(""The reach of B-movie legend {highlight_text('Eric Roberts', '#bb0a1e', 'b')} is featured below across his 27 films. Prolific performers determined by the top 500<br>actors by eigenvalue centrality.""),
       caption = ""Data: **IMDB** | Graphic: **@jakekaupp**"") +
  theme_jk(grid = FALSE,
           markdown = TRUE) +
  theme(axis.text = element_blank())

ggsave(here(""2019"", ""week43"", ""tw43_plot.png""), plot, width = 10, height = 6, device = ragg::agg_png())

  
",2019,43
53,53,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week44/R/analysis.R,"library(tidyverse)
library(waffle)
library(lubridate)
library(jkmisc)
library(scales)
library(colorspace)
library(patchwork)
library(ggtext)
library(glue)
library(here)

#Get the data ----
nyc_squirrels <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-29/nyc_squirrels.csv"")

# Process the data ----
activity_data <- nyc_squirrels %>% 
  mutate(date = mdy(date)) %>% 
  pivot_longer(names_to = ""activity"", running:foraging) %>%
  group_by(date, activity) %>% 
  summarize(value = sum(as.numeric(value))) %>% 
  mutate(activity = factor(activity, labels = unique(activity)),
         activity = fct_reorder(activity, value, .fun = sum)) %>% 
  arrange(date, desc(activity))

# Make my palette

slate_ramp <- colorRampPalette(c(""#3B454A"", lighten(""#3B454A"", 0.8)))(5) 

grey_ramp <- grey.colors(5, 0.5, 0.9)

pal <- set_names(slate_ramp, unique(activity_data$activity))

pal[""foraging""] <- ""#DD2A7B""

partition_waffle <- function(x, start, nrows, flip = FALSE) {
  
   offset <- start - x
  
   offset_rows <- offset %/% nrows
   
   offset_blocks <- offset %% nrows
   
  
   comp_blocks <- nrows - offset_blocks
   
   if (comp_blocks != nrows) {
     
     rows <- (x - comp_blocks) %/% nrows
     
     blocks <- (x - comp_blocks) %% nrows
     
     start_row <- offset_rows + rows + 1
     
     if (blocks == 0) {
       
       end_row <- start_row
       
     } else {
       
       end_row <- start_row + 1 
       
     }
     
   } else {
     
     rows <- x %/% nrows
     
     blocks <- x %% nrows
     
     start_row <- rows + offset_rows
     
     end_row <- rows + offset_rows + 1
     
   }
   
   
   if (flip) {
     
     tibble(y = c(start_row, start_row, end_row),
            yend = c(start_row, end_row, end_row),
            x = c(blocks, blocks, 0),
            xend = c(nrows, blocks, blocks))
     
     
   } else {
     
     tibble(x = c(start_row, start_row, end_row),
            xend = c(start_row, end_row, end_row),
            y = c(blocks, blocks, 0 -1),
            yend = c(nrows + 1, blocks, blocks))
   }
     
     

   
  
} 

# Waffles by activity ----

activity_outlines <- activity_data %>% 
  ungroup() %>% 
  arrange(desc(activity), date) %>% 
  mutate(cum_sum = cumsum(value)) %>% 
  mutate(lines = map2(value, cum_sum, partition_waffle, flip = FALSE, nrows = 20)) %>% 
  unnest(lines) %>% 
  filter(date == last(date))

activity_labels <- activity_data %>% 
  group_by(activity) %>% 
  summarize(total = sum(value)) %>%
  left_join(filter(activity_outlines, yend == 21), by = ""activity"")

waffle_by_activity <- activity_data %>% 
  arrange(desc(activity), date) %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = activity_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""black"", size = 0.5) +
  geom_richtext(data = activity_labels, aes(label = glue(""<b>{str_to_title(activity)}</b>""), x = x, y = yend, color = activity), fill = ""white"", hjust = 1, vjust = 0, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_richtext(data = activity_labels, aes(label = glue(""{total}""), x = x + 1, y = -1), fill = ""white"", color = ""black"", hjust = 0, vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  coord_equal() +
  labs(x = NULL, 
       y = NULL) +
  theme_jk(grid = FALSE) +
  scale_x_continuous(expand = c(0,0)) +
  expand_limits(y = c(-5, 25), x = c(0, 200)) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

# Waffles by day ----

by_day_outlines <- activity_data %>% 
  ungroup() %>% 
  mutate(cum_sum = cumsum(value)) %>% 
  mutate(lines = map2(value, cum_sum, partition_waffle, nrows = 20, flip = FALSE)) %>% 
  unnest(lines) %>% 
  filter(activity == ""chasing"")

by_day_labels <- activity_data %>% 
  group_by(date) %>% 
  summarize(total = sum(value)) %>% 
  left_join(filter(by_day_outlines, yend == 21))

waffle_by_day <- activity_data %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = by_day_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""grey20"", size = 0.5) +
  geom_richtext(data = by_day_labels, aes(label = glue(""{month.abb[month(date)]} {day(date)}""), x = x, y = yend), fill = ""white"", color = ""black"", hjust = c(rep(1,10), 0), vjust = 0, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_richtext(data = by_day_labels, aes(label = glue(""{total}""), x = x, y = -1), fill = ""white"", color = ""black"", hjust = c(rep(0,9),1,0), vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  scale_x_continuous(expand = c(0,0)) +
  coord_equal(clip = ""off"") +
  labs(x = NULL, 
       y = NULL) +
  expand_limits(y = c(-5, 25), x = c(0, 205)) +
  theme_jk(grid = FALSE) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

# Waffle bars by day ----

waffle_day_outlines  <- activity_data %>% 
  arrange(desc(date)) %>% 
  filter(activity == ""foraging"") %>% 
  ungroup() %>%
  group_split(date) %>% 
  map(~mutate(.x, cum_sum = cumsum(value))) %>%
  map_dfr(~mutate(.x, lines = map2(value, cum_sum, partition_waffle, nrows = 20, flip = FALSE))) %>%
  unnest(lines) 

waffle_labels <- waffle_day_outlines %>% 
  filter(y == -1)

mday_label <- function(x) {
  
  glue(""{month.abb[month(x)]} {day(x)}"")
  
}

split_waffle_by_day <- activity_data %>% 
  arrange(desc(date)) %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = waffle_day_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""grey20"", size = 0.5) +
  geom_richtext(data = waffle_labels, aes(label = glue(""{value}""), x = x + 1, y = -1), fill = ""white"", color = ""black"", hjust = 0, vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  coord_equal() +
  scale_color_manual(values = pal) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE) +
  facet_wrap(~date, nrow = 1, as.table = FALSE, strip.position = ""top"", labeller = labeller(date = mday_label)) +
  expand_limits(y = c(-5, 20)) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

out <- wrap_plots(waffle_by_activity, waffle_by_day, split_waffle_by_day, ncol = 1) +
  plot_annotation(title = ""Breakdown of Observed Squirrel Activity from the 2018 NYC Squirrel Census"",
                  subtitle = glue(""Below are  waffle charts of activity totals (I), daily totals (II) and exploded daily activity (III) views of observed squirrel activity.<br>{highlight_text('Foraging', '#DD2A7B', 'b')} is the most frequently observed activity recorded in the census, not surprising for squirrels in the fall.""),
                  caption = ""Data: **NYC Data Portal** | Graphic: **@jakekaupp**"",
                  tag_levels = ""I"",
                  theme = theme_jk(markdown = TRUE))

ggsave(here(""2019"", ""week44"", ""tw44_plot.png""), out, width = 11, height = 8, dev = ragg::agg_png(), dpi = ""print"")

",2019,44
54,54,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week45/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(glue)

commute_mode <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-05/commute.csv"")

total_avg <- commute_mode %>% 
  group_by(city, mode) %>% 
  filter(n() > 1) %>% 
  group_by(mode) %>% 
  summarize(avg = mean(percent/100)) %>% 
  mutate(state = ""US"",
         state_abb = ""US"")

slope_data <- commute_mode %>% 
  group_by(city, mode) %>% 
  filter(n() > 1) %>% 
  group_by(state, state_abb, mode) %>% 
  summarize(avg = mean(percent/100)) %>% 
  ungroup() %>% 
  mutate(state_abb = ifelse(is.na(state_abb), ""DC"", state_abb))

direct_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  top_n(8, avg) %>% 
  arrange(-avg) %>% 
  slice(-3:-6)

direct_labels_bike <- slope_data %>% 
  filter(mode == ""Bike"") %>% 
  top_n(5, avg)

mid_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  top_n(8, avg) %>% 
  arrange(-avg) %>% 
  slice(3:6) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
    avg = mean(.$avg),
    y = min(.$avg),
    yend = max(.$avg))
  
lower_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  arrange(-avg) %>% 
  slice(9:nrow(.)) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
            avg = mean(avg),
            y = min(.$avg),
            yend = max(.$avg))

lower_bike_labels <- slope_data %>% 
  filter(mode == ""Bike"") %>% 
  arrange(-avg) %>% 
  slice(6:nrow(.)) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
            avg = mean(avg),
            y = min(.$avg),
            yend = max(.$avg))


ggplot(slope_data, aes(x = mode, y = avg)) +
  geom_line(aes(group = state), size = 0.2) +
  geom_line(data = total_avg, aes(group = state), color = ""#DD2A7B"", size = 1) +
  geom_point(shape = 21, color = ""white"", stroke = 0.2, fill = ""black"", size = 2) +
  geom_text(data = direct_labels, aes(label = state_abb),  nudge_x = 0.05, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = direct_labels_bike, aes(label = state_abb),  nudge_x = -0.05, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = mid_labels, aes(label = state_abb),  nudge_x = 0.1, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = lower_labels, aes(label = str_wrap(state_abb, 30)), nudge_x = 0.1, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = lower_bike_labels, aes(label = str_wrap(state_abb, 30)), nudge_x = -0.5, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.06, xend = 2.06, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.01, xend = 2.06, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.01, xend = 2.06, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.06, xend = 2.09, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.06, xend = 2.06, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.01, xend = 2.06, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.01, xend = 2.06, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.06, xend = 2.09, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.95, xend = 0.95, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.98, xend = 0.95, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.98, xend = 0.95, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.95, xend = 0.91, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = NULL,
       y = NULL,
       title = ""Bicycling and Walking to Work in the United States: 2008-2012"",
       subtitle = glue(""Illustrated below is a slopegraph contrasting the percentage of population that bikes to work and the percentage<br>that bikes to work as well as {highlight_text('the US average', '#DD2A7B', 'b')}"")) +
  scale_x_discrete(labels = c(""Bike to Work"", ""Walk to Work"")) +
  theme_jk(grid = ""XY"",
           markdown = TRUE) +
  theme(panel.grid.major = element_line(linetype = ""dashed""))
  
",2019,45
55,55,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week5/R/analysis.R,"library(tidyverse)
library(ggalt)
library(jkmisc)
library(here)
library(scales)

milk_cow_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-29/milkcow_facts.csv"")

milk_product_facts <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-29/milk_products_facts.csv"")

us_pop <- read_csv(here(""2019"", ""week5"", ""data"",""us-population-1990-to-2016.csv""))

totals <- milk_product_facts %>% 
  mutate(total_consumption = rowSums(select(., -year))) %>% 
  select(year, total_consumption)

full_data <- left_join(milk_cow_data, totals) %>% 
  left_join(us_pop) %>% 
  mutate(total_consumption_lbs = total_consumption * population)


ggplot(full_data, aes(y = total_consumption_lbs, x = milk_production_lbs)) +
  geom_xspline2(aes(s_open = TRUE, s_shape = 0.5)) +
  geom_point(shape = 21, fill = ""black"", color = ""white"", stroke = 1) +
  scale_y_continuous(labels = scales::unit_format(unit = ""B"", scale = 10e-10, sep = """"), breaks = pretty_breaks(6)) +
  scale_x_continuous(labels = scales::unit_format(unit = ""B"", scale = 10e-10, sep = """"), breaks = pretty_breaks(6)) +
  labs(x = ""US Milk Production (lbs)"",
       y = ""US Average Dairy Consumption (lbs)"",
       title = ""100 Slices of American Cheese or, the Fable of Supply Management"",
       subtitle = str_wrap(""The connected scatterplot below illustrates the relationship between total average dairy consumption and total milk production over the past 25 years.
                           US supply far exceeds the demand, highlighting overproduction and a case for supply management."", 120)) +  
  theme_jk()
",2019,5
56,56,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week6/R/analysis.R,"library(tidyverse)
library(ggalt)
library(jkmisc)
library(lubridate)
library(here)
library(scales)
library(janitor)
library(ggrepel)
library(patchwork)

state_hpi <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-05/state_hpi.csv"")

prime_rates <- read_csv(here(""2019"",""week6"",""data"",""MPRIME.csv"")) %>% 
  clean_names() %>% 
  mutate(year = year(date)) %>% 
  select(-date) %>% 
  group_by(year) %>% 
  summarize_all(mean) %>% 
  filter(year %in% min(state_hpi$year):max(state_hpi$year))

highs <- filter(prime_rates, mprime %in% range(mprime)) %>% 
  distinct(mprime, .keep_all = TRUE)

plot_data <- state_hpi %>% 
  group_by(year, state) %>% 
  summarize_all(mean, na.rm = TRUE) 

prime <- ggplot(prime_rates, aes(x = year, y = mprime)) +
  geom_line(color = viridis_pal()(1), size = 0.5) +
  geom_point(data = highs, color = viridis_pal()(1)) +
  geom_text_repel(data = highs, aes(label = paste0(mprime, ""%"")), color = viridis_pal()(1), nudge_x = 2, nudge_y = 2, family = ""Oswald"", segment.size = 0) +
  scale_x_continuous(breaks = c(1975, seq(1980, 2010, 10), 2018)) +
  theme_jk(grid = ""XY"") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Interest Rates Fall, Housing Prices on the Rise"",
       subtitle = str_wrap(""The top chart shows the average prime interest rate by year since 1975.  The bottom heatmap illustrates the yearly average housing price index by State since 1975."", 100))


heatmap <- ggplot(plot_data, aes(x = year, y = fct_reorder(state, price_index, .fun = mean), fill = price_index)) +
  geom_tile(color = ""white"", size = 0.05) +
  scale_x_continuous(breaks = c(1975, seq(1980, 2010, 10), 2018)) +
  scale_fill_viridis_c(""House Price Index"", option = ""viridis"", direction = 1, breaks = pretty_breaks(5)) +
  scale_color_identity() +
  labs(x = NULL, y = NULL, caption = ""Data: FRED | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"",
        legend.key.width = unit(1, ""cm""))


out <- patchwork::wrap_plots(prime, heatmap, heights = c(0.2,1), ncol = 1)

ggsave(here(""2019"", ""week6"", ""tw6_plot.png""), out, width = 8, height = 10)
",2019,6
57,57,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week7/R/analysis.R,"library(tidyverse)
library(here)
library(readxl)
library(janitor)
library(jkmisc)
library(nord)

oecd_data <- here(""2019"", ""week7"", ""data"", ""OECD--1.xlsx"") %>% 
  read_excel(skip = 1, na = c(""na"")) %>% 
  clean_names() %>% 
  filter(!is.na(x1995)) %>% 
  rename(country = x1) %>% 
  gather(year, intensity, -country) %>% 
  arrange(country, year) %>% 
  fill(intensity, .direction =  ""down"") %>% 
  mutate(year = parse_number(year)) %>% 
  group_by(year) %>% 
  arrange(year, intensity) %>% 
  mutate(rank = row_number(-intensity)) %>% 
  ungroup() %>% 
  mutate(color = if_else(country == ""Canada"", nord(""victory_bonds"")[2], nord(""snowstorm"", 1)),
         text_color = if_else(country == ""Canada"", nord(""snowstorm"", 1), ""black""))




plot <- ggplot(oecd_data, aes(x = year, y = -rank, group = country)) +
  geom_line(aes(color = color)) +
  geom_point(aes(color = color)) +
  geom_text(data = filter(oecd_data, year == min(year)), aes(label = rank, color = color), x = 1994, hjust = 0, family = ""Oswald"") +
  geom_text(data = filter(oecd_data, year == max(year)), aes(label = country, color = color), x = 2016.5, hjust = 0, family = ""Oswald"") +
  expand_limits(x = c(1994, 2019)) +
  scale_x_continuous(breaks = 1995:2016) +
  scale_color_identity() +
  theme_jk(dark = TRUE, grid = FALSE) +
  theme(axis.text.y = element_blank()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Canada is Losing a Step In the Global Research Race."",
       subtitle = str_wrap(""Shown below is the ranking of research intensity (% of Gross Domestic Product devoted to Research) from 1995-2016. Canada has been on a decline since hitting a peak in 2001.  Most notably is 2009-2016, which coincides with the systematic defunding of Canadian research scientists by the Conservative Harper Government."", 120),
       caption = ""Data: OECD | Graphic: @jakekaupp"")

ggsave(here(""2019"", ""week7"", ""tw7_plot.png""), plot, width = 9, height = 4.5)
",2019,7
58,58,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week8/R/analysis.R,"library(tidyverse)
library(here)
library(fs)
library(readxl)
library(janitor)
library(jkmisc)
library(scales)
library(egg)

parse_table31 <- function(file) {
  
  df <- file %>% 
    read_excel(na = ""na"") %>% 
    clean_names()
  
  start <- min(str_which(df$x3,""\\d{4}""))
  
  end <- pull(df, 1) %>% 
    str_which(""Since"") %>% 
    max()
  
  df <- slice(df, start:end)
  
  field_idx <- select(df, -1) %>% 
    map_df(is.na) %>% 
    pmap_lgl(all)
  
  labels <- select(df, 1) %>%
    filter(field_idx) %>% 
    na.omit() %>% 
    pull() %>% 
    str_remove(""[abcd]$"")
  
  years <- slice(df, 1) %>% 
    select(-1) %>% 
    flatten_chr() %>% 
    str_remove(""\\.0+$"")
  
 rep <- filter(df, !field_idx) %>% 
    slice(-1) %>% 
    select(1) %>% 
    n_distinct()
  
  filter(df, !field_idx) %>% 
    slice(-1) %>% 
    mutate(discipline = rep(labels, each = rep)) %>% 
    set_names(c(""category"", years, ""discipline"")) %>% 
    gather(year, value, matches(""[0-9]{4}"")) %>% 
    mutate_at(vars(-category, -discipline), as.numeric) %>% 
    mutate_at(""category"", function(x) str_remove(x, ""[abcd]$""))
  
  
}


files <- here(""2019"",""week8"",""data"") %>% 
  dir_ls() 

data <- map_df(files, parse_table31) %>% 
  ungroup() %>% 
  distinct()

plot_data <- data %>% 
  filter(discipline != ""Other"", !str_detect(category, ""doctoral"")) %>% 
  filter(!str_detect(discipline, ""and (?!computer)"")) %>% 
  filter(!str_detect(discipline, ""Physical"")) %>% 
  arrange(year) %>% 
  group_by(category, discipline, year) %>% 
  summarize(value = mean(value, na.rm = TRUE)) %>% 
  ungroup()

plot <- ggplot(plot_data, aes(x = year, y = value, color = discipline)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(breaks = pretty_breaks(), limits = c(0, 25)) +
  scale_color_manual(""Discipline"", values = tol7qualitative) +
  scale_x_continuous(limits = c(1985, 2017)) +
  expand_limits(x = c(1985, 2025)) +
  facet_wrap(~category, labeller = as_labeller(str_to_title), nrow = 1) +
  labs(x = NULL,
       y = NULL,
       title = ""Median Completion Time for Doctoral Degrees Are Getting Shorter"",
       subtitle = str_wrap(""Median completion time in years from 1985 to 2017 contrasting selected disciplines for both University and Graduate School experience.  Education, Humanities and Social Sciences doctoral candidates have a higher than average time to completion in both categories compared to other disciplines. "", 120),
       caption = ""Data: NSF | Analysis: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"")

ggsave(here('2019','week8',""tw8_plot.png""), plot, width = 10, height = 7)


bonus_plot_data <- plot_data %>% 
  filter(discipline == ""All fields"") %>% 
  rename(overall = value) %>% 
  select(-discipline) %>% 
  left_join(filter(plot_data, discipline != ""All fields"")) %>% 
  mutate(diff = abs(value - overall)/2) %>%
  filter(between(year, 1990, 2011))


order <- c(""Education"", ""Mathematics and computer sciences"",  ""Engineering"", ""Humanities"",""Life sciences"", 
   ""Social sciences"")

bonus_plot <- ggplot(bonus_plot_data, aes(ymin = -diff, ymax = diff, x = year, fill = fct_relevel(discipline, order))) +
  geom_ribbon(color = ""white"", size = 0.2, alpha = 0.8) +
  geom_segment(data = filter(bonus_plot_data, year == 2000, category == ""Since bachelor's"", discipline == ""Education""), aes(y = -diff, yend = diff, x = year, xend = year), color = ""grey20"", arrow = arrow(length = unit(0.25, ""cm""), ends = ""both"", type = ""closed"")) +
  facet_wrap(~category, labeller = as_labeller(str_to_title)) +
  scale_fill_viridis_d(""Discipline"") +
  scale_y_continuous(breaks = c(-5, -2.5, -1, 0, 1, 2.5, 5), labels = c(""10 Years"", ""5 Years"", ""2 Years"", ""Group Median"", ""2 Years"", ""5 Years"", ""10 Years"")) +
  labs(x = NULL,
       y = NULL,
       title = ""Relative Differences in Median Doctoral Completion Time from the Group Median by Discipline and Interval"",
       subtitle = str_wrap(""The streamgraph below presents the difference between discipline median completion time and the group median completion time (All Fields) as the width of each colored band (discipline) from 1990 to 2011."", 120),
       caption = ""Data: NSF | Analysis: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"")

bonus_plot <- tag_facet(bonus_plot, x = 1996, y = 5.5, open = """", close = """", tag_pool = c(""Difference from Group Median = 9.1 years"", """"),  fontface = 1, family = ""Oswald"") 



ggsave(here('2019','week8',""tw8_bonus_plot.png""), bonus_plot, width = 12, height = 6)
",2019,8
59,59,https://github.com/jkaupp/tidytuesdays,jkaupp,tidytuesdays,2019/week9/R/analysis.R,"library(tidyverse)
library(here)
library(ggforce)
library(jkmisc)
library(sf)
library(osmdata)

full_trains <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/full_trains.csv"")

available_tags(""railway"")

railways <- st_read(here(""2019"", ""week9"", ""data"", ""railways.shp""))

q <- getbb(""fr"") %>%
  opq(timeout=25*1000)%>%
  add_osm_feature(""railway"")

stations <- osmdata_sf(q)

ggplot(stations$osm_lines) +
  geom_sf()

railways$geometry[[1]] %>% st_coordinates() %>% as_tibble -> line

ggplot(line, aes(x = X, y = Y)) +
  geom_link2() +
  coord_sf(datum=NA)

nat_trains <- full_trains %>% 
  filter(service == ""National"") %>% 
  group_by(year, departure_station, arrival_station) %>% 
  summarize_at(vars(journey_time_avg, total_num_trips, avg_delay_late_at_departure, avg_delay_late_on_arrival), mean, na.rm = TRUE)


# Orbit test

centre <- ""PARIS LYON""

test_data <- filter(nat_trains, departure_station == centre | arrival_station == centre) %>% 
  arrange(departure_station)



positions <- test_data %>% 
  filter(arrival_station == centre) %>% 
  group_by(arrival_station, departure_station) %>% 
  summarize(dist = mean(journey_time_avg)) 


circles <- test_data %>% 
  group_by(departure_station) %>% 
  summarize(centre_radius = mean(avg_delay_late_at_departure)) %>% 
  left_join(positions)

main <- circles %>% 
  filter(departure_station == centre)

circles <- circles %>% 
  filter(departure_station != centre) %>% 
  mutate(fraction = nrow(.) - (nrow(.) - seq_along(departure_station)),
         delta = 360/nrow(.)*fraction) %>% 
  bind_rows(main) %>% 
  mutate(x0 = if_else(departure_station == centre, 0, dist*cos((delta*pi/180))),
         y0 = if_else(departure_station == centre, 0, dist*sin((delta*pi/180)))) 


link_coords <- function(dept, arr, lnk) {
  
  circles %>% 
    filter(departure_station == dept | departure_station == arr) %>%
    summarise(x = ifelse(lnk == ""from"", x0[x0 != 0], 0),
           xend = ifelse(lnk == ""from"", 0, x0[x0 != 0]),
           y = ifelse(lnk == ""from"", y0[y0 != 0], 0),
           yend = ifelse(lnk == ""from"", 0, y0[y0 != 0]))
  
  
  
}

links <- test_data %>% 
  group_by(departure_station, arrival_station) %>% 
  mutate(total_delay = ((avg_delay_late_at_departure + avg_delay_late_on_arrival)/journey_time_avg),
         total_trips = sum(total_num_trips)) %>% 
  summarize(size = mean(total_delay),
            alpha = mean(total_num_trips)/max(total_num_trips)) %>% 
  mutate(link = if_else(departure_station == centre, ""to"", ""from"")) %>% 
  ungroup() %>% 
  mutate(links = pmap(list(departure_station, arrival_station, link), ~link_coords(..1, ..2, ..3))) %>% 
  unnest() %>% 
  arrange(link, departure_station)




ggplot() +
  geom_curve(data = links, aes(x = x, xend = xend, y = y, yend = yend, size = size, color = link, alpha = alpha), lineend = ""round"", angle = 270) +
  geom_circle(data = circles, aes(x0 = x0, y0 = y0, group = departure_station, r = 5), fill = ""white"", color = ""#2b41a7"") +
  scale_size(range = c(1,6)) +
  scale_color_manual(values = c(""#2b41a7"", ""#c7ad24"")) +
  scale_fill_distiller(palette = ""Greys"")+
  scale_alpha_identity() +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank())

",2019,9
60,60,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week1/R/tw1_plot.R,"library(here)
library(readxl)
library(tidyverse)
library(glue)
library(ggrepel)

tidy_data <- dir(here(""week1"", ""data""), full.names = TRUE, pattern = ""us_avg"") %>%
  read_excel() %>%
  gather(year, avg_tuition, -State) %>%
  rename(state = State)


nat_avg <- tidy_data %>%
  filter(year %in% c(""2005-06"", ""2015-16"")) %>%
  group_by(year) %>%
  summarize(avg_tuition = mean(avg_tuition)) %>%
  mutate(state = ""National Average"")


plot_data <- tidy_data %>%
  filter(year %in% c(""2005-06"", ""2015-16"")) %>%
  left_join(select(nat_avg, year, nat_avg = avg_tuition), by = ""year"") %>%
  bind_rows(nat_avg)

labels <- plot_data %>%
  group_by(state) %>%
  filter(all(avg_tuition > nat_avg)) %>%
  pull(state) %>%
  unique()

plot <- plot_data %>%
  ggplot(., aes(x = year, y = avg_tuition, group = state)) +
  geom_text_repel(data = filter(plot_data, state %in% labels, year == ""2015-16""), aes(label = state), direction = ""y"", nudge_x = 0.1, segment.size = 0.1, hjust = 0, family = ""Oxygen"", size = 3) +
  geom_path(color = ""grey50"", size = 0.5, alpha = 0.5) +
  geom_point(color = ""grey50"") +
  geom_path(data = nat_avg, color = ""red"", size = 1) +
  geom_point(data = nat_avg, color = ""red"") +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = NULL, y = NULL, title = ""Comparison of the average US tuition growth between 2005 and 2015"", subtitle = ""Eastern and Northeastern students consistently face tutition above the national average, indicated by the red line."", caption = ""\nData: http://trends.collegeboard.org/ | Graphic: @jakekaupp"") +
  theme_minimal(base_family = ""Oswald Light"") +
  theme(panel.grid.minor = element_blank())

ggsave(plot, filename = glue('{here(""week1"")}/tidyweek-{Sys.Date()}.png'), height = 8, width = 6, dpi = 300)

",2018,1
61,61,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week11/R/tw11_plot.R,"library(here)
library(tidyverse)
library(treemap)
library(sysfonts)
library(showtext)
library(grid)
library(nord)

raw_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week11_fifa_audience.csv"") %>% 
  select(-X1)

font_add_google(""Oswald"",""Oswald-Light"", regular.wt = 300)
font_add_google(""Scope One"",""Scope One"")

showtext_auto()

vplayout <- function(x, y) viewport(width=11/3, height=8.5, layout.pos.row = x, layout.pos.col = y)

build_treemap <- function(x, y, size)  {
  
  title <- set_names(c(""Population Share"", ""TV Audience Share"", ""GDP Weighted Share""), c(""population_share"",""tv_audience_share"", ""gdp_weighted_share""))
  
  treemap(raw_data,
          index = c(""confederation"",""country""),
          vSize = size,
          vColor = ""confederation"",
          type = ""categorical"",
          title = title[size],
          title.legend = """",
          fontfamily.title = ""Oswald-Light"",
          fontsize.labels = c(20, 10),
          fontfamily.labels = ""Oswald-Light"",
          fontcolor.labels = ""#f0f0f0"",
          lowerbound.cex.labels = 1,
          bg.labels = 0,
          inflate.labels = FALSE,
          border.col = ""white"",
          border.lwds = 1,
          position.legend = ""none"",
          palette = nord(""baie_mouton""),
          align.labels = list(c(""left"",""top""), c(""right"",""bottom"")),
          drop.unused.levels = TRUE,
          vp = vplayout(x,y))
  
  
  
}

fifa_maps <- function() {
  grid.newpage()
  pushViewport(viewport(layout = grid.layout(3, 3, heights = c(0.1, 0.8, 0.1))))
  par(mai=c(0,0,0,0))
  
  grid.text(""Comparing FIFA Share Differences by Confederation and Country"", x = 0.1, hjust = 0, vp = vplayout(1,1), gp = gpar(fontfamily = ""Oswald-Light"", fontsize = 30))
  build_treemap(2, 1, ""population_share"")
  build_treemap(2, 2, ""tv_audience_share"")
  build_treemap(2, 3, ""gdp_weighted_share"")
  grid.text(""Data: fivethirtyeight.com | Graphic: @jakekaupp"", x = 0.5, vp = vplayout(3,3), gp = gpar(fontfamily = ""Scope One"", fontsize = 10))
  
}

png(here(""week11"", ""Fifa Treemaps.png""), width = 11, height=8.5, units = ""in"", res = 100)
fifa_maps()
dev.off()

",2018,11
62,62,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week12/R/tw12_plot.R,"library(tidyverse)
library(lubridate)
library(jkmisc)
library(ggridges)
library(nord)
library(here)
library(showtext)

font_add_google(""Oswald"", ""Oswald"", regular.wt = 400)
font_add_google(""Scope One"", ""Scope One"")

trend_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_google_trends.csv"", skip = 2, col_names = TRUE) %>% 
  set_names(str_extract(names(.), ""(?<=Hurricane )(\\w+)|(Day)"")) %>% 
  rename(Date = Day) %>% 
  mutate(source = ""Google Trends"") %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

mediacloud_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_mediacloud_hurricanes.csv"", col_names = TRUE) %>% 
  mutate(source = ""Online News"") %>% 
  mutate(Date = mdy(Date)) %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

tv_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/week12_tv_hurricanes.csv"") %>% 
  mutate(source = ""TV"") %>% 
  mutate(Date = mdy(Date)) %>% 
  mutate_at(vars(-Date, -source), funs(./max(.)))

all_data <- bind_rows(trend_data, mediacloud_data, tv_data) %>%
  gather(hurricane, value, -Date, - source) %>% 
  set_names(tolower(names(.)))

showtext_auto()

plot <- ggplot(all_data, aes(x = date, y = source)) +
  geom_ridgeline(aes(height = value, fill = factor(hurricane)), size = 0.1, scale = 0.8, alpha = 0.8) +
  labs(title = ""On nearly every form of media, hurricanes that hit mainland US received more sustained coverage than Maria in Puerto Rico"",
       subtitle = ""Ridgeline plots of normalized media shares (TV, Online News and Google Trends)"",
       caption = ""Data: fivethirtyeight | Graphic: @jakekaupp"",
       y = NULL,
       x = NULL) +
  scale_x_date(expand = c(0,0)) +
  scale_fill_nord(name = ""Hurricane"", palette = ""lumina"") +
  theme_jk(plot_title_size = 28, subtitle_size = 24, base_size = 20, caption_size = 12,  grid = ""X"") +
  theme(axis.text.y = element_text(vjust = -2))

ggsave(plot, filename = here(""week12"", ""ROCK YOU LIKE A HURRICANE.png""), width = 6, height = 3)
",2018,12
63,63,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week18/R/tw18_plot.R,"library(here)
library(tidyverse)
library(treemap)
library(sysfonts)
library(showtext)
library(grid)
library(nord)
library(readxl)
library(ggalt)
library(jkmisc)


raw_data <- read_xlsx(here(""week18"", ""data"", ""week18_dallas_animals.xlsx""), sheet = 1)

data <- raw_data %>% 
  filter(animal_type %in% c(""CAT"",""DOG""), mo_year == ""2017"") %>% 
  count(animal_type, month, mo_year, outcome_type) %>% 
  group_by(animal_type, month, mo_year) %>% 
  mutate(percent = n/sum(n)) %>% 
  ungroup() %>% 
  select(-n) %>% 
  filter(outcome_type %in% c(""EUTHANIZED"",""ADOPTION"")) %>% 
  mutate_if(is.character, tolower) %>%
  spread(outcome_type, percent) %>% 
  mutate_if(is.character, tools::toTitleCase) %>% 
  mutate(month = ifelse(month == ""may"", ""May"", month)) %>% 
  arrange(month) %>% 
  complete(month = month.abb, mo_year, animal_type, fill = list(adoption = NA, euthanized = NA)) %>% 
  mutate(ratio = adoption/euthanized) %>% 
  mutate(month = factor(month, month.abb))



ggplot(data, aes(x = month, y = ratio, group = animal_type, color = animal_type)) +
  geom_hline(yintercept = 1.0, size = 0.1, color = ""firebrick"", linetype = ""dashed"") +
  geom_line(size = 0.5) +
  geom_text(data = filter(data, month == ""Sep""), aes(label = animal_type), nudge_x = 0.3, family = ""Oswald"") +
  theme_jk(grid = ""XY"") +
  scale_color_nord(""victory_bonds"") +
  labs(x = NULL, y = ""Ratio of Adopted/Euthanized"", title = ""2017 was a bad time to be a cat in a shelter"", subtitle = ""Cats in the shelters were euthanized more than adopted compared to dogs."") +
  theme(legend.position = ""none"")
  



",2018,18
64,64,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week2/R/tw2_plot.R,"library(here)
library(readxl)
library(tidyverse)
library(glue)
library(janitor)
library(rvest)
library(nord)
library(jkmisc)
library(viridis)

# Function to scrape the top avg cap salary by player ----
pull_position_data <- function(year, position) {
  
  Sys.sleep(5)
  
  url <- glue(""http://www.spotrac.com/nfl/positional/{year}/{position}"")
  
  read_html(url) %>% 
    html_nodes(""#main > div.teams > table:nth-child(6)"") %>% 
    html_table() %>%
    flatten_df() %>% 
    set_names(c(""rank"",""player"",""cap_dollars"", ""cap_percent""))
}


# Formatter for 538 year labels 
labels_538 <- function(labels) {
  labels_out <- sprintf(""20%s"", str_sub(labels, 3, 4))
  labels_out <- c(labels_out[1], glue(""'{str_sub(labels_out[-1], 3, 4)}""))
  return(labels_out)
}

# Create the data scaffold 
years <- 2011:2018
positions <- c(""quarterback"", ""running-back"", ""fullback"", ""guard"", ""center"", ""left-tackle"", ""right-tackle"", ""tight-end"",""wide-receiver"",""defensive-end"",""cornerback"",""defensive-tackle"", ""inside-linebacker"", ""outside-linebacker"", ""free-safety"", ""strong-safety"", ""kicker"",""punter"",""long-snapper"")

scaffold <- tibble(year = years,
                   position = list(positions)) %>% 
  unnest() 

# Populate the scaffold
if(!file.exists(here(""week2"", ""data"", ""position_cap_data_named.RDS""))) {
  
  scaffold <- scaffold %>% 
    mutate(data = map2(year, position, ~pull_position_data(.x, .y))) %>% 
    unnest() %>% 
    mutate_at(c(""cap_dollars"", ""cap_percent""), parse_number) %>% 
    mutate(side = case_when(position %in% c(""quarterback"", ""running-back"", ""fullback"", ""guard"", ""center"", ""left-tackle"", ""right-tackle"", ""tight-end"",""wide-receiver"") ~ ""Offense"",
                            position %in% c(""kicker"",""punter"",""long-snapper"") ~ ""Special Teams"",
                            TRUE ~ ""Defense""))
  
  
  # Save it to avoid re-scraping 
  saveRDS(scaffold, file = here(""week2"", ""data"", ""position_cap_data_named.RDS""))
} else {
  
  scaffold <- readRDS(here(""week2"", ""data"", ""position_cap_data_named.RDS""))
  
}


# Make data for the plot
plot_data <- scaffold %>% 
  group_by(year, position, side) %>% 
  top_n(16, cap_dollars) %>% 
  summarize(avg_pay = mean(cap_dollars))
  
# Make a heatmap!
ggplot(plot_data, aes(x = year, y = position, fill = avg_pay)) +
  geom_tile(color = ""white"", size = 0.1) +
  coord_equal() +
  labs(x = NULL, y = NULL, title = ""The Fullback Gets No Respect"", subtitle = ""Average cap value of the 16 highest payed players in all positions"", caption = ""Data: http://www.spotrac.com/ | Graphic: @jakekaupp"") +
  scale_x_continuous(labels = labels_538, breaks = 2011:2018) +
  scale_y_discrete(labels = function(x) str_to_title(gsub(""[[:punct:]]"", "" "", x))) +
  scale_fill_viridis(discrete = FALSE, labels = scales::dollar, name = ""Average Salary"") +
  theme_jk(grid = FALSE, base_size = 14)

ggsave(here(""week2"", ""tw2_heatmap.png""), width = 8, height = 8)
",2018,2
65,65,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week3/R/tw3_plot.R,"library(tidyverse)
library(here)
library(readxl)
library(jkmisc)

# Read in the data
mortality_data <- dir(here(""week3"",""data""), pattern = ""global"", full.names = TRUE) %>% 
  read_excel()


# Tidy up the data
tidy_mort <- mortality_data %>% 
  gather(cause_of_death, percentage, -country:-year) %>% 
  mutate(cause_of_death = trimws(str_remove(cause_of_death, ""\\(\\%\\)""))) %>% 
  mutate(percentage = ifelse(is.na(percentage), NA, percentage/100))

# Get just the data pertaining to suicides
suicide_data <- tidy_mort %>% 
  filter(cause_of_death == ""Suicide"", !is.na(country_code))

# Get the World percentage
global_rate <- suicide_data %>% 
  filter(country == ""World"") %>% 
  select(year, percentage)

# Get the top 40 problem countries, those with the suicide rate constantly over the world average (note the all statement in the filter)
problem_countries <- suicide_data %>% 
  filter(country != ""World"") %>% 
  left_join(global_rate, by = ""year"") %>% 
  group_by(country) %>% 
  filter(all(percentage.x > percentage.y)) %>% 
  summarize(percentage = mean(percentage.x, na.rm = TRUE)) %>% 
  top_n(40, percentage) %>% 
  arrange(desc(percentage)) %>% 
  pull(country)

# Create the data to make the plot, and arrange descending by the overall avg rate of suicide
plot_data <- suicide_data %>% 
  filter(country %in% problem_countries) %>% 
  mutate(country = factor(country, problem_countries))

# Create the sad plot
sad_plot <- ggplot(plot_data, aes(x = year, y = percentage)) +
  geom_segment(aes(x = min(year), xend = max(year), y = 0, yend = 0)) +
  geom_area(fill = ""steelblue4"") +
  geom_path(color = ""grey30"", size = 0.2) +
  geom_area(data = global_rate, fill = ""steelblue3"") +
  geom_path(data = global_rate, color = ""grey30"", size = 0.2) +
  facet_wrap(~country, nrow = 5) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = ""Countries Coping With the Tradgedy and Pain of Suicide"",
       subtitle = ""Dark blue indicates suicide rate by year, Light blue fill indicates the global average suicide rate by year."",
       x = NULL,
       y = NULL,
       caption = ""Data: ourworldindata.org | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Y"")

# Save the sad plot
ggsave(here(""week3"",""tw3_sad_plot.png""), sad_plot, width = 16, height = 10)
",2018,3
66,66,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week4/R/tw4_plot.R,"library(tidyverse)
library(here)
library(jkmisc)
library(scales)
library(ggiraph)
library(glue)
library(waffle)

make_tooltip <- function(occupation, female, male, income_gap, ...) {
 
  glue('<div class=""tipchart"">
      <h3>{occupation}</h3>
      <h4>Mens taxable income {ifelse(income_gap <= 1, percent(1-round(income_gap, 2)), percent(round(income_gap, 2)))} {ifelse(income_gap <= 1, ""less"", ""more"")} than womens</h4>
      <table>
      <tr class=""tiprow"">
      <td class=""tipheader"">Average Male Taxable Income:</td>
      <td class=""tiptext"">{dollar(male)}</td>
      </tr>
      <tr class=""tiprow"">
      <td class=""tipheader"">Average Female Taxable Income:</td>
      <td class=""tiptext"">{dollar(female)}</td>
      </tr>
      </table>
      </div>')
  
}


# Read in the data
income_data <- dir(here(""week4"",""data""), pattern = ""salary"", full.names = TRUE) %>% 
  read_csv(locale = locale(""en""))
 
# Clean occupation up a bit.  Some rouge unicodes in there.
tidy_gap <- income_data %>% 
  mutate(occupation = iconv(occupation, ""UTF-8"", ""UTF-8"",sub='')) %>% 
  spread(gender, average_taxable_income) %>%
  set_names(tolower(names(.))) %>% 
  group_by(occupation) %>% 
  summarize_at(c(""female"", ""male""), sum, na.rm = TRUE) %>% 
  filter(female != 0, male != 0) %>% 
  mutate(income_gap = male/female)

plot_data <- tidy_gap %>% 
 mutate(fill = if_else(income_gap >= 1, ""grey80"", ""#ffd700""),
         alpha = if_else(income_gap >= 1, 0.2, 1)) %>% 
  mutate(tooltip = pmap(., make_tooltip)) %>% 
  mutate(tooltip = gsub(""\\\n"", """", tooltip)) %>% 
  mutate(tooltip = gsub(""'"", """", tooltip)) %>% 
  mutate(idx = row_number())

tooltip_css <- ""background-color:white;padding:10px;border-radius:20px 20px 20px 20px;border-color:black;border-style:solid;border-width:1px""

plot <- ggplot(plot_data, aes(x = female, y = male, fill = fill)) +
  geom_segment(x = 0, xend = 600000, y = 0, yend = 600000, size = 0.05, color = ""grey80"") +
  geom_point_interactive(aes(alpha = alpha, tooltip = tooltip, data_id = idx), shape = 21, color = ""grey30"", size = 3) +
  scale_y_continuous(labels = dollar, limits = c(0, 600000)) +
  scale_x_continuous(labels = dollar, limits = c(0, 600000)) +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL, 
       title = ""Gender Differences in Taxble Income in Australia"",
       subtitle = str_wrap(""Average male taxable income plotted against average female taxable income by occupation. Yellow dots indicate occupations where women have more taxable income than their male counterparts, 
       line indicates income equality. Hover over points for occupation, % difference and detailed income."", 100),
       caption = ""Data: data.gov.au | Graphic: @jakekaupp"") +
  theme_jk()

ggiraph(ggobj = plot, width_svg = 9, width = 1, tooltip_extra_css = tooltip_css)

waffle_data <- tidy_gap %>% 
  ungroup() %>% 
  mutate(category = case_when(income_gap > 1 ~ ""Men have more income"",
                              income_gap < 1 ~ ""Women have more income""))%>% 
  count(category) %>% 
  pull(n) %>% 
  set_names(., c(""Men have more income"", ""Women have more income""))

waffle(waffle_data, 
       rows = 14,
       size = 1,
       colors = c(""dodgerblue3"", ""deeppink""), 
       legend_pos = ""bottom"", 
       title = ""Out of 1092 occupations on record, men have more taxable income than women in 1011 of them.  That's 92.5% of occupations for those counting at home."") + 
  theme_jk() +
  labs(caption = ""Data: data.gov.au | Graphic: @jakekaupp"") +
  theme(axis.text = element_blank(),
        legend.position = ""bottom"")
",2018,4
67,67,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week5/R/tw_5plot.R,"library(tidyverse)
library(here)
library(janitor)
library(albersusa)
library(sf)
library(sp)
library(rgeos)
library(maptools)
library(ggthemes)
library(viridis)
library(scales)
library(glue)
library(jkmisc)


census_data <- dir(here(""week5"", ""data""), full.names = TRUE) %>% 
  read_csv() %>% 
  clean_names()

# Lets look at commuting!
commuting_data <- census_data %>% 
  select(census_id, state, county, total_pop, drive:mean_commute)

# Despacito is 3:47 in length
despacito_length <- 3 + 47/60

# Mapping things
us <- counties_composite()
us_map <- fortify(us, region=""fips"") 

# Calculate the how many despacitos the average commute has
despacito_commute <- commuting_data %>% 
  mutate(despacitos = mean_commute/despacito_length,
         id = ifelse(str_length(as.character(census_id)) < 5, glue(""0{census_id}""), as.character(census_id))) %>% 
  right_join(us_map)


# Make the map!
map <- ggplot() +
 geom_map(data = us_map, map = us_map,
                    aes(x = long, y = lat, map_id = id),
                    color =""#2b2b2b"", size = 0.05, fill = NA) +
  geom_map(data = despacito_commute, map = us_map,
           aes(x = long, y = lat, map_id = id, fill = despacitos),
           color =""#2b2b2b"", size = 0.05) + 
  scale_fill_viridis(name=""How many despactios?"", na.value = viridis(5, option = ""cividis"")[3], option = 'cividis', breaks = seq(1,12,2)) +
  labs(title = ""Just how much do you like your commute?"",
       subtitle = str_wrap(""What if your commute was defined by hearing a song on repeat?  
                           What if that song was the most streamed song on the planet, Despacito? 
                           Illustrated below is the average number of times you'd hear it on your way home across the US."", 80),
       caption = ""Data: census.gov | Graphic: @jakekaupp"") +
  coord_map() +
  theme_map(base_family=""Scope One"", 
            base_size = 16) +
  theme(legend.title = element_text(size = 10),
        plot.title = element_text(family = ""Oswald""),
        legend.background = element_rect(fill = NA),
        plot.caption = element_text(size = 10),
        legend.position = c(0.9,0.1))

ggsave(here(""week5"", ""tw5_choropleth map.png""), width = 10, height = 6)


",2018,5
68,68,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week6/R/tw_6plot.R,"library(tidyverse)
library(readxl)
library(here)
library(janitor)
library(glue)
library(fuzzyjoin)
library(stringi)
library(ggalt)
library(jkmisc)
library(nord)



provinces <- set_names(c(""Alberta"", ""British Columbia"", ""Manitoba"", ""New Brunswick"", ""Newfoundland and Labrador"",
                         ""Nova Scotia"", ""Northwest Territories"", ""Nunavut"", ""Ontario"", ""Prince Edward Island"", ""Quebec"",
                         ""Saskatchewan"", ""Yukon""),
                       c(""AB"", ""BC"", ""MB"", ""NB"", ""NL"", ""NS"", ""NT"", ""NU"", ""ON"", ""PE"", ""QC"", ""SK"", ""YT""))

# Just get the Tims data just for Canada
tim_hortons <- dir(here(""week6"", ""data""), full.names = TRUE, pattern = ""xlsx"") %>% 
  read_excel(sheet = ""timhorton"") %>% 
  filter(country == ""ca"") %>% 
  rename(province = state) 

# Counts at the City/Province level
tims_city_prov <- tim_hortons %>% 
  count(city, province)

# Counts at the National level
tims_national <- tim_hortons %>% 
  count(province) %>% 
  mutate(color = ifelse(province == ""ON"", nord(""victory_bonds"", 1), ""grey50""))

national <- ggplot(tims_national, aes(x = reorder(province,n), y = n)) +
  geom_lollipop(aes(color = color)) +
  scale_color_identity() +
  scale_y_continuous(expand = c(0.01,0.05),  breaks = scales::pretty_breaks()) +
  scale_x_discrete(labels = function(x) provinces[x]) +
  coord_flip() +
  labs(x = NULL, y = NULL, title = ""Ontario, we have a problem...."", subtitle = ""The highest number of Tim Hortons per province goes to Ontario, a land where you can't even get an oat cake."",
       caption = ""\nData: timhortons.com | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Xx"")

ggsave(plot = national, here(""week6"", ""National Tims.png""), width = 10, height = 6)

census_2011 <- dir(here(""week6"", ""data""), full.names = TRUE, pattern = ""2011 census"") %>% 
  read_csv() %>% 
  clean_names() %>% 
  remove_empty(""rows"") %>% 
  select(city = geographic_name, population = population_2011) %>% 
  mutate(province = stri_extract_last_regex(city, ""\\(([A-Za-z\\.]+?)\\)""),
         city = stri_replace_all_regex(city, ""\\((.*?)\\)"", """"),
         province = gsub(""[[:punct:]]"", """", province)) %>% 
  mutate(province = case_when(province == ""Que"" ~ ""QC"",
                              province == ""Ont"" ~ ""ON"",
                              province == ""Man"" ~ ""MB"",
                              province == ""Sask"" ~ ""SK"",
                              province == ""Alta"" ~ ""AB"",
                              province == ""NWT"" ~ ""NT"",
                              province == ""Nvt"" ~ ""NU"",
                              province == ""PEI"" ~ ""PE"",
                              TRUE ~ province)) %>% 
  mutate_if(is.character, trimws)



tims_density <- regex_right_join(census_2011, tims_city_prov,  by = c(""city"", ""province""), ignore_case = TRUE) 


plot_data <- tims_density %>% 
  select(population, city = city.y, province = province.x, n) %>% 
  group_by(city, province) %>% 
  summarize_at(c(""n"", ""population""), sum, na.rm = TRUE) %>% 
  ungroup() %>% 
  filter(population != 0, n > 1, population > 10000) %>% 
  mutate(density = (n/(population/1000))) %>% 
  top_n(25, density) %>% 
  mutate(color = ifelse(density == max(density), nord(""victory_bonds"", 1), ""grey50""))


most_tims <- ggplot(plot_data, aes(x = reorder(city, density), y = density)) +
  geom_lollipop(aes(color = color)) +
  scale_color_identity() +
  scale_y_continuous(expand = c(0,0.01),  breaks = scales::pretty_breaks(), limits = c(0,1.2)) +
  coord_flip() +
  labs(y = ""Number of Tim Hortons stores per 1,000 people"", x = NULL, title = ""However, the title of most Tim Hortons per capita belongs to Cold Lake, Alberta"", 
       subtitle = ""When looking at towns/cities with population > 10,000 and with more than two Tim Hortons. \nMy hometown of Truro, Nova Scotia comes in a puzzling fourth."",
       caption = ""\nData: timhortons.com | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Xx"")

ggsave(plot = most_tims, here(""week6"", ""Most Tims.png""), width = 10, height = 6)

",2018,6
69,69,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2018/week7/R/tw_7plot.R,"library(tidyverse)
library(here)
library(janitor)
library(likert)
library(jkmisc)
library(nord)

star_wars <- dir(here(""week7"", ""data""), pattern = ""StarWars"", full.names = TRUE) %>% 
  read_csv() 

clean_names <- stringi::stri_trans_general(names(star_wars), ""latin-ascii"") %>% 
  gsub(""[^\\x{00}-\\x{7f}]"", """", ., perl = TRUE) %>% 
  clean_names()

star_wars <- set_names(star_wars, clean_names) 

headers <- slice(star_wars, 1) %>% 
  flatten_chr()

clean_names <- gsub(""X\\d+"", NA_character_, clean_names) %>% 
  enframe() %>% 
  fill(value) %>% 
  pull(value)


shiny_clean_names <- paste(clean_names, headers, sep = ""|"")

long_star_wars <- set_names(star_wars, c(""RespondentID"", shiny_clean_names[-1])) %>% 
  slice(-1) %>% 
  gather(item, value, -1) %>% 
  separate(item, c(""question"", ""category""), sep = ""\\|"") %>% 
  mutate(category = if_else(category == ""Response"", NA_character_, category)) %>% 
  mutate(index = group_indices(., question))


plot_data <- long_star_wars %>% 
  filter(index == 12) %>% 
  replace_na(list(value = ""Unfamiliar (N/A)"")) %>% 
  filter(value != ""Unfamiliar (N/A)"") %>% 
  spread(category, value) %>% 
  mutate_at(vars(-RespondentID, -question, -index), function(x)
    factor(x, 
            levels = c(""Very unfavorably"", ""Somewhat unfavorably"",""Neither favorably nor unfavorably (neutral)"", ""Somewhat favorably"", ""Very favorably""),
            labels = 1:5
    )) 
            
            
likert_data <- plot_data %>% 
  select(-RespondentID, -question, -index) %>%
  as.data.frame() %>% 
  likert()


ggplot2::update_geom_defaults(""text"", list(family = ""Scope One"", size = 4))
  
plot <- likert.bar.plot(likert_data) + 
  scale_fill_nord(""mountain_forms"", labels = c(""Very unfavorably"", ""Somewhat unfavorably"",""Neither favorably nor unfavorably (neutral)"", ""Somewhat favorably"", ""Very favorably""), name = ""Response"") +
  labs(title = ""The Favorability Rankings of Star Wars Characters"", subtitle = ""People look favourably upon the scruffy nerf herder, and would give a ride to the EVIL RAISIN THAT SHOOTS LIGHTNING FROM HIS HANDS before the goofy gungan."") +
  theme_jk(grid = ""XY"") +
  theme(plot.title = element_text(family = ""Oswald""))

ggsave(here(""week7"", ""tw7_likert.png""), width = 16, height = 10)
  
  
 
",2018,7
70,70,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week1/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)


# week-of-month function
wom <- function(date) { 
  first <- wday(as.Date(paste(year(date),month(date),1,sep=""-"")))
  return((mday(date)+(first-2)) %/% 7+1)
}

# Get the TidyTuesday Tweets Data
tt_tweet_data <- readRDS(here(""2019"", ""week1"", ""data"", ""tidytuesday_tweets.rds""))

# Get the R tweet data 
r_tweet_data <- readRDS(here(""2019"", ""week1"", ""data"", ""rstats_tweets.rds""))

# Most that tweet about R tweet about the r4ds tidy tuesday.
no_rstats <- anti_join(tt_tweet_data, r_tweet_data, by = ""screen_name"") %>% 
  mutate(rstats_tag = case_when(grepl(""rstats"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""r4ds"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""visualization"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""data"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""code"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""plot"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""chart"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""graph"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""drob"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""ggplot"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""rstudio"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""model"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""median"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""average"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""week \\d+"", text, ignore.case = TRUE) ~ TRUE,
                                grepl(""@thomas_mock"", text, ignore.case = TRUE) ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  mutate(rstats_tag = case_when(screen_name %in% c(""NosyOwl"", ""sebastianhwells"", ""JenniferCai7"", ""matthwong"",
                                                   ""scrite_jones"", ""jrosenblum123"", ""zlipp"") ~ TRUE,
                                TRUE ~ rstats_tag)) %>% 
  filter(rstats_tag == FALSE)
  

plot_data <- anti_join(tt_tweet_data, no_rstats, by = ""screen_name"") %>% 
  mutate(created_at = as_date(created_at)) %>% 
  mutate(day = wday(created_at, label = TRUE, abbr = FALSE),
         week = wom(created_at),
         iweek = isoweek(created_at),
         month = month(created_at, label =  TRUE, abbr = FALSE),
         year = year(created_at))


count(plot_data, day, iweek) %>% 
  complete(day, iweek = 1:52, fill = list(n = NA)) %>% 
  ggplot(aes(x = iweek, y = day, fill = n)) +
  geom_tile(color = ""white"", size = 0.1) +
  scale_fill_viridis_c(""Tweet Frequency"", option = ""cividis"", na.value = ""grey95"", labels = seq(0,25,5), breaks = seq(0,25,5), limits = c(0,25)) +
  coord_equal() +
  labs(title = ""Tidy Tuesday or Tardy Tuesday?"",
       subtitle = ""A glance at when the community decides to submit their work."",
       y = NULL,
       x = ""Week of the Year"",
       caption = ""Data: rtweet | Analysis: @jakekaupp"") +
  scale_x_continuous(limits = c(1, 53), breaks = c(1,10,20,30,40,50), expand = c(0, 0)) +
  theme_jk(grid = FALSE, ticks = FALSE) +
  theme(legend.position = c(0.5,-0.7),
        legend.direction = ""horizontal"",
        legend.title = element_text(family = ""Scope One"", vjust = 0.8))

ggsave(here(""2019"", ""week1"",""tidy_or_tardy.png""), width = 8, height = 4)
",2019,1
71,71,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week10/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(ggrepel)
library(here)

jobs_gender <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/jobs_gender.csv"")


plot_data <- jobs_gender %>% 
  select(-year) %>% 
  group_by(occupation, major_category, minor_category) %>% 
  summarize_all(mean, na.rm = TRUE) %>% 
  filter(str_detect(occupation, ""engineer""), str_detect(major_category, ""Engineering"")) %>% 
  mutate(diff = if_else((total_earnings_female - total_earnings_male) > 0, ""firebrick"", ""grey80"")) %>% 
  mutate(alpha = if_else(diff == ""firebrick"", 1, 0.2)) %>% 
  gather(variable, value, starts_with(""total_earnings_"")) %>% 
  mutate(variable = factor(variable, c(""total_earnings_male"", ""total_earnings_female""), c(""Men"", ""Women"")))
 
slope_data <- build_slopegraph(plot_data, ""variable"", ""value"", ""occupation"") %>% 
  left_join(distinct(plot_data, occupation, diff, alpha), by = c(""group"" = ""occupation"")) %>% 
  mutate(group = case_when(str_detect(group, ""Mining"") ~ ""Mining Engineers"",
                                str_detect(group, ""Computer"") ~ ""Computer Engineers"",
                                str_detect(group, ""Electrical"") ~ ""Electrical Engineers"",
                                str_detect(group, ""Marine"") ~ ""Marine Engineers"",
                                str_detect(group, ""Industrial"") ~ ""Industrial Engineers"",
                           TRUE ~ str_to_title(group))) %>% 
  mutate(group = str_replace(group, ""Engineers"", ""Engineering""))


labels <- pretty(slope_data$y, 9)
breaks <- pretty(slope_data$ypos, 5)


plot <- ggplot(slope_data, aes(x = x, y = ypos, group = group, color = diff)) +
  geom_point() +
  geom_line() +
  geom_label_repel(data = filter(slope_data, x == ""Women""), aes(label = group), direction = ""y"", hjust = 0, nudge_x = 1, segment.alpha = 0.3, family = ""Oswald"", label.size = 0, fill = ""white"") +
  theme_jk(grid = ""XY"") +
  expand_limits(x = c(0, 5)) +
  scale_color_identity() +
  scale_alpha_identity() +
  scale_y_continuous(labels = scales::dollar(labels), breaks = breaks, limits = range(breaks)) +
  theme(panel.grid.major.x = element_line(linetype = ""dashed"", color = ""black"")) +
  labs(x = NULL,
       y = NULL,
       title = ""The Unnecessary and Unethical Pay Disparity in Engineering."",
       subtitle = str_wrap(""A slopegraph presenting the average total earnings from 2014-2016 for men and women across engineering disciplines.  Mining Engineering is the only discipline with women earning more than men on average."", 70),
       caption = ""Data: Census Bureau | Graphic: @jakekaupp"")

ggsave(here(""2019"", ""week10"", ""tw10_plot.png""), plot, width = 6.5, height = 9, type = ""cairo"")
",2019,10
72,72,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week10/R/helpers.R,"# Functions from an old friend.
# https://github.com/jkeirstead/r-slopegraph/blob/master/slopegraph.r

build_slopegraph <- function(df, x, y, group, min.space=0.05) {
  
  ## First rename the columns for consistency
  ids <- match(c(x, y, group), names(df))
  
  df <- df[,ids]
  
  names(df) <- c(""x"", ""y"", ""group"")
  
  ## Expand grid to ensure every combination has a defined value
  tmp <- expand.grid(x=unique(df$x), group=unique(df$group))
  
  tmp <- merge(df, tmp, all.y=TRUE)
  
  df <- mutate(tmp, y=ifelse(is.na(y), 0, y))
  
  spaced_sort(df, min.space=min.space)
  
}



spaced_sort <- function(df, min.space=0.05) {
  ## Define a minimum spacing (5% of full data range)
  min.space <- min.space*diff(range(df$y))
  
  ## Transform the data
  
  df <- split(df, ""x"") %>% 
    map_df(~calc_spaced_offset(.x, min.space))
  
  return(df)
}

##' Calculates the vertical offset between successive data points
##' 
##' @param df a data frame representing a single year of data
##' @param min.space the minimum spacing between y values
##' @return a data frame
calc_spaced_offset <- function(df, min.space) {
  
  ## Sort by value
  ord <- order(df$y, decreasing=T)
  ## Calculate the difference between adjacent values
  delta <- -1*diff(df$y[ord])
  ## Adjust to ensure that minimum space requirement is met 
  offset <- (min.space - delta)
  offset <- replace(offset, offset<0, 0)
  ## Add a trailing zero for the lowest value
  offset <- c(offset, 0)
  ## Calculate the offset needed to be added to each point
  ## as a cumulative sum of previous values
  offset <- rev(cumsum(rev(offset)))
  ## Assemble and return the new data frame
  df.new <- data.frame(group=df$group[ord],
                       x=df$x[ord],
                       y=df$y[ord],
                       ypos=offset+df$y[ord])
  return(df.new)
}
",2019,10
73,73,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week11/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(here)

board_games <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv"")

prolific <- board_games %>% 
  separate_rows(designer, sep = "","") %>% 
  filter(!str_detect(designer, ""Uncredited""), !str_detect(designer, ""Jr|III""), year_published >= 1990) %>% 
  group_by(designer) %>% 
  filter(n() >= 10) %>% 
  group_by(designer, year_published) %>% 
  summarize(year_avg_rating = mean(average_rating, na.rm = TRUE),
            games_published = n()) 

top_rated <- prolific %>% 
  group_by(designer) %>% 
  summarize(total_games = sum(games_published),
            overall_rating = mean(year_avg_rating, na.rm = TRUE)) %>% 
  top_n(1, overall_rating) %>% 
  pull(designer)

top_publishing <- prolific %>% 
  group_by(designer) %>% 
  summarize(total_games = sum(games_published),
            overall_rating = mean(year_avg_rating, na.rm = TRUE)) %>% 
  top_n(1, total_games) %>% 
  pull(designer)

overall_avg <- prolific %>% 
  group_by(year_published) %>% 
  summarize(year_avg_rating = mean(year_avg_rating, na.rm = TRUE),
            designer = ""Overall"")


plot_data <- prolific %>%
  bind_rows(overall_avg) %>% 
  mutate(color = case_when(designer == top_rated ~ ""#eebd31"" ,
                           designer == ""Overall"" ~ ""firebrick"",
                           designer == top_publishing ~ ""dodgerblue"",
                           TRUE ~ ""black""),
         alpha = case_when(designer == top_rated ~ 1,
                           designer == ""Overall"" ~ 1,
                           designer == top_publishing ~ 1,
                           TRUE ~ 0.05),
         size = case_when(designer == top_rated ~ 0.5,
                           designer == ""Overall"" ~ 0.5,
                           designer == top_publishing ~ 0.5,
                           TRUE ~ 0.3),
         point_size = case_when(designer == top_rated ~ 2,
                          designer == ""Overall"" ~ 2,
                          designer == top_publishing ~ 2,
                          TRUE ~ 1),
         line = case_when(designer == top_rated ~ ""solid"",
                           designer == ""Overall"" ~ ""dashed"",
                           designer == top_publishing ~ ""solid"",
                           TRUE ~ ""solid""))

plot <- ggplot(plot_data, aes(x = year_published, y = year_avg_rating, group = designer)) +
  geom_path(aes(color = color, alpha = alpha, linetype = line, size = size)) +
  geom_point(aes(fill = color, alpha = alpha, size = point_size), color = ""white"", shape = 21) +
  annotate(""label"", x = 1989.8, y = 2, label = ""Most Prolific: Reiner Knizia with 229 published games."", family = ""Oswald"", label.size = 0, fill = ""white"", color = ""dodgerblue"", hjust = 0) +
  annotate(""segment"", x = 1990, xend = 1990, y = 2.3, yend = 5.5, arrow = arrow(type = ""closed"", length = unit(1, ""mm"")), color = ""dodgerblue"") +
  annotate(""label"", x = 2002, y = 9, label = ""Highest Average Rating: Mark H. Walker with a 7.70 rating."", family = ""Oswald"", label.size = 0, fill = ""white"", color = ""#eebd31"") +
  annotate(""segment"", x = 2002, xend = 2002.8, y = 8.8, yend = 7.7, arrow = arrow(type = ""closed"", length = unit(1, ""mm"")), color = ""#eebd31"") +
  scale_y_continuous(limits = c(0, 10), breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_identity() +
  scale_linetype_identity() +
  scale_size_identity() +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL,
       title = ""It's Not A Habit, It's Cool, I'm a Prolific Game Designer"",
       subtitle = str_wrap(""A comparison from 1990 to 2016 of the the top rated and top published designer amongst those with 10 or more published games. Red dashed line represents the overall average designer rating."", 150),
       caption = ""Data: Board Game Geek | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"")

ggsave(here(""2019"",""week11"", ""tw11_plot.png""), width = 12, height = 6)
",2019,11
74,74,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week13/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(fs)
library(janitor)
library(jkmisc)

pet_licenses <- here(""2019"", ""week13"", ""data"") %>% 
  dir_ls(regexp = ""Seattle"") %>% 
  read_csv() %>% 
  clean_names() %>% 
  mutate_at(""license_issue_date"", mdy)

burst_name <- function(df) { 
  
  df %>% 
    distinct(license_issue_date) %>% 
    pull(license_issue_date) %>% 
    kleinberg()
    
  }

out <- pet_licenses %>% 
  filter(!is.na(animals_name)) %>% 
  mutate(animals_name = str_to_lower(animals_name)) %>% 
  group_by(animals_name) %>% 
  filter(n() >= 100) %>% 
  nest() %>% 
  mutate(bursts = map(data, burst_name)) %>% 
  unnest(bursts) %>% 
  arrange(desc(animals_name), level) %>% 
  mutate(id = ntile(animals_name, 1)) %>%
  mutate(color = case_when(level == 1 ~ ""grey50"",
                           level == 2 ~ ""#6baed6"",
                           level == 3 ~ ""#3182bd"",
                           level == 4 ~ ""#08519c""),
         alpha = case_when(level == 1 ~ 0.5,
                           level == 2 ~ 1,
                           level == 3 ~ 1,
                           level == 4 ~ 1)) %>% 
  ungroup()

facet_labels <- out %>% 
  group_by(id) %>% 
  summarize(label = sprintf(""%s to %s"", last(str_sub(animals_name, 1, 1)), first(str_sub(animals_name, 1, 1)))) %>% 
  pull(label) %>% 
  set_names(., sort(unique(out$id)))

order <- out %>% 
  filter(level == 1) %>% 
  arrange(desc(start)) %>% 
  pull(animals_name)

plot <- ggplot(out) +
  geom_segment(aes(x = start, xend = end, y = factor(animals_name, order), yend = factor(animals_name, order), color = color, alpha = alpha), size = 4, lineend = ""square"") +
  scale_color_identity() +
  scale_alpha_identity() +
  scale_x_date(limits = c(ymd(""2006/01/01""),ymd(""2019/01/01"")),  date_breaks = ""1 year"", date_labels = ""%Y"", expand = c(0.02, 0)) +
  scale_y_discrete(position = ""right"") +
  theme_jk(grid = ""XY"") +
  labs(x = NULL,
       y = NULL,
       title = ""What is it, Lassie? 'Bark! Bark-bark-bark! Bark-bark!' What, Timmy's fallen in the well?"",
       subtitle = str_wrap(""Illustrated below is the recorded use of and bursts in popularity of registered pet names (frequency of use > 100) in Seattle from 2006 to 2019.  The grey bar indicates the duration the name is in use, and the blue segments indicate bursts of increased use of the name.  Darker blue segments represent repeated bursts indicating an increased intensity of use."", 100),
       caption = ""Data: seattle.gov | Graphic: @jakekaupp"")

ggsave(here(""2019"",""week13"",""tw13_plot.png""), plot, width = 8, height = 10, type = ""cairo-png"")
",2019,13
75,75,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week14/R/analysis.R,"library(tidyverse)
library(lubridate)
library(jkmisc)
library(here)
library(patchwork)

bike_traffic <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-02/bike_traffic.csv"")

clean_bikes <- bike_traffic %>% 
  mutate(date = mdy_hms(date),
         month = month(date),
         year = year(date)) %>% 
  filter(between(year, 2014, 2018))

by_year <- clean_bikes %>% 
  group_by(year, month, crossing) %>% 
  summarize(total_bikes = sum(bike_count, na.rm = TRUE)) 

glyph <- ggplot(by_year, aes(x = crossing, y = total_bikes, group = year)) +
  geom_path() +
  geom_point(size = 1, aes(color = crossing)) +
  facet_grid(year ~ month, labeller = labeller(.cols = set_names(month.abb, 1:12)), switch = ""y"") +
  scale_color_manual(values = tol7qualitative) +
  theme_jk(grid = FALSE, plot_title_size = 14) +
  labs(x = NULL, 
       y = NULL) +
  theme(axis.text = element_blank(),
        strip.text.y = element_text(angle = 180),
        legend.position = ""none"")

main <- by_year %>% 
  filter(year == 2015, month == 1) %>% 
  ggplot(aes(x = crossing, y = total_bikes, group = year)) +
  geom_path() +
  geom_point(size = 3, aes(color = crossing)) +
  scale_color_manual(""Crossing"", values = tol7qualitative) +
  theme_jk(grid = FALSE, plot_title_size = 14) +
   labs(x = NULL, 
       y = NULL) +
  theme(axis.text = element_blank(),
        strip.text.y = element_text(angle = 180))

plot <- wrap_plots(list(main, glyph), widths = c(0.25, 0.75)) +
  plot_annotation(title = ""Annual Patterns in Seatle Bicycle Traffic"", 
                  subtitle = str_wrap(""This chart is glyph plot using multiple parallel coordinates plots to illustrate the monthly bike traffic at Seattle crossings.  A  colored dot represents each crossing and vertical position represents the total number of riders counted each month.  You can observe the year over year trends, as well as see which crossings experience cyclical patterns and which remain stable."", 155),
                  theme = theme_jk())

ggsave(here(""2019"", ""week14"", ""tw14_plot.png""), plot, width = 12, height = 6)
",2019,14
76,76,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week15/R/analysis.R,"library(tidyverse)
library(lubridate)
library(ggbeeswarm)
library(here)
library(jkmisc)
library(nord)

player_dob <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/player_dob.csv"")

grand_slams <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/grand_slams.csv"")

grand_slam_timeline <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-09/grand_slam_timeline.csv"")


plot_data <- player_dob %>% 
  select(name, date_of_birth) %>% 
  left_join(grand_slams, by = ""name"") %>% 
  mutate(age = interval(date_of_birth, tournament_date)/years(1)) %>% 
  group_by(name) %>% 
  filter(n()>1)
  

order <- plot_data %>% 
  group_by(name) %>% 
  filter(rolling_win_count == max(rolling_win_count)) %>% 
  arrange(rolling_win_count) %>% 
  pull(name)
  

plot <- ggplot(plot_data, aes(x = age, y = factor(name, order), size = rolling_win_count, color = gender, alpha = rolling_win_count)) +
  geom_point(aes(group = name)) + 
  facet_wrap(~gender, scales = ""free_y"") +
  scale_color_manual(values = c(""#C01E65"",""#117AB3"")) +
  scale_size_area(""Rolling Win Count"") +
  guides(size = guide_legend(override.aes = list(shape = 21, color = ""black"")), color = FALSE, alpha = FALSE) +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") +
  labs(x = NULL,
       y = NULL,
       title = ""Bright Stardom or Fading Obscurity: Looking at Players Major Wins Across their Careers."",
       subtitle = str_wrap(""The chart plots cumulative major wins against player age. Size and transparency of each point are mapped to the cumulative number of majors won.  Looking at the data, we can see the hot streaks in individual players, as well as the dominance of certain champions."", 110),
       caption = ""Data: wikipedia | Graphic: @jakekaupp"")
  
ggsave(here(""2019"",""week15"",""tw15_plot.png""), type = ""cairo"", width = 10, height = 12)
",2019,15
77,77,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week16/R/analysis.R,"library(tidyverse)
library(here) 
library(jkmisc)
library(ggalt)
library(grid)
library(Cairo)

dogs <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-16/dogs.csv"")

png(file = here(""2019"", ""week16"", ""tw16_plot.png""), width = 4, height = 4, units = ""in"", res = 300, type = ""cairo"")
                       
ggplot(dogs, aes(x = avg_weight, y = avg_neck)) +
  geom_xspline(size = 1) +
  geom_point(shape = 21, fill = ""black"", color = ""white"", stroke = 0.5, size = 2) +
  geom_text(data = filter(dogs, year == min(year)), aes(label = year), hjust = 0, nudge_x = 0.1, nudge_y = 0.01, family = ""Oswald"", size = 3) +
  geom_text(data = filter(dogs, year == max(year)), aes(label = year), hjust = 1, nudge_x = -0.1, family = ""Oswald"", size = 3) +
  annotate(""segment"", arrow = arrow(length = unit(0.2, ""cm""), type = ""closed""), x = 20.48, xend = 20.2, y = 44.3, yend = 44.03) +
  scale_y_continuous(limits = c(42, 45), breaks = 42:45) +
  expand_limits(x = c(17.5, 21)) +
  labs(title = ""Fit as a butcher's dog"",
       subtitle = ""Characteristics of dogs registered with the UK's\nKennel Club, average when fully grown"",
       x = bquote(""Weight*, kg""),
       y = NULL,
       caption = ""Sources: Kennel Club;\n The Economist "") +
  theme_jk(grid = ""XY"") +
  theme(plot.caption = element_text(hjust = -0.1))

grid.text(expression(paste(Neck~size, "", "", cm^""\u2020"")), x = 0.1, y = 0.78, gp = gpar(fontfamily = ""Oswald"", cex = 0.8))
grid.text(bquote(""* Where at leat 50 are registered per year""), x = 0.98, y = 0.075, gp = gpar(fontfamily = ""Scope One"", cex = 0.8), hjust = 1)
grid.text(expression(""\u2020""~Where~at~least~100~are~registered~per~year), x = 0.98, y = 0.040, gp = gpar(fontfamily = ""Scope One"", cex = 0.8), hjust = 1)

dev.off()
",2019,16
78,78,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week17/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)

tidy_anime <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/tidy_anime.csv"")

plot_data <- tidy_anime %>% 
  mutate(title = coalesce(title_english, name)) %>% 
  mutate(end_date = if_else(is.na(end_date), as.Date(""2019-04-01""), end_date)) %>% 
  mutate(interval = interval(start_date, end_date)) %>% 
  filter(type != ""Unknown"") %>% 
  distinct(animeID, .keep_all = TRUE) 


scaffold <- tibble(year = rep(1917:2019, each = 6),
       type = rep(c(""Movie"", ""Music"", ""ONA"", ""OVA"", ""Special"", ""TV""), length(1917:2019))) 

timeline <- scaffold %>% 
  mutate(count = map2_dbl(year, type, ~nrow(filter(plot_data, ymd(sprintf(""%s/01/01"", .x)) %within% interval , type == .y))))

order <- timeline %>% 
  filter(year == last(year)) %>% 
  arrange(desc(count)) %>% 
  pull(type)

# Area ----
area <- timeline %>% 
  mutate(type = factor(type, order, order)) %>% 
  ggplot(aes(x = year, y = count)) +
  geom_area(aes(fill = type)) +
  scale_fill_manual(""Anime Type"", values = tol6qualitative) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Fourty Years of Growth: The Rapid Rise of Anime"",
       subtitle = str_wrap(""The area chart below presents the number of anime titles released from 1919 to the present by release type.  Anime releases have increased over 400% since the 1980s, to meet the increasing demand driven by the invention of the VCR, the internet and the rise of streaming media services."", 95),
       caption = ""Data: MyAnimeList | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") +
  guides(fill = guide_legend(nrow = 1, byrow = TRUE))

# Popularity vs Score Hexbin ----
hex <- ggplot(plot_data, aes(x = popularity, y = score)) +
  geom_hex() +
  facet_wrap(~type, nrow = 1) +
  scale_fill_viridis_c(option = ""plasma"") +
  scale_x_continuous(trans = ""reverse"", breaks = scales::pretty_breaks(), labels = c("""","""",""Higher\nPopularity"", """", ""Lower\nPopularity"", """")) +
  scale_y_continuous(breaks = scales::pretty_breaks(), limits = c(0,10)) +
  guides(fill = guide_colorbar(title = ""Titles per Hex""), alpha =""none"") +
  labs(x = NULL, 
       y = NULL,
       title = ""The Relationship Between Ratings and Popularity on MyAnimeList"",
       subtitle = str_wrap(""The chart below plots the ratings score (out of 10) against popularity (rank) for all anime titles and anime types.  The data were hexangonally binned to illustrate areas of high occurance. It appears that a relationship may exist between ratings and popularity, warranting further analysis."", 100),
       caption = ""Data: MyAnimeList | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"") 

ggsave(here(""2019"",""week17"",""tw17_hex.png""), hex, width = 8, height = 6)

ggsave(here(""2019"",""week17"",""tw17_area.png""), area, width = 8, height = 6)

                         ",2019,17
79,79,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week18/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)

bird_collisions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/bird_collisions.csv"")


plot_data <- bird_collisions %>%
  filter(locality == ""CHI"") %>% 
  mutate(month = month(date),
         year = year(date)) %>% 
  unite(""binomial_name"", genus, species, sep = "" "") %>% 
  count(year, month, binomial_name) %>% 
  complete(nesting(year, binomial_name), month = 1:12, fill = list(n = 0)) %>% 
  group_by(year, binomial_name) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))


flower <- ggplot(plot_data, aes(x = month, y = percent, fill = binomial_name)) +
  geom_area(size = 0, position = position_dodge(), alpha = 0.2) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  guides(fill = guide_colorbar()) +
  coord_polar() +
  labs(x = NULL,
       y = NULL,
       title = ""Overall"") +
  theme_jk(dark = FALSE, grid = ""X"", strip_text_size = 10, plot_title_size = 14) +
  theme(axis.text = element_blank(),
        legend.position = ""none"")

petals <- flower +
  aes(group = year) +
  geom_path(aes(color = binomial_name), size = 0.2, show.legend = FALSE) +
  labs(title = ""By Species"") +
  facet_wrap(~binomial_name, labeller = label_wrap_gen(10), nrow = 7) 

legend <- plot_data %>% 
  filter(binomial_name == ""Setophaga fusca"") %>% 
  ggplot(aes(x = month, y = percent, fill = binomial_name, group = year)) +
  geom_area(size = 0, position = position_dodge(), alpha = 0.1) +
  geom_path(aes(color = binomial_name), size = 0.2, show.legend = FALSE) +
  annotate(""text"", x = 11, y = 0.8, label = ""One year of\ncollisions in October"", family = ""Scope One"", size = 3, hjust = 0) +
  annotate(""segment"", x = 10.8, y = 0.8, xend = 10, yend = 0.8, arrow = arrow(length = unit(0.2, ""cm""))) +
  annotate(""text"", x = 3.5, y = 0.8, label = ""Multiple years of\ncollisions in May"", family = ""Scope One"", size = 3) +
  annotate(""segment"", x = 3.8, y = 0.8, xend = 5, yend = 0.8, arrow = arrow(length = unit(0.2, ""cm""))) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  labs(x = NULL,
       y = NULL,
       title = ""How to Interpret This Chart"",
       subtitle = str_wrap(""A flower represents the recorded total collisions of each bird species with the individual petals representing the normalized events during each year (from 0-1).  The position of the petals indicates the month or months collisions occur, with overlaps indicating repeated year-over-year collisions."", 70)) +
  guides(fill = guide_colorbar()) +
  coord_polar(theta = ""x"", start = 0) +
  theme_jk(dark = FALSE, grid = ""XY"", plot_title_size = 14) +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

out <- wrap_plots(flower / legend, petals, ncol = 2, widths = c(1, 2)) +
  plot_annotation(title = ""Seasonality of Bird Collisions in Chicago"",
                  subtitle = str_wrap(""Presented below is a petal chart of of bird collisions, with instructions on how to interpret this chart in the lower left.  The upper left flower represents collisions recorded across all years and species, with individual species presented as small multiple flowers on the right."", 220),
                  caption = ""Data: Winger et al. (2019) Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Proceedings of the Royal Society B 286(1900): 20190364. https://doi.org/10.1098/rspb.2019.0364 | Graphic: @jakekaupp"",
      theme = theme_jk())

ggsave(here(""2019"",""week18"", ""tw18_plot.png""), out, width = 16, height = 10, type = ""cairo"")



",2019,18
80,80,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week18/R/experiment.R,"library(tidyverse)
library(lubridate)
library(here)
library(jkmisc)
library(patchwork)


bird_collisions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/bird_collisions.csv"")


plot_data <- bird_collisions %>%
  filter(locality == ""CHI"") %>% 
  mutate(month = month(date),
         year = year(date)) %>% 
  unite(""binomial_name"", genus, species, sep = "" "") %>% 
  count(year, month, binomial_name) %>% 
  complete(nesting(year, binomial_name), month = 1:12, fill = list(n = 0)) %>% 
  group_by(year, binomial_name) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))

petals <- plot_data %>% 
  filter(n != 0) %>% 
  split(list(.$year, .$month, .$binomial_name), drop = TRUE) %>% 
  map(~complete(.x, year, binomial_name, month = 1:12, fill = list(n = 0, percent = 0))) %>% 
  map(~geom_area(data = .x, aes(color = binomial_name), size = 0.2, alpha = 0.1))


base_plot <- ggplot(plot_data, aes(x = month, y = percent, fill = binomial_name)) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_d(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_d(option = ""plasma"", direction = 1) +
  labs(x = NULL,
       y = NULL) +
  coord_polar(theta = ""x"", start = 0) +
  theme_jk(dark = FALSE, grid = ""XY"", plot_title_size = 14) +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

out <- base_plot + petals + facet_wrap(~binomial_name, labeller = label_wrap_gen(10), nrow = 7)


ggsave(here(""2019"",""week18"", ""test.png""), plot = out)

",2019,18
81,81,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week19/R/analysis.R,"library(tidyverse)
library(readxl)
library(here)
library(fs)
library(jkmisc)
library(janitor)
library(countrycode)
library(patchwork)

ratio_data <- here(""2019"", ""week19"", ""data"") %>% 
  dir_ls(regexp = ""student_teacher_ratio"") %>% 
  read_excel(na = c("".."")) %>% 
  set_names(tolower(names(.))) %>% 
  gather(year, ratio, -1:-2,convert = TRUE)



plot_region <- function(data) {
  
  ggplot(data, aes(x = year, y = ratio, group = region, color = continent)) +
    geom_line(size = 0.3, show.legend = FALSE) +
    geom_ribbon(aes(ymin = 0, ymax = ratio, fill = continent), alpha = 0.5, color = NA, show.legend = FALSE) +
    facet_wrap(~region, nrow = 1) +
    scale_color_manual(values = colors) +
    scale_fill_manual(values = colors) +
    scale_x_continuous(breaks = seq(1970, 2010, 10)) +
    scale_y_continuous(limits = c(0, 40)) +
    labs(x = NULL,
         y = NULL,
         title = unique(data$continent)) +
    theme_jk(grid = ""XY"") 
  
  }

plot_continent <- function(data) {
  
  ggplot(data, aes(x = year, y = ratio, color = continent)) +
    geom_line(size = 0.3, show.legend = FALSE) +
    geom_ribbon(aes(ymin = 0, ymax = ratio, fill = continent), alpha = 0.5, color = NA, show.legend = FALSE) +
    facet_wrap(~continent, ncol = 1) +
    scale_color_manual(values = colors) +
    scale_fill_manual(values = colors) +
    scale_x_continuous(breaks = seq(1970, 2010, 10)) +
    scale_y_continuous(limits = c(0, 40)) +
    scale_shape_identity() +
    labs(x = NULL,
         y = NULL,
         title = unique(data$continent)) +
    theme_jk(grid = ""XY"") +
    theme(strip.text = element_blank())
  
}

plot_data <- ratio_data %>% 
  filter(type == ""Countries"") %>% 
  mutate(country_code = countrycode(country, ""country.name"", ""iso3c"")) %>% 
  mutate(region = countrycode(country_code, ""iso3c"", ""region"")) %>% 
  mutate(continent = countrycode(country_code, ""iso3c"", ""continent"")) %>% 
  filter(!is.na(region))

colors <- set_names(c(""#171635"", ""#00225D"", ""#763262"", ""#CA7508"", ""#E9A621""), c(unique(plot_data$continent)))

individual <- plot_data %>% 
  group_by(year, region, continent) %>% 
  summarize(ratio = mean(ratio, na.rm = TRUE)) %>% 
  filter(!is.nan(ratio)) %>% 
  split(.$continent) %>% 
  map(plot_region) 

summary <- plot_data %>% 
  group_by(year, continent) %>% 
  summarize(ratio = mean(ratio, na.rm = TRUE)) %>% 
  filter(!is.nan(ratio)) %>% 
  split(.$continent) %>% 
  map(plot_continent) 
  
plots <- map2(summary, individual, ~wrap_plots(.x, .y, nrow = 1, widths = c(1, 1)))
  
out <- wrap_plots(plots, ncol = 1) +
  plot_annotation(title = ""Working to Two Sigma: Student Teacher Ratios Improving Since the 1970s"",
                  subtitle = str_wrap(""Illustrated below is the average student to teacher ratio across each continent (left column) and region (right column).  Continent and region assigned from iso3c coding of country name and are consistent with the World Bank Dvelopment Indicators."", 210),
                  caption = ""Data: UNESCO Institute of Statistics | Graphic: @jakekaupp"",
                  theme = theme_jk())

ggsave(here(""2019"",""week19"",""tw19_plot.png""), out, width = 16, height = 10)
",2019,19
82,82,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week2/R/analysis.R,"library(tidyverse)
library(ggraph)
library(tidygraph)
library(jkmisc)
library(lubridate)
library(here)
library(patchwork)
library(nord)

set.seed(42)

source(here(""2019"", ""week2"", ""R"", ""functions.R""))

# Read data from github repo
tv_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-08/IMDb_Economist_tv_ratings.csv"") %>% 
  rename(title_id = titleId,
         season_number = seasonNumber) %>% 
  mutate(year = year(date))

# Make this into a nodes tibble
list <- tv_data %>% 
  split(.$year) %>% 
  map(share_packed_circle)

out <- wrap_plots(list, ncol = 10, nrow = 3) +
  plot_annotation(title = ""The Evolution and Differentiation of Dramas Across the Golden Age of Television"",
                  subtitle = str_wrap(""This chart presents a time series of circle-packed network representations of the television dramas.  
                                      The larger dark blue circle represents the year, light blue represents the genre (Action, Comedy, etc.) and the pale pink represents the individual program. 
                                      The area of each circle (node) is porportional to the sum of the audience share of the smaller circles within (child nodes)."", 180),
                  caption = ""data: IMDb | graphic: @jakekaupp"",
                  theme = theme_jk(plot_title_size  = 22, subtitle_size = 14) %+replace% theme(plot.background = element_rect(fill =""#2E3440""),
                                                      text = element_text(color = ""white"")))
 
ggsave(here(""2019"",""week2"", ""tt_week2.png""), out, width = 16, height = 8)
",2019,2
83,83,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week2/R/functions.R,"share_packed_circle <- function(df) {
 
  nodes <- make_nodes(df)
  
  edges <- make_edges(df)
 
  mygraph <- tbl_graph(nodes = nodes, edges = edges)
  
  # Make the plot
  plot <- ggraph(mygraph, layout = 'circlepack', weight = ""size"") + 
    geom_node_circle(aes(fill = depth)) +
    theme_void() +
    labs(title = unique(df$year)) +
    coord_equal() +
    scale_fill_nord(""lumina"", discrete = FALSE, reverse = TRUE) +
    #scale_fill_viridis(option = ""plasma"") +
    theme(legend.position = ""none"", 
          plot.background = element_rect(fill = ""#4C566A""),
          plot.title = element_text(family = ""Oswald"", hjust = 0.5, color = ""white""),
          )
  
  return(plot)
}


make_nodes <- function(df) {
  
  size <- df %>% 
    group_by(title, genres, year) %>% 
    summarize(share = mean(share)) %>% 
    distinct(genres, title, share) %>% 
    rename(name = title, size = share)
  
  genre_size <- size %>% 
    group_by(genres) %>% 
    summarize(size = sum(size)) %>% 
    rename(name = genres)
  
  title_size <- size %>% 
    ungroup() %>% 
    distinct(name, size) %>% 
    mutate(size = size)
  
  total_size <- df %>% 
    distinct(title, share) %>% 
    summarize(name = as.character(unique(df$year)),
              size = sum(share))
  
  sizes <- bind_rows(genre_size, title_size, total_size)
  
  nodes <- df %>% 
    group_by(title, genres, year) %>% 
    summarize(share = mean(share)) %>% 
    gather(variable, name, title, genres, year) %>% 
    arrange(variable, name) %>% 
    distinct(name) %>% 
    left_join(sizes, by = ""name"") %>% 
    mutate(size = if_else(size == 0, 0.001, size)) %>% 
    arrange(size)
  
  return(nodes)
  
  
}

make_edges <- function(df) {
  
  base <- tibble(from = as.character(unique(df$year)), to = unique(df$genres))
  
  inner <- df %>% 
    select(from = genres, to = title) %>% 
    distinct() 
  
  edges <- bind_rows(base, inner) 
  
  return(edges)
  
}
",2019,2
84,84,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week20/R/analysis.R,"library(tidyverse)
library(here)
library(fs)
library(rcrossref)
library(ggbeeswarm)
library(jkmisc)


# Not re-downloading things, the citation count pulls take 2hrs.
if (length(dir_ls(here(""2019"", ""week20"", ""data""))) <= 0) {
 
  nobel_winners <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winners.csv"")
  nobel_winners_all_pubs <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winner_all_pubs.csv"")
  
  saveRDS(nobel_winners, here(""2019"", ""week20"", ""data"", ""nobel_winners.RDS""))
  
  saveRDS(nobel_winners_all_pubs, here(""2019"", ""week20"", ""data"", ""nobel_winners_all_pubs.RDS""))

  
} else {
  
  nobel_winners <- dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""nobel_winners.RDS"") %>% 
    readRDS()
  
  nobel_winners_all_pubs <- dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""nobel_winners_all_pubs.RDS"") %>% 
    readRDS()
 
}


if (length(dir_ls(here(""2019"", ""week20"", ""data""), regexp = ""cite_count"")) <= 0) {

  dois <- nobel_winners_all_pubs$doi

  list  <- split(dois, rep(1:ceiling(length(dois)/50), each=50)[1:length(dois)])

wait_cr_citation_count <- function(doi, index, list_len) {
  
  print(sprintf(""%s complete"", scales::percent(index/list_len)))
  
  Sys.sleep(1)
  
  cr_citation_count(doi)
  
}

cite_count <- imap_dfr(list, ~wait_cr_citation_count(.x, .y, length = length(list)))

saveRDS(cite_count, here(""2019"", ""week20"", ""data"", ""cite_count.RDS"")) 

} else {
  
cite_count <- readRDS(here(""2019"", ""week20"", ""data"", ""cite_count.RDS""))
  
}

highlights <- c(""einstein, a"", ""hill, av"", ""heeger, a"")

plot_data <- nobel_winners_all_pubs %>%
  left_join(cite_count) %>%
  distinct(laureate_id, paper_id, .keep_all = TRUE) %>%
  select(pub_year, laureate_name, is_prize_winning_paper, count, category) %>% 
  replace_na(list(count = 0)) %>% 
  group_by(laureate_name, pub_year, category) %>%
  summarize(count = sum(count)) %>% 
  group_by(laureate_name) %>% 
  mutate(rolling_sum = cumsum(count)) %>% 
  mutate(color = if_else(laureate_name %in% highlights, ""#F24534"", ""#21344F""),
         alpha = if_else(laureate_name %in% highlights, 1, 0.2))


everyone <- filter(plot_data, laureate_name %notin% highlights)

focus <- filter(plot_data, laureate_name %in% highlights) %>% 
  ungroup() %>% 
  mutate(laureate_name = case_when(laureate_name == ""einstein, a"" ~ ""Einstein, A"",
                                   laureate_name == ""hill, av"" ~ ""Hill, AV"",
                                   laureate_name == ""heeger, a"" ~ ""Heeger, A"")) %>% 
  group_by(laureate_name)


plot <- ggplot(plot_data, aes(x = pub_year, y = rolling_sum, group = laureate_name)) +
  geom_step(aes(color = color, alpha = alpha)) +
  geom_step(data = focus, aes(color = color, alpha = alpha)) +
  geom_text(data = filter(focus, pub_year == last(pub_year)), aes(color = color, alpha = alpha, label = laureate_name), x = 2018, family = ""Oswald"", hjust = 0) +
  scale_x_continuous(limits = c(1900, 2100), breaks = c(1900, 1925, 1950, 1975, 2000, 2018)) +
  scale_y_continuous(breaks = scales::pretty_breaks(), labels = scales::number) +
  scale_color_identity() +
  scale_alpha_identity() +
  facet_wrap(~category, labeller = as_labeller(str_to_title)) +
  labs(x = NULL,
       y = NULL,
       title = ""Growth Patterns in How Often Nobel Prize Winning Researchers Are Cited"",
       subtitle = str_wrap(""Cummulative citation count by year (1900-2018).  Highlighted are A. Heeger (conductive polymers), A.V. Hill (heat and work in muscle) and A. Einstein (photoelecric effect). Each exhibit different citation patterns, likely attributed to the continued relevance and impact of their work."", 150),
       caption = ""Data: Li, Jichao; Yin, Yian; Fortunato, Santo; Wang Dashun, 2018, 'A dataset of publication records for Nobel laureates', https://doi.org/10.7910/DVN/6NJ5RN, Harvard Dataverse. | Graphic: @jakekaupp"") +
  theme_jk(grid = ""Y"") +
  theme(legend.position = ""bottom"") 

ggsave(here(""2019"", ""week20"", ""tw20_plot.png""), plot, width = 12, height = 6)

",2019,20
85,85,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week21/R/analysis.R,"library(tidyverse)
library(here)
library(readxl)
library(fs)
library(janitor)
library(jkmisc)
library(patchwork)


# Plotting Function to make separate ordered stacked bars by group----
make_bars <- function(df, pals) {
  
  order <- df %>% 
    arrange(desc(other)) %>% 
    pull(country)
  
  labs <- c(""HIC"" = ""High Income Group"",
            ""UMI"" = ""Upper Middle Income Group"",
            ""LMI"" = ""Lower Middle Income Group"",
            ""LI"" = ""Low Income Group"")
  
  
  df %>% 
    gather(type, value, c(""other"", ""plastic_waste"", ""inadequate_waste"", ""littered_waste"")) %>% 
    mutate(type = factor(type, c(""other"", ""inadequate_waste"", ""plastic_waste"", ""littered_waste""), c(""Other"", ""Inadequately Managed Waste"", ""Plastic Waste"", ""Littered Waste""))) %>% 
    mutate(country = factor(country, order)) %>% 
    mutate(alpha = if_else(type == 'Other', 0.5, 0.8)) %>% 
    ggplot() +
    geom_col(aes(x = country, y = value, fill = type, alpha = alpha), width = 0.90, size = 0.1) +
    coord_flip() +
    scale_fill_manual("""", values = pals) +
    scale_alpha_identity() +
    scale_y_continuous(expand = c(0,0.05), labels = scales::percent) +
    labs(x = NULL, y = NULL) +
    facet_wrap(~economic_status, scales = ""free_y"", labeller = as_labeller(labs)) +
    theme_jk(grid = FALSE) +
    theme(legend.direction = ""horizontal"")
  
}


# Function to extract ggplot legends ----
extract_legend <- function(ggp){
  
  tmp <- ggplot_gtable(ggplot_build(ggp))
  
  leg <- which(map_lgl(tmp$grobs, function(x) x$name == ""guide-box""))
  
  legend <- tmp$grobs[[leg]]
  
  return(legend)}


# Read in Coastal Waste Data----
# Plastic waste inputs from land into the ocean
# BY JENNA R. JAMBECK, ROLAND GEYER, CHRIS WILCOX, THEODORE R. SIEGLER, MIRIAM PERRYMAN, ANTHONY ANDRADY, RAMANI NARAYAN, KARA LAVENDER LAW
# 
# SCIENCE13 FEB 2015 : 768-771

coastal_waste <- here(""2019"", ""week21"", ""data"") %>% 
  dir_ls(regexp = ""xlsx"") %>% 
  read_excel() %>% 
  clean_names() %>% 
  set_names(str_remove(names(.), ""_*[0-9]$"")) %>% 
  mutate(country = str_remove(country, ""[0-9]"")) %>% 
  mutate(country = case_when(str_detect(country, ""Palestine"") ~ ""Palestine"",
                             str_detect(country, ""Korea, South"") ~ ""South Korea"",
                             str_detect(country, ""Korea, North"") ~ ""North Korea"",
                             str_detect(country, ""Congo"") ~ ""Congo"",
                             TRUE ~ country)) %>% 
  filter(!grepl(""Burma"", country)) %>% 
  filter(complete.cases(.)) %>% 
  mutate(other = 100 - (percent_plastic_in_waste_stream + percent_inadequately_managed_waste + percent_littered_waste)) %>% 
  rename(plastic_waste = percent_plastic_in_waste_stream, inadequate_waste  = percent_inadequately_managed_waste, littered_waste = percent_littered_waste) %>% 
  mutate_at(c(""other"", ""plastic_waste"", ""inadequate_waste"", ""littered_waste""), function(x) x/100) %>% 
  mutate(other = if_else(other < 0, 0, other)) %>% 
  mutate(total_waste = waste_generation_kg_day * 365/1000,
         total_plastic_waste = total_waste * plastic_waste,
         total_inadequate_waste = total_waste * inadequate_waste,
         total_littered =  total_waste * littered_waste,
         other_waste = total_waste * other)


# Palette for plot----
pal <- c(""#F5F0F6"", ""#629460"", ""#385F71"", ""#2B4162"")

avg <- coastal_waste %>% 
  summarize(total_waste = mean(total_waste, na.rm = TRUE),
            other_waste  = mean(other_waste, na.rm = TRUE),
            total_plastic_waste  = mean(total_plastic_waste, na.rm = TRUE),
            total_inadequate_waste = mean(total_inadequate_waste, na.rm = TRUE),
            total_littered = mean(total_littered, na.rm = TRUE)) %>% 
  mutate(country = ""Global Average"")


order <- coastal_waste %>% 
  top_n(50, total_inadequate_waste) %>% 
  bind_rows(avg) %>% 
  top_n(50, total_inadequate_waste) %>% 
  arrange(desc(total_inadequate_waste)) %>% 
  pull(country) 



overall_mismanaged <- coastal_waste %>% 
  top_n(50, total_inadequate_waste) %>% 
  bind_rows(avg) %>% 
  top_n(50, total_inadequate_waste) %>% 
  arrange(desc(total_inadequate_waste)) %>% 
  gather(type, value, c(""other_waste"", ""total_plastic_waste"", ""total_inadequate_waste"", ""total_littered"")) %>% 
  mutate(type = factor(type,  c(""other_waste"", ""total_plastic_waste"", ""total_inadequate_waste"", ""total_littered""), c(""Other"", ""Inadequately Managed Waste"", ""Plastic Waste"", ""Littered Waste""))) %>% 
  mutate(country = factor(country, rev(order))) %>% 
  mutate(alpha = if_else(type == 'Other', 0.5, 0.8)) %>% 
  mutate(strip = ""Top 50 Producers & Global Average of Total Indequately Managed Waste (kg)"") %>% 
  ggplot() +
  geom_col(aes(x = country, y = value, fill = type, alpha = alpha), width = 0.90, size = 0.1) +
  coord_flip() +
  scale_fill_manual("""", values = pal) +
  scale_alpha_identity() +
  scale_y_continuous(expand = c(0,0.05), labels = scales::comma) +
  labs(x = NULL, y = NULL) +
  facet_wrap(~strip) +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""none"")


# Make plots----
list <- coastal_waste %>% 
  split(.$economic_status) %>% 
  map(make_bars, pal)

# Extract legend----
legend <- extract_legend(list[[1]])

# Remove legend from list of plots----
list <- map(list, ~.x + theme(legend.position = ""none""))
  
# Finish plot----
out <- (overall_mismanaged + wrap_plots(list[c(""HIC"", ""UMI"", ""LMI"", ""LI"")], nrow = 1) + plot_layout(widths = c(0.3, 0.7))) / legend + plot_layout(heights = c(0.95, 0.05)) +
  plot_annotation(title = ""The Relationship Between World Bank Income Classification and Mismanaged Waste"",
                  subtitle = str_wrap(""Illustrated below is the percentage of waste by category for each country by World Bank income classification.  The lower the classification, the higher the mismanaged waste.  Much of this mismanaged waste (especially plastics) ends up in waterways that ultimately lead to our oceans, suggesting that global income inequality plays a role in ocean pollution by hampering the implementation of effective waste management strategies."", 240),
                  caption = ""Data: Jambeck, Jenna R., et al. 'Plastic waste inputs from land into the ocean.' Science 347.6223 (2015): 768-771. | Graphic: @jakekaupp"",
                  theme = theme_jk())

ggsave(here(""2019"", ""week21"", ""tw21_plot.png""), out, width = 19, height = 12)



",2019,21
86,86,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week22/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(lubridate)
library(here)
library(ggridges)
library(tidytext)
library(countrycode)
library(ggwordcloud)
library(patchwork)

source(here(""2019"", ""week22"", ""R"", ""packed_bars.R""))

wine_ratings <- here(""2019"", ""week22"", ""data"", ""winemag-data-130k-v2.csv"") %>% 
  read_csv()

wine_counts <- wine_ratings %>% 
  count(country) %>% 
  mutate(max_rel_val = n/sum(n)) %>% 
  filter(!is.na(country))
 
summary_ratings <- wine_ratings %>% 
  group_by(country) %>% 
  summarize_at(c(""points"",""price""), mean, na.rm = TRUE) %>% 
  filter(!is.na(country))

summary_data <- left_join(wine_counts, summary_ratings)

plot_data <- pack_bars(summary_data, number_rows = 4, max_rel_val)

packed_bar <- ggplot(plot_data) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = ""white"") +
  geom_text(data = filter(plot_data, fill == ""#4B384C""), aes(x = xmin, y = (ymin + ymax)/2, label = country), family = ""Oswald"", color = ""white"", nudge_x = 0.01, hjust = 0) +
  scale_fill_identity() +
  scale_color_identity() +
  scale_x_continuous(labels = scales::percent) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

word_counts <- wine_ratings %>%
  select(country, description) %>%
  group_by(country) %>% 
  filter(n() > 2) %>% 
  filter(!is.na(country)) %>% 
  unnest_tokens(word, description) %>% 
  anti_join(stop_words) %>% 
  count(country, word) %>% 
  mutate(word = trimws(word)) %>% 
  filter(!str_detect(word, ""[0-9]""), !str_detect(word, ""aroma|wine|note|nose|notes|aromas|drink|drinks|feel|feels|finish"")) %>% 
  group_by(country) %>% 
  top_n(300, n) 

clouds <- word_counts %>% 
  ungroup() %>% 
  mutate(iso2 = tolower(countrycode(country, ""country.name"", ""iso2c"")),
         iso2 = if_else(country == ""England"", ""gb"", iso2)) %>% 
  filter(country %in%  c(""US"", ""France"", ""Italy"", ""Spain"")) %>% 
  mutate(country = factor(country, levels = c(""US"", ""France"", ""Italy"", ""Spain"")),
         iso2 = factor(iso2, levels = c(""us"",""fr"", ""it"", ""es""))) %>% 
  group_by(iso2) %>% 
  nest() %>% 
  arrange(iso2) %>% 
  mutate(clouds =  map2(iso2, data, create_wc))

word_clouds <- wrap_plots(clouds$clouds, ncol = 1) 

out <- packed_bar + word_clouds +
  plot_annotation(title = ""Wine-ing: The Top 4 Countries and What Reviewers Say About Their Wines"",
                  subtitle = str_wrap(""On the left, a packed bar chart showing the % of reviewed wines by country.  On the right, wordclouds of the top 300 most frequent terms used in reviews."", 100),
                  caption = ""Data: Kaggle via WineEnthusiast | Graphic: @jakekaupp"",
                  theme = theme_jk()
                  )

ggsave(here('2019', ""week22"", ""tw22_plot.png""), out, width = 8, height = 12)

ggsave(here('2019', ""week22"", ""packed_bar.png""), packed_bar + labs(title = ""Top 4 Countries Reviewed as Packed Bar Chart"",
                                                                   subtitle = str_wrap(""The visualizion below is a packed bar chart, developed by Xan Gregg.  It combines the ordered nature of a bar chart with the total view and condensed nature of a treemap.  Colour denotes the focus, while the each gray sections represents each other reviwed country. This gives a sense of how many secondary categories there are, their magnitude and distribution. Additionally, since they are on the same scale of the focused bars we can even estimate some of the values from the length they span on the axis."", 100)), width = 8, height = 6)
",2019,22
87,87,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week22/R/packed_bars.R,"
pack_bars <- function(data, number_rows, value_column, fill_color = ""#4B384C"", border_color = ""white"") {

value_column <- ensym(value_column)  
  
color_bar_data <- summary_data %>% 
  top_n(number_rows, !!value_column) %>% 
  arrange(desc(!!value_column))

# calc row height based on num rows
bar_h = 1/number_rows

color_bars <- color_bar_data %>% 
  mutate(fill = fill_color,
         color = border_color,
         xmin = 0,
         xmax = !!value_column,
         ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1)),
         ymax = map_dbl(1:number_rows, ~1 - bar_h*.x))

gray_bar_data <- summary_data %>% 
  anti_join(color_bar_data) %>% 
  arrange(desc(!!value_column))

#get max x level for each bar level
row_sums <- pull(color_bar_data, !!value_column)

#gen gray ramp function
gray_gen <- colorRampPalette(c(""#E8E8E8"", ""#cccccc""))

#gen gray ramp
grays <- gray_gen(105)
low_grays <- grays[1:50]
hi_grays  <- grays[56:105]

last_gray <- sample(c(low_grays, hi_grays), number_rows, replace = TRUE)

gray_bar_list <- vector('list', nrow(gray_bar_data))


for (i in 1:nrow(gray_bar_data)) {
  
  row <- gray_bar_data[i,]
  
  # Determine placing of each block by looking at the minium starting values of colored bars
  # adding on the new block and setting value to represent the new block length
  vert_pos <- which.min(row_sums + pull(row, !!value_column))
  
  # Assign alternating random grays to fill
  if (i == 1) {
    
    gray_fill <- sample(low_grays, 1)
    
  } else {
    
    last_gray <- last_gray[vert_pos]
    
    gray_fill <- ifelse(last_gray %in% low_grays, sample(hi_grays, 1), sample(low_grays, 1))
  }
  
  last_gray[vert_pos] <- gray_fill
  
  # Generate aes for geom_rect
  gray_bar_list[[i]]  <- mutate(row, 
                                fill = gray_fill,
                                color = border_color,
                                xmin = row_sums[[vert_pos]],
                                xmax = row_sums[[vert_pos]] + !!value_column,
                                ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1))[[vert_pos]],
                                ymax = map_dbl(1:number_rows, ~1 - bar_h*.x)[[vert_pos]]
  )
  
  # Assign the new color_bar + rectangle as the max value for that row 
  row_sums[[vert_pos]] <- gray_bar_list[[i]]$xmax
  
}

gray_bars <- bind_rows(gray_bar_list)

bind_rows(color_bars, gray_bars)

}


create_wc <- function(iso2, data) {
  
  cntry_mask <- png::readPNG(here(""2019"", ""week22"", ""data"", ""png maps"", iso2, ""1024.png""))
  
  ggplot(data, aes(label = word, size = n)) +
    geom_text_wordcloud(family = ""Oswald"", mask = cntry_mask, rm_outside = TRUE) +
    scale_radius(range = c(0, 40)) +
    theme_jk() 
  
  
}
",2019,22
88,88,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week23/R/analysis.R,"library(tidyverse)
library(glue)
library(rvest)
library(xml2)
library(lubridate)
library(here)
library(jkmisc)



#Scraping functions----

get_urls <- function(sitemap_url) {
  
  read_xml(sitemap_url) %>%
    xml_children() %>% 
    xml_children() %>% 
    xml_text() %>% 
    keep(~str_detect(.x, ""https://www.theramenrater.com/[0-9]{4}/[0-9]{2}/[0-9]{1,}/\\w+""))
  
}

get_post_title <- function(url, idx, rows) {
  
  print(sprintf(""Progress: %s/%s"", idx, rows))
  
  read_html(url) %>% 
    html_node("".entry-title"") %>% 
    html_text()
  
  
}

slowly_get_post_title <- slowly(~ get_post_title(.x, .y, rows), rate = rate_delay(pause = 0.5), quiet = TRUE)

#Week of month
wom <- function(date) { # week-of-month
  first <- wday(as.Date(paste(year(date), month(date), 1, sep=""-"")))
  return((mday(date) + (first - 2)) %/% 7 + 1)
}

#Plotting functions----
month_outline <- function(df) {
  
  top1 <- with(df, tibble(x = min(wmonth) - 0.5,
                          xend = wday[day == min(day)] - 0.5,
                          y = wmonth[day == min(day)] + 0.5,
                          yend = wmonth[day == min(day)] + 0.5,
                          line = ""top1"")) 
  
  top2 <- with(df, tibble(x = wday[day == min(day)] - 0.5,
                          xend = max(wday) + 0.5,
                          y = min(wmonth) - 0.5,
                          yend = min(wmonth) - 0.5,
                          line = ""top2"")) 
  
  left1 <- with(df, tibble(x = wday[day == min(day)] - 0.5,
                           xend = wday[day == min(day)] - 0.5,
                           y = wmonth[day == min(day)] + 0.5,
                           yend = min(wmonth) - 0.5,
                           line = ""left1""))
  
  left2 <- with(df, tibble(x = min(wmonth) - 0.5,
                           xend = min(wmonth) - 0.5,
                           y = wmonth[day == min(day)] + 0.5,
                           yend = wmonth[day == max(day)] + 0.5,
                           line = ""left2""))
  

  right1 <- with(df, tibble(x = max(wday) + 0.5,
                            xend = max(wday) + 0.5,
                            y = min(wmonth) - 0.5,
                            yend = wmonth[day == max(day)] - 0.5,
                            line = ""right1""))
  
  right2 <- with(df, tibble(x = wday[day == max(day)] + 0.5,
                            xend = wday[day == max(day)] + 0.5,
                            y = wmonth[day == max(day)] - 0.5,
                            yend = wmonth[day == max(day)] + 0.5,
                            line = ""right2""))

  
  bottom1 <- with(df, tibble(x = min(wmonth) - 0.5,
                             xend = wday[day == max(day)] + 0.5,
                             y = wmonth[day == max(day)] + 0.5,
                             yend = wmonth[day == max(day)] + 0.5,
                             line = ""bottom1""))
  
  bottom2 <- with(df, tibble(x = wday[day == max(day)] + 0.5,
                             xend = max(wday) + 0.5,
                             y = wmonth[day == max(day)] - 0.5,
                             yend = wmonth[day == max(day)] -0.5,
                             line = ""bottom2""))
  
  top <- bind_rows(top1, top2)
  left <- bind_rows(left1, left2)
  bottom <- bind_rows(bottom1, bottom2) 
  right <- bind_rows(right1, right2) 
    
    bind_rows(top, left, right, bottom) %>% 
      mutate(year = unique(df$year),
             month = unique(df$month)) 
    
  
}


if(!file.exists(here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))) {
  
  # Add Recent ratings #3181-319
  recent_ratings <- tibble(review_number = c(3181:3189, 1676, 2745, 2991),
                           stars = c(3.75, 3.25, 4.0, 3.25, 2.0, 3.75, 3.75, 3.5, 2.25, 4.25, 5, 3.25),
                           brand = c(""Nissin Yakisoba"", ""Maruchan"", ""Uni-President"", ""Maruchan"", ""Sakruai Foods"", ""Nissin Mago"", ""Big Bon"", ""Sapporo Ichiban"", ""Canton"", ""A1"", ""Nissin"", ""Big Bon""),
                           variety = c(""Instant Panict Savory Beef Flavour"", ""Maruchan Ramen Noodle Soup Roast Beef Flavour"", ""Imperial Big Meal Super Hot Pot Beef Flavour"",
                                       ""Ramen Noodle Soup Pork Beef Flavour"", ""Vegetarian Stir Fry Noodles"", ""Nissin Lamen Light Legumes "", ""Spice Mix Piquant"", ""Momosan Ramen Tokyo Chicken"", ""Instant Noodles Spicy Tomato"",
                                       ""Emperor Herbs Chicken Noodle"", ""U.F.O. Big Wasabi-Mayo Yakisoba"", ""Chicken & Salsa Sauce Instant Noodles""),
                           country = c(""Phillipines"", ""United States"", ""Taiwan"", ""United States"", ""Japan"", ""Brazil"", ""Russia"" , ""United States"", ""India"", ""Malaysia"", ""Japan"", ""Russia""),
                           style = c(""Cup"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack"", ""Pack""))
  
  # Read tidytesday data----
  ramen_ratings <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv"") %>% 
    bind_rows(recent_ratings)
  
  out <- tibble(sitemap_url = glue(""https://www.theramenrater.com/post-sitemap{1:5}.xml""),
                contents = map(sitemap_url, get_urls)) %>% 
    unnest() 
  
  rows <- nrow(out)
  
  # Scrapin der web purges----
  out <-  out %>% 
    mutate(title = imap(contents, ~slowly_get_post_title(.x, .y, rows))) %>% 
    mutate(date = parse_date(str_extract(contents, ""[0-9]{4}/[0-9]{2}/[0-9]{1,}""), ""%Y/%m/%d""),
           review_number = as.numeric(str_extract(title, ""(?!#)[0-9]{1,4}(?=\\:)""))) %>% 
    filter(!is.na(review_number)) %>% 
    left_join(ramen_ratings) %>% 
    filter(review_number < 4000) %>% 
    select(-sitemap_url)
  
  # Saving the new dataset----
  saveRDS(out, here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))
  
} else {
  
  ramen_data <- readRDS(here(""2019"",""week23"", ""data"", ""full_ramen_data.RDS""))
  
}

#Make months
all_dates <- tibble(date = seq.Date(from = ymd(""2009/01/01""), to =ymd(""2019/12/31""), by =""day"")) %>% 
  mutate(day = day(date),
         month = month(date),
         year = year(date))

plot_data <- ramen_data %>%
  mutate(day = day(date),
         month = month(date),
         year = year(date)) %>% 
  group_by(year, month, day) %>% 
  summarize(brands = toString(sprintf(""%s: %s"", brand, variety)),
            count = n(),
            avg_stars = mean(stars)) %>% 
  right_join(all_dates) %>% 
  ungroup() %>% 
  mutate(wday = wday(date, label = TRUE, week_start = 7),
         month = month(date,label = TRUE),
         wmonth = wom(date),
         week = week(date))
  
outlines <- all_dates %>% 
  mutate(wday_label = wday(date, label = TRUE),
         wday = wday(date),
         month = month(date,label = TRUE),
         wmonth = wom(date),
         week = week(date)) %>% 
  split(list(.$year, .$month), drop = TRUE) %>% 
  map_df(month_outline)



plot <- ggplot(data = plot_data, aes(x = wday, y = wmonth, fill = avg_stars)) +
  geom_tile(color = ""grey80"", size = 0.1) +
  geom_segment(data = outlines, aes(x = x, xend = xend, y = y, yend = yend, group = line), color = ""grey30"", inherit.aes = FALSE) +
  scale_y_continuous(trans = ""reverse"", labels = NULL) +
  scale_x_discrete(labels = NULL) +
  scale_fill_gradientn(""Average Stars"", colors = rev(parula(100)), na.value = ""grey95"") +
  facet_grid(month ~ year, switch = ""y"") +
  labs(x = NULL,
       y = NULL,
       title = ""The Prolfic Nature of the Ramen Rater and a Birds-Eye View of Ramen Quality"",
       subtitle = str_wrap(""Below is a heatmap calendar of the all the Ramen Raters ramen ratings by the published date of the review.  In the early days, multiple reviews were posted in a single day, until reaching the usual pattern of a single review per day.  However, there are still some reviews that get posted en masse."", 100),
       caption = ""Data: The Ramen Rater | Graphic: @jakekaupp"") +
  theme_jk(grid = FALSE) +
  theme(strip.text.y = element_text(angle = 180),
        panel.spacing.y = unit(-0.2, ""lines""),
        legend.position = ""bottom"")

ggsave(here(""2019"", ""week23"", ""tw23_plot.png""), height = 11, width = 8.5, type = ""cairo"")





  
",2019,23
89,89,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week24/R/analysis.R,"library(nord)
library(tidyverse)
library(ggmap)
library(here)
library(countrycode)
library(jkmisc)
library(patchwork)

source(here(""2019"", ""week24"", ""R"", ""packed_bars.R""))

if (!file.exists(here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))) {
  
  slow_revgeocode <- slowly(~revgeocode(.x, output = ""address""), rate = rate_delay(0.03), quiet = TRUE)
  
  reverse_geocoded <- meteorites %>% 
    distinct(long, lat) %>% 
    mutate(location = map2_chr(long, lat, ~slow_revgeocode(c(.x, .y))))
  
  saveRDS(reverse_geocoded, here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))
  
  
} else {
  
  meteorite_locations <- readRDS(here(""2019"", ""week24"", ""data"", ""rev_geocoded_meteorites.RDS""))
  
  
}

meteorites <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv"")

meteorites <- left_join(meteorites, meteorite_locations) %>% 
  mutate(country_code = countrycode(location, ""country.name"", ""iso3c"")) %>% 
  mutate(country_code = case_when(str_detect(location, ""UK"") ~ ""GBR"",
                                  str_detect(location, ""USA"") ~ ""USA"",
                                  str_detect(location, ""China"") ~ ""CHN"",
                                  str_detect(location, ""Philippines"") ~ ""PHL"",
                                  str_detect(location, ""Serbia"") ~ ""RUS"",
                                  str_detect(location, ""Australia"") ~ ""AUS"",
                                  str_detect(location, ""Chile"") ~ ""CHL"",
                                  str_detect(location, ""Shopian"") ~ ""IND"",
                                  str_detect(location, ""Argentina"") ~ ""ARG"",
                                  str_detect(location, ""Bass Strait"") ~ ""AUS"",
                                  TRUE ~ country_code)) %>% 
  mutate(country_code = case_when(str_detect(name, ""Indarch"") ~ ""AZE"",
                                  str_detect(name, ""Oum Dreyga"") ~ ""ESH"",
                                  str_detect(name, ""Zag"") ~ ""ESH"",
                                  str_detect(name, ""Al Haggounia"") ~ ""ESH"",
                                  str_detect(name, ""Bou Kra"") ~ ""ESH"",
                                  TRUE ~ country_code)) %>% 
  rename(iso3c = country_code) %>% 
  filter(!is.na(iso3c)) 


world_tile_grid <- read_csv(""https://gist.githubusercontent.com/maartenzam/787498bbc07ae06b637447dbd430ea0a/raw/9a9dafafb44d8990f85243a9c7ca349acd3a0d07/worldtilegrid.csv"")

meteorite_wtg <- meteorites %>% 
  group_by(iso3c) %>% 
  summarize(n = n(),
            mass = sum(mass, na.rm = TRUE)/1000) %>%
  mutate(per_meteorite = mass/n) %>% 
  right_join(world_tile_grid, by = c(""iso3c"" = ""alpha.3"")) %>% 
  mutate(text_color = if_else(per_meteorite < 1, ""white"", ""black"")) %>% 
  replace_na(list('alpha.2' = ""NA"",
                  ""text_color"" = ""black"")) 


meteorite_map <- ggplot(meteorite_wtg, aes(x, y, fill = odds, group = iso3c)) +
  geom_tile(color = ""grey30"", size = 0.1) +
  geom_text(aes(label = alpha.2, color = text_color), family = ""Oswald"") +
  labs(x = NULL,
       y = NULL) +
  scale_y_reverse() +
  scale_fill_viridis_c(name = ""Average Metorite Mass (kg, log scale)"",option = ""cividis"", na.value = ""white"", breaks = c(1, 10, 100, 1000, 10000, 100000), guide = guide_colourbar(title.position = ""top"", title.hjust = 0)) +
  scale_color_identity() +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank(),
        legend.direction = ""horizontal"",
        legend.key.width = unit(2, ""lines""),
        legend.position = c(0.2, 0.05))


plot_data <- meteorite_wtg %>%
  select(alpha.2, n) %>% 
  mutate(n = log10(n)) %>% 
  replace_na(list(n = 0)) %>% 
  pack_bars(10, value_column = n, fill_color = last(nord(""lumina"", 5)))


packed_bars <- ggplot(plot_data) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = ""white"", size = 0.1) +
  geom_text(data = filter(plot_data, (xmax - xmin) > 0.1), aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = alpha.2), family = ""Oswald"", color = ""white"") +
  scale_fill_identity() +
  scale_color_identity() +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())


out <- packed_bars + meteorite_map  + plot_annotation(title = ""You May Need More Than An Umbrella in Russia:  Where the Most, and Heaviest, Meteorites fall"",
                                                subtitle = str_wrap(""On the left is a packed bar chart showing the top 10 regions struck by the most meteorites, while the tile map on the right shows the average meteorite mass across all regions.  Both measures have been scaled logathrimically to aid in comparability."", 180),
                                                caption = ""Data: NASA | Graphic: @jakekaupp"",
                                              theme = theme_jk())

ggsave(here(""2019"", ""week24"", ""tw24_plot.png""), out, width = 14, height = 7)
",2019,24
90,90,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week24/R/packed_bars.R,"
pack_bars <- function(data, number_rows, value_column, fill_color = ""#4B384C"", border_color = ""white"") {

value_column <- ensym(value_column)  
  
color_bar_data <- data %>% 
  top_n(number_rows, !!value_column) %>% 
  arrange(desc(!!value_column))

# calc row height based on num rows
bar_h = 1/number_rows

color_bars <- color_bar_data %>% 
  mutate(fill = fill_color,
         color = border_color,
         xmin = 0,
         xmax = !!value_column,
         ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1)),
         ymax = map_dbl(1:number_rows, ~1 - bar_h*.x))

gray_bar_data <- data %>% 
  anti_join(color_bar_data) %>% 
  arrange(desc(!!value_column))

#get max x level for each bar level
row_sums <- pull(color_bar_data, !!value_column)

#gen gray ramp function
gray_gen <- colorRampPalette(c(""#E8E8E8"", ""#cccccc""))

#gen gray ramp
grays <- gray_gen(105)
low_grays <- grays[1:50]
hi_grays  <- grays[56:105]

last_gray <- sample(c(low_grays, hi_grays), number_rows, replace = TRUE)

gray_bar_list <- vector('list', nrow(gray_bar_data))


for (i in 1:nrow(gray_bar_data)) {
  
  row <- gray_bar_data[i,]
  
  # Determine placing of each block by looking at the minium starting values of colored bars
  # adding on the new block and setting value to represent the new block length
  vert_pos <- which.min(row_sums + pull(row, !!value_column))
  
  # Assign alternating random grays to fill
  if (i == 1) {
    
    gray_fill <- sample(low_grays, 1)
    
  } else {
    
    last_gray <- last_gray[vert_pos]
    
    gray_fill <- ifelse(last_gray %in% low_grays, sample(hi_grays, 1), sample(low_grays, 1))
  }
  
  last_gray[vert_pos] <- gray_fill
  
  # Generate aes for geom_rect
  gray_bar_list[[i]]  <- mutate(row, 
                                fill = gray_fill,
                                color = border_color,
                                xmin = row_sums[[vert_pos]],
                                xmax = row_sums[[vert_pos]] + !!value_column,
                                ymin = map_dbl(1:number_rows, ~1 - bar_h*(.x-1))[[vert_pos]],
                                ymax = map_dbl(1:number_rows, ~1 - bar_h*.x)[[vert_pos]]
  )
  
  # Assign the new color_bar + rectangle as the max value for that row 
  row_sums[[vert_pos]] <- gray_bar_list[[i]]$xmax
  
}

gray_bars <- bind_rows(gray_bar_list)

bind_rows(color_bars, gray_bars)

}

",2019,24
91,91,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week25/R/analysis.R,"library(tidyverse)
library(waffle)
library(jkmisc)
library(here)

bird_counts <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-18/bird_counts.csv"")

top_10 <- bird_counts %>% 
  separate(species_latin, c(""genus"", ""family""), sep = "" "") %>% 
  group_by(genus) %>% 
  summarize(total = sum(how_many_counted_by_hour, na.rm = TRUE)) %>% 
  top_n(10, total) %>% 
  pull(genus)

by_genus <- filter(bird_counts, year >= 2000) %>% 
  separate(species_latin, c(""genus"", ""family""), sep = "" "") %>% 
  filter(genus %in% top_10) %>% 
  group_by(year, genus) %>% 
  summarize(counts_by_hour = sum(how_many_counted_by_hour, na.rm = TRUE))

colors <- set_names(gray.colors(10), top_10)

colors[""Anas""] <- ""#ffd45c""

ducks <- ggplot(by_genus, aes(values = counts_by_hour, fill = genus)) +
  geom_waffle(color = ""white"", size = .25, n_rows = 10, flip = TRUE, show.legend = FALSE) +
  facet_wrap(~year, nrow = 1, strip.position = ""bottom"") +
  coord_equal() +
  labs(title = str_to_title(""The Duck is one of the most noble, agile and intelligent creatures in the animal kingdom.""),
       subtitle = str_wrap(""Total counts per hour, of the top 10 genera from since 2000.  Duck counts (genus Anas) are highlighted in yellow, because if it looks like a duck, and quacks like a duck, we have at least to consider the possibility that we have a small aquatic bird of the family anatidae on our hands."", 120),
       caption = ""Data: www.birdscanada.org/ | Graphic: @jakekaupp"") +
  scale_x_discrete() + 
  scale_y_continuous(labels = function(x) x * 10,
                     expand = c(0,0)) +
  scale_fill_manual(values = colors) +
  theme_jk(grid = FALSE, ticks = TRUE) +
  theme(strip.text = element_text(size = rel(0.8)))

cobra_chicken <- bird_counts %>% 
  filter(year >=1950) %>% 
  group_split(species) %>% 
  map_dfr(~mutate(.x, color = if_else(species == ""Canada Goose"", ""#CB181D"", sample(gray.colors(255), 1)),
                  alpha = if_else(species == ""Canada Goose"", 1, 0.25))) %>%   
  ggplot(aes(x = year, y = how_many_counted_by_hour, group = fct_relevel(species, ""Canada Goose"", after = Inf), fill = color, alpha = alpha), color = ""grey30"") +
  geom_area() +
  scale_y_continuous(expand = c(0.01, 0.1), breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = c(seq(1950, 2010, 10), 2017)) +
  scale_fill_identity() +
  scale_alpha_identity() +
  labs(x = NULL, 
       y = NULL, 
       title = ""Rise Of The Cobra Chicken, the Scourge of the Hamilton Waterfront"",
       subtitle = str_wrap(""The Canada Goose, hilarious and aptly referred to as a 'Cobra Chicken' has been a threat to the delicate ecosystem of Hamilton's Harbor."", 120),
       caption = ""Data: www.birdscanada.org | Graphic: @jakekaupp"") +
  theme_jk(grid = FALSE, ticks = TRUE)
  

ggsave(here(""2019"", ""week25"", ""tw25_ducks.png""), ducks, width = 10, height = 4)
ggsave(here(""2019"", ""week25"", ""tw25_canada_goose.png""), cobra_chicken, width = 10, height = 4)
",2019,25
92,92,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week26/R/analysis.R,"library(sf)
library(albersusa)
library(here)
library(jsonlite)
library(RCurl)
library(janitor)
library(jkmisc)
library(cowplot)
library(tidyverse)



# Get ufo  & pop. density data----
ufo_sightings <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv"")

pop_density <- read_csv(here(""2019"", ""week26"", ""data"", ""pop_density.csv""), skip = 1) %>% 
  clean_names() %>% 
  filter(target_geo_id2 > 1000) %>% 
  mutate(fips = str_pad(target_geo_id2, 5, side = ""left"", pad = ""0"")) %>% 
  select(fips, density = contains(""density""))

# Get alberusa us county sf object----
us_counties <- counties_sf()

# Conver the us ufo sightings to an sf object and join with the alberusa to the the county fips----
usa_sightings_fips <- ufo_sightings %>% 
  filter(country == 'us') %>% 
  st_as_sf(crs = 4326, coords = c(""longitude"", ""latitude"")) 

cont_usa_sightings <- st_join(us_counties, usa_sightings_fips) 



# Some are missing!----
missing <- st_join(usa_sightings_fips, us_counties)  %>% 
  filter(is.na(fips)) %>% 
  semi_join(ufo_sightings,.)

# Make a function to call to the fcc census block API----
geocode_fips <- function(latitude, longitude, index) {
  
  url <- sprintf(""https://geo.fcc.gov/api/census/block/find?latitude=%f&longitude=%f&format=json"",  latitude, longitude)
  
  response <- getURL(url)
  
  json <- fromJSON(response)
  
  print(index)
  
  as.character(json$County['FIPS'])
}

# Make this work insistently----
insistent_geocode <- insistently(~geocode_fips(..1, ..2, ..3), rate = rate_backoff())

# Make it return NA if it fails ----
poss_insistent_geocode <- possibly(~insistent_geocode(..1, ..2, ..3), otherwise = NA_character_)

# Get the missing fips ----

if(!file.exists(here(""2019"", ""week26"", ""data"", ""missing_fips.RDS""))) {
  missing_fips <- missing %>% 
    distinct(latitude, longitude) %>% 
    mutate(index = row_number()) %>% 
    mutate(fips = pmap_chr(list(latitude, longitude, index), poss_insistent_geocode)) } else {
      
      missing_fips <- readRDS(here(""2019"", ""week26"", ""data"", ""missing_fips.RDS""))
      
    }

# Join it back to missing to fill in fips ----
missing <- left_join(missing, missing_fips) %>% 
  dplyr::select(-index) 

# Bind rows back to cont_usa_sightings for full_usa data ----
full_usa <- cont_usa_sightings %>% 
  left_join(missing, by = c(names(ufo_sightings)[c(1:2,4:9)], ""state.x"" = ""state"")) %>% 
  mutate_at(vars(contains(""fips"")), as.character) %>% 
  mutate(fips = coalesce(`fips.x`, `fips.y`)) %>% 
  select(-fips.x, -fips.y, -state_fips, -county_fips, -latitude, -longitude)

# Summarize sightings, create a ratio and add in population densities----
plot_data <- full_usa %>% 
  group_by_at(.vars = vars(fips, name, lsad, census_area, state.y, iso_3166_2)) %>% 
  summarize(sightings = n()) %>% 
  ungroup() %>% 
  mutate(sightings_ratio = 100*sightings/sum(sightings)) %>% 
  left_join(pop_density)


# create 3 buckets for variables ---
quantiles_sightings <- plot_data %>%
  pull(sightings_ratio) %>%
  quantile(probs = seq(0, 1, length.out = 4))

quantiles_density <- plot_data %>%
  pull(density) %>%
  quantile(probs = seq(0, 1, length.out = 4))

# create color scale that encodes two variables
# red for sightings and blue for population density
bivariate_color_scale <- tibble(
  ""3 - 3"" = ""#3F2949"", # high sightings, high density
  ""2 - 3"" = ""#435786"",
  ""1 - 3"" = ""#4885C1"", # low sightings, high density
  ""3 - 2"" = ""#77324C"",
  ""2 - 2"" = ""#806A8A"", # medium sightings, medium density
  ""1 - 2"" = ""#89A1C8"",
  ""3 - 1"" = ""#AE3A4E"", # high sightings, low density
  ""2 - 1"" = ""#BC7C8F"",
  ""1 - 1"" = ""#CABED0"" # low sightings, low density
) %>%
  gather(""group"", ""fill"")


# Assign each fips area to their correct group and assign the fill from the bivariate scale ----
plot_data <- plot_data %>%
  mutate(sightings_quantiles = cut(sightings_ratio,
                              breaks = quantiles_sightings,
                              include.lowest = TRUE),
    density_quantiles = cut(density,
                            breaks = quantiles_density,
                            include.lowest = TRUE),
    group = paste(as.numeric(sightings_quantiles), ""-"", as.numeric(density_quantiles))) %>%
  left_join(bivariate_color_scale, by = ""group"")


# Making ze plot ----
plot <- ggplot(plot_data) +
  geom_sf(aes(fill = fill), size = 0.05, color = ""#2b2b2b"") +
  scale_fill_identity() +
  labs(title = ""If A UFO Flew Over The Desert And No One Was Around To See It, Would Senators Be Briefed?"",
       subtitle = str_wrap(""Below is a bivariate choropleth map by county illustrating the relationship between the UFO sightings (% of recorded sightings since 1911) and population density (people per sq. mile circa 2010).  Densely populated coastal and lakeside areas along with the sparsely populated southwest have the highest sightings, whereas the less populous midwest and Alaska have lower percentages of sightings."", 110),
       caption = ""Data: NUFORC & 2010 US Census | Graphic: @jakekaupp"",
       x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank()) +
  coord_sf(clip = ""off"")

# Making ze legend ---
bivariate_legend <- bivariate_color_scale %>% 
  separate(group, into = c(""sightings"", ""density""), sep = "" - "") %>%
  mutate_at(c(""sightings"", ""density""), as.integer)

legend <- ggplot(bivariate_legend) +
  geom_tile( aes(x = sightings, y = density, fill = fill)) +
  scale_fill_identity() +
  labs(x = expression(paste(""More Sightings "", symbol('\256'))),
       y = expression(paste(""More People "", symbol('\256')))) +
  theme_jk(grid = FALSE) +
  theme(axis.title = element_text(size = 6),
        axis.text = element_blank()) +
  coord_fixed(clip = ""off"")

finished_plot <- ggdraw() +
  draw_plot(plot, 0, 0, 1, 1) +
  draw_plot(legend, 0.75, 0.075, 0.2, 0.2)


ggsave(here(""2019"", ""week26"", ""tw26_plot.png""), plot = finished_plot, width = 10, height = 6)
",2019,26
93,93,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week28/R/analysis.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(glue)


squads <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/squads.csv"")


data <- squads %>% 
  mutate(idx = goals/caps) %>% 
  filter(pos != ""GK"", caps > 0, goals > 0) %>% 
  mutate(pos = case_when(pos == ""DF"" ~ ""Defense"",
                         pos == ""FW"" ~ ""Forward"",
                         pos == ""MF"" ~ ""Mid-Field"")) %>% 
  mutate(desc = glue(""{club}\n{caps} Matches, {goals} Goals"")) %>% 
  mutate(pos = factor(pos, c(""Defense"", ""Mid-Field"", ""Forward"")))

means <- data %>% 
  group_by(pos) %>% 
  summarize(idx = mean(idx))

plot <- ggplot(data, aes(x = age, y = idx)) +
  geom_point(color = ""grey20"") +
  geom_mark_circle(aes(label = player, description = desc, filter = player == ""Khadija Shaw""), expand = unit(4, ""mm"")) +
  geom_mark_circle(aes(label = player, description = desc, filter = player  == ""Lea Schller""), expand = unit(4, ""mm"")) +
  geom_mark_circle(aes(label = player, description = desc, filter = player  == ""Ainon Phancha""), expand = unit(4, ""mm"")) +
  geom_hline(data = means, aes(yintercept = idx), color = ""firebrick"") +
  theme_jk() +
  facet_wrap(~pos, nrow = 1) +
  labs(y = NULL,
       x = ""Age"",
       title = ""Efficient Scorers Competing in the Womens World Cup by Position and Age"",
       subtitle = str_wrap(""Goals per games played in international play by player age.  Red line illustrates the average goals per game at each position.  The highly efficient players at each position are a mix of newcomers and seasoned veterans, illustrating consistency in some players through their career."", 120),
       caption = ""Data: data.world | Graphic : @jakekaupp"")

ggsave(here(""2019"", ""week28"", ""tw28_plot.png""), plot = plot, width = 10, height = 6)

",2019,28
94,94,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week29/R/analysis.R,"library(tidyverse)
library(tricolore)
library(ggtern)
library(here)
library(jkmisc)
library(magick)

r4ds_members <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-16/r4ds_members.csv"")


tern_plot <- Tricolore(r4ds_members, ""percent_of_messages_public_channels"",
          ""percent_of_messages_private_channels"",
          ""percent_of_messages_d_ms"", breaks = 5, show_data = FALSE)

legend <- tern_plot$key +
  labs(title = ""Color Legend"",
       x       = ""Public\nChannels"",
       y       = ""Private\nChannels"",
       z       = ""Direct\nMessages"") +
  theme_hidetitles() +
  theme_hidelabels() +
  theme_hideticks() +
  theme(plot.title = element_text(hjust = 0.5, family = ""Scope One"", size = 40),
        axis.text = element_text(family = ""Scope One""),
        axis.title = element_text(family = ""Scope One""))

png(here(""2019"", ""week29"", ""legend.png"")) 
legend
dev.off()

legend <- image_read(here(""2019"", ""week29"", ""legend.png""))

plot <- r4ds_members %>% 
  mutate(color = tern_plot$rgb,
         year = lubridate::year(date)) %>% 
  ggtern(aes(x = percent_of_messages_public_channels, y = percent_of_messages_private_channels, z = percent_of_messages_d_ms, color = color)) +
  geom_point(size = 3) +
  scale_color_identity() +
  labs(title = str_to_title(""The Dialogue in the R4DS Slack indicates an Open and Inclusive Learning Community""),
       subtitle = str_wrap(""Below is a ternary digram presenting the message composition in public channels, private channels and direct messages as a percentage.  Each day is represented by a point with the composition represented by position relative to each axes.  Composition is additionally encoded by color as illustrated on the inset legend."", 100),
       x       = ""Public\nChannels"",
       xarrow  = ""More Public Channel Messages"",
       y       = ""Private\nChannels"",
       yarrow  = ""More Private Channel Messages"",
       z       = ""Direct\nMessages"",
       zarrow  = ""More Direct Messages"",
       caption = ""Data: R4DS Community | Graphic: @jakekaupp"") +
  theme(panel.background = element_rect(fill = ""#2E3440""),
        panel.grid = element_line(color = ""#ffffff"", size = 0.1),
        panel.grid.minor = element_blank(),
        text = element_text(family = ""Oswald""),
        plot.subtitle = element_text(family = ""Scope One""),
        axis.text = element_text(family = ""Scope One""),
        axis.title = element_text(family = ""Scope One"")) +
  theme_showarrows() +
  theme_arrowlong() 


png(here(""2019"", ""week29"", ""tw29_plot.png""), width = 10, height = 8, units = ""in"", res = 200)
grid::grid.newpage()
plot
grid::grid.raster(legend, width = 0.18, height = 0.2, x = 0.75, y = 0.7)
dev.off()
",2019,29
95,95,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week3/R/analysis.R,"library(tidyverse)
library(here)
library(nord)
library(jkmisc)
library(ggbeeswarm)
library(ggrepel)

agencies <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-15/agencies.csv"")

launches <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-15/launches.csv"")


us_launch_data <- launches %>% 
  filter(agency == ""US"" | state_code == ""US"") %>% 
  mutate(type = gsub(""Zenit-"", ""Zenit "", type),
         type = gsub(""/"", "" "", type),
         type = gsub(""Minotaur-"", ""Minotaur "", type)) %>% 
  separate(type, ""type"", sep = "" "", extra = ""drop"") %>% 
  mutate(type = if_else(type == ""Space"", ""Space Shuttle"", sprintf(""%s Program"",type))) %>% 
  mutate(label = if_else(type == ""Space Shuttle"" & category == ""F"", ""Challenger Disaster"", NA_character_)) %>% 
  group_by(type) %>% 
  filter(n() > 10)

plot <- ggplot(us_launch_data, aes(x = launch_year, y = type), size = 4) +
  geom_quasirandom(data = filter(us_launch_data, category == ""O""), alpha = 0.2, fill = nord(""polarnight"", 2)[2], shape = 21, groupOnX = FALSE) +
  geom_quasirandom(data = filter(us_launch_data, category == ""F""), fill = nord(""victory_bonds"", 5)[1], shape = 21, groupOnX = FALSE, color = ""grey30"", stroke = 0.2) +
  theme_jk(grid = ""XY"", dark = FALSE) +
  labs(x = NULL,
       y = NULL,
       title = ""From the Space Race to Space-X: 1548 Successes and 101 Failures of US Launch Vehicles from 1958-2018."",
       subtitle = str_wrap(""A beeswarm plot illustrating the success or failure of a launch vehicle program over time. Red dots indicate failed launches, grey dots indicate success.  Deeper grey colors indicate a higher frequency of success in a given year due to multiple launches. Only includes programs with more than 10 launches"", 120),
       caption = ""Data: JSR Launch Vehicle Database | Analysis: @jakekaupp"")

plot <- plot + annotate(""segment"", x = 1987, xend = 1986.2, y = 8.7, yend = 8.2, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 1986, y = 9, label = ""Challenger Disaster"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1]) +
  annotate(""segment"", x = 1959, xend = 1958.2, y = 1.7, yend = 1.2, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 1959, y = 2.5, label = ""First Communication\nSatellite Protoype"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1], hjust = 0) +
  annotate(""segment"", x = 2015, xend = 2015, y = 5.5, yend = 3.1, arrow = arrow(length = unit(1, ""mm"")), color = nord(""victory_bonds"", 5)[1]) +
  annotate(""text"", x = 2013, y = 6.5, label = ""SpaceX Falcon 9\nStrut Failure"", family = ""Scope One"", color = nord(""victory_bonds"", 5)[1], hjust = 0)

ggsave(here(""2019"", ""week3"", ""tt_week3.png""), plot, width = 11, height = 5)

  ",2019,3
96,96,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week30/R/analysis.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(geofacet)
library(patchwork)
library(glue)
library(cowplot)

wildlife_impacts <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-23/wildlife_impacts.csv"")


# Petal charts ----
plot_data <- wildlife_impacts %>% 
  #mutate(airport_type = if_else(str_detect(airport, ""INTL""), ""INT"", ""DOM"")) %>% 
  filter(state %in% state.abb) %>% 
  count(state,  incident_month, incident_year) %>% 
  complete(incident_year = 1990:2018, state,  incident_month = 1:12, fill = list(n = 0)) %>% 
  group_by(incident_year, state) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent))


state_flower_grid <- plot_data %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_area(position = position_identity(), alpha = 0.5, size = 0.1) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  scale_y_continuous(limits = c(0,1), breaks = c(0.5, 0.1)) +
  scale_fill_viridis_c(""Year"", option = ""plasma"", direction = 1) +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  labs(x = NULL, 
       y = NULL) +
  coord_polar() +
  facet_geo(~ state, grid = ""us_state_grid2"") +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines""))

flower_legend <- plot_data %>% 
  filter(state == ""ME"") %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_area(position = position_identity(), alpha = 0.5, size = 0.1) +
  geom_mark_circle(aes(label = glue(""{month.name[incident_month]}""), description = ""Single colour petal represents a single collison event during this month"", filter = incident_year == 1991 & incident_month == 3), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm"")) +
  geom_mark_circle(aes(label = glue(""{month.name[incident_month]}""), description = ""Multiple coloured petals represent repeated annual incidents during this month"", filter = incident_year == 1996 & incident_month == 11), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm"")) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  scale_y_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  scale_fill_viridis_c(""Year"", option = ""plasma"", direction = 1, breaks = c(seq(1990, 2020, by = 5))) +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  guides(fill = guide_colorbar(), color = ""none"") +
  labs(x = NULL, y = NULL) +
  coord_polar(clip = ""off"") +
  theme_jk(grid = ""XY"") +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

color_legend <- tibble(year = 1990:2018,
                       y = 1) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = y, fill = year), show.legend = FALSE, color = ""white"", size = 0.1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  scale_x_continuous(breaks = c(1990, 2000, 2010, 2018)) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  labs(x = NULL, y = NULL) +
  theme(axis.text.y = element_blank())

finished_legend <- ggdraw() +
  draw_plot(flower_legend, 0, 0, 1, 1) +
  draw_plot(color_legend, 0.3, -0.17, 0.4, 0.4)

out <- wrap_plots(finished_legend, state_flower_grid,  nrow = 1, widths = c(0.85, 1.2)) +
   plot_annotation(title = ""Seasonality of Wildlife-Aircraft Collisions by State"",
                   subtitle = str_wrap(""Presented below is a petal chart of of wildlife collisions with aircraft, with an inset legend showing assisting interpretation.  Wildlife collisions by state are presented as small multiples, geographically arranged.  Smaller compact flowers illustrate states with collisions occuring year round, while the bigger flowers tend to see single or concentrated spikes of collision activity.  Flowers with diverse colours indicate repeated annual collisons while the single-hued flowers illustrate more sparse or isolated annual events."", 210),
                   caption = ""Data: FAA Wildlife Strike Database | Graphic: @jakekaupp"",
                   theme = theme_jk())


ggsave(here(""2019"",""week30"", ""tw30_plot.png""), out, width = 16, height = 10, type = ""cairo"")


",2019,30
97,97,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week30/R/experiment.R,"library(tidyverse)
library(here)
library(jkmisc)
library(ggforce)
library(geofacet)
library(patchwork)
library(glue)
library(cowplot)

wildlife_impacts <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-23/wildlife_impacts.csv"")


# Petal charts ----
plot_data <- wildlife_impacts %>% 
  filter(state %in% state.abb) %>% 
  count(state,  incident_month, incident_year) %>% 
  group_by(incident_year, state) %>% 
  mutate(percent = n/sum(n)) %>% 
  mutate(percent = ifelse(is.nan(percent), 0, percent)) %>% 
  mutate(angle = 90 - (incident_month-1)*30,
         angle = ifelse(angle < 0, 360 + angle, angle),
         radians = angle*pi/180,
         x0 = percent * cos(radians),
         y0 = percent * sin(radians))
         
         
big_flower <- ggplot(plot_data) +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = percent, b = percent/3, angle = radians, fill = incident_year), alpha = 0.2, size = 0, color = ""white"") +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  theme_jk(grid = FALSE, plot_title_size = 12) +
  labs(x = NULL, y = NULL, title = ""National"") +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines""),
        plot.title = element_text(hjust = 0.5)) +
  coord_equal() 

state_flower <- big_flower +
  facet_geo(~ state, grid = ""us_state_grid2"")


flower_axes_lines <- tibble(idx = 1:12,
                            angle = 90 - (idx-1)*30,
                            angle2 = ifelse(angle < 0, 360 + angle, angle),
                            radians = angle2*pi/180)

axes_lines <- function(radius) {
  
  tibble(segment = 1:6,
                     x = c(0, radius*cos(pi/3), radius*cos(pi/6), radius, radius*cos(pi/6), radius*cos(pi/3)),
                     xend = c(0, -radius*cos(pi/3), -radius*cos(pi/6), -radius, -radius*cos(pi/6), -radius*cos(pi/3)),
                     y = c(radius, radius*sin(pi/3), radius*sin(pi/6), 0, -radius*sin(pi/6), -radius*sin(pi/3)),
                     yend = c(-radius, -radius*sin(pi/3), -radius*sin(pi/6), 0, radius*sin(pi/6), radius*sin(pi/3))) 
  }

axes_labels <- function(radius) {
  tibble(month = 1:12,
                      label = month.abb[month],
                      x = c(axes_lines(radius)$x, axes_lines(radius)$xend),
                      y = c(axes_lines(radius)$y, axes_lines(radius)$yend))  }


flower_legend <- plot_data %>% 
  filter(state == ""ME"") %>% 
  ggplot(aes(x = incident_month, y = percent, group = incident_year, color = incident_year, fill = incident_year)) +
  geom_segment(data = axes_lines(2), aes(x = x, xend = xend, y = y , yend = yend), size = 0.1, color = ""#cccccc"", inherit.aes = FALSE) +
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), inherit.aes = FALSE, size = 0.1, color = ""#cccccc"") +
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE, size = 0.1, color = ""#cccccc"") +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = percent, b = percent/3, angle = radians, fill = incident_year), alpha = 0.5, size = 0.1, color = ""white"") +
  geom_mark_circle(aes(x = 2*x0, y = 2*y0, label = glue(""{month.name[incident_month]}, {incident_year}""), description = ""Single colour long petal represents 100% of collison event during this month and year"", filter = incident_year == 1991 & incident_month == 3), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm""), inherit.aes = FALSE) +
  geom_mark_circle(aes(x = x0, y = y0, label = glue(""{month.name[incident_month]}, Multiple years""), description = ""Multiple coloured petals represent repeated annual incidents during this month"", filter = incident_year == 1996 & incident_month == 11), expand = unit(1, ""mm""), label.family = c(""Oswald"", ""Scope One""), label.fontsize = 10, label.buffer = unit(5, ""mm""), inherit.aes = FALSE) +
  geom_text(data = filter(axes_labels(2.15), label != ""Feb""), aes(x = x, y = y, label = label), inherit.aes = FALSE, family = ""Oswald"") +
  scale_color_viridis_c(option = ""plasma"", direction = 1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  labs(x = NULL, y = NULL) +
  theme_jk(grid = FALSE) +
  coord_fixed(clip = ""off"") +
  theme(axis.text = element_blank(),
        legend.position = ""none"")

color_legend <- tibble(year = 1990:2018,
                       y = 1) %>% 
  ggplot() +
  geom_tile(aes(x = year, y = y, fill = year), show.legend = FALSE, color = ""white"", size = 0.1) +
  scale_fill_viridis_c(option = ""plasma"", direction = 1) +
  scale_x_continuous(breaks = c(1990, 2000, 2010, 2018)) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  labs(x = NULL, y = NULL) +
  theme(axis.text.y = element_blank())


finished_legend <- ggdraw() +
  draw_plot(flower_legend, 0, 0, 1, 1) +
  draw_plot(color_legend, 0.3, -0.175, 0.4, 0.4)

state_flower_grid <- ggdraw() +
  draw_plot(state_flower, 0, 0, 1, 1) + 
  draw_plot(big_flower, 0.75, 0.15, 0.25, 0.25)

out <- wrap_plots(finished_legend, state_flower_grid, nrow = 1, widths = c(0.85, 1.2)) +
  plot_annotation(title = ""Seasonality of Wildlife-Aircraft Collisions by State"",
                  subtitle = str_wrap(""Presented below is a petal chart of of wildlife collisions with aircraft across the US from 1990-2018. Below this is an inset legend showing assisting interpretation of the plots.  On the right are wildlife-aircraft collisions by state presented as small multiples, geographically arranged, with an inset flower representing the National data. Petal length is the annual proportion of collisions in a given month.  Smaller compact flowers illustrate states with collisions occuring year round, while the bigger flowers tend to see single or concentrated spikes of collision activity.  Flowers with diverse colours indicate repeated annual collisons while the single-hued flowers illustrate more sparse or isolated annual events."", 210),
                  caption = ""Data: FAA Wildlife Strike Database | Graphic: @jakekaupp"",
                  theme = theme_jk())


ggsave(here(""2019"",""week30"", ""tw30_plot_remix.png""), out, width = 16, height = 10, type = ""cairo"")
",2019,30
98,98,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week31/R/analysis.R,"library(tidyverse)
library(here)
library(ggbeeswarm)
library(jkmisc)

video_games <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-30/video_games.csv"")  
 
all_games <- video_games %>% 
  filter(!is.na(game), !is.na(metascore))  %>% 
  mutate(developer = tolower(developer),
         idx = row_number())

plot_data <- tibble(facet = c(""bioware"", ""valve"", ""ubisoft"", ""rockstar"", ""square enix""),
       data = list(all_games)) %>% 
  mutate(filtered = map2(data, facet, ~mutate(.x, option = case_when(str_detect(tolower(developer), .y) ~ ""selected"", 
                                                                     TRUE ~ ""other"")))) %>% 
  unnest(filtered) %>% 
  mutate(facet = case_when(facet == ""bioware"" ~ ""BioWare"",
                           facet == ""valve"" ~ ""Valve"",
                           facet == ""ubisoft"" ~ ""Ubisoft"",
                           facet == ""rockstar"" ~ ""Rockstar"",
                           facet == ""square enix"" ~ ""Square Enix"")) %>% 
  mutate(option = factor(option, c(""other"", ""selected""))) %>% 
  arrange(facet, option)



mean <- all_games %>% 
  summarize(metascore = mean(metascore, na.rm = TRUE)) %>% 
  pull(metascore)

min_labels <- plot_data %>% 
  filter(option == ""selected"", !is.na(metascore)) %>% 
  group_by(facet) %>% 
  filter(metascore == min(metascore))

max_labels <- plot_data %>% 
  filter(option == ""selected"", !is.na(metascore)) %>% 
  group_by(facet) %>% 
  filter(metascore == max(metascore)) %>% 
  mutate(game = if_else(str_detect(""FINAL FANTASY"", game), ""Final Fantasy IX"", game)) %>% 
  slice(1)

plot <- ggplot(plot_data) +
  geom_quasirandom(aes(y = metascore, x = 0, alpha = option, fill = option, size = option), shape = 21, method = ""tukey"", show.legend = FALSE) +
  geom_label(data = min_labels, aes(x = 0, y = metascore, label = game), family = ""Oswald"", nudge_y = -2, fill = ""#E5E4E2"", label.r = unit(0, ""lines""), alpha = 0.5) +
  geom_label(data = max_labels, aes(x = 0, y = metascore, label = game), family = ""Oswald"", nudge_y = +2, fill = ""#E5E4E2"", label.r = unit(0, ""lines""), alpha = 0.5) +
  geom_hline(yintercept = mean, color = ""firebrick"", size = 0.5, linetype = ""dashed"") +
  labs(x = NULL, 
       y = NULL,
       title = ""How Do The Big Developers Score Against The Competition on Steam?"",
       subtitle = str_wrap(""Presented below is a jittered strip plot of metascore by developer.  Titles worked on by that developer are highlighted in yellow, the average metascore (72) is shown as a dashed red line. Annotations show the top an bottom rated titles for each developer.  Sqaure and Ubisoft have the most titles with less than average reviews amongst the large developers."", 180),
       caption = ""Data: SteamSpy | Graphic: @jakekaupp"") +
  scale_size_manual(values = c(""other"" = 2, ""selected"" = 2)) +
  scale_y_continuous(limits = c(20, 100), breaks = seq(20, 100, 20)) +
  scale_fill_manual(values = c(""other"" = ""#E5E4E2"", ""selected"" = ""#ffd644"")) +
  scale_alpha_manual(values = c(""other"" = 0.05, ""selected"" = 1)) +
  theme_jk(grid = ""Y"", dark = TRUE) +
  facet_wrap(~facet, nrow = 1) +
  theme(strip.text = element_text(color = ""white""),
      axis.text.x = element_blank())

ggsave(here(""2019"", ""week31"", ""tw31_plot.png""), plot, width = 14, height = 8)
",2019,31
99,99,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week32/R/analysis.R,"library(tidyverse)
library(ggforce)
library(here)
library(jkmisc)
library(patchwork)

bob_ross_paintings <- here(""2019"", ""week32"", ""data"", ""tidytuesday_201932_bob_ross_paintings.csv"")

data <- read_csv(bob_ross_paintings, col_names = c('episode', 'title', 'color', 'color_name')) %>% 
  mutate(season = parse_number(str_extract(episode, ""S\\d+"")),
         color = if_else(color == ""#FFFFFF"", ""grey80"", color))


plot_data <- data %>% 
  count(season, title, color_name, color) %>% 
  group_by(season, title) %>% 
  mutate(percent = n/sum(n)) %>% 
  ungroup() %>% 
  mutate(color_number = as.numeric(factor(color_name))) %>% 
  mutate(angle = (color_number-1)*(360/15),
         angle = ifelse(angle < 0, 360 + angle, angle),
         radians = angle*pi/180,
         x0 = percent * cos(radians),
         y0 = percent * sin(radians))
  

plot_spiros <- function(data) {
  
season <- sprintf(""Season %s"",unique(data$season))
  
ggplot(data) +
  geom_spiro(aes(R = ifelse(percent == 1, 0.1, 1 - percent), r = percent, d = radians, color = color, group = color), size = 0.1) +
  scale_color_identity() +
  theme_jk(grid = FALSE, plot_title_size = 8, strip_text_size = 8) +
  facet_wrap(~ title, ncol = 1, labeller = label_wrap_gen(15)) +
  labs(x = NULL, y = NULL, title = season) +
  theme(axis.text = element_blank(),
        legend.position = ""none"",
        panel.spacing = unit(0.1, ""lines"")) +
  coord_equal()  }


plots <- plot_data %>% 
  split(.$season) %>% 
  map(plot_spiros)


all_seasons <- wrap_plots(plots, nrow = 1) + plot_annotation(title = ""Happy Accidents with 1960s Toys: Sprirographs of Palette Colors of Bob Ross Paintings for 31 Seaons"",
                                                             subtitle = ""Illustrated below is a spirograph tracing of the 15 distinct un-mixed palette colours used in each of Bob Ross' paintings.  The more colours used in a painting, the larger the spirograph and it appears similar to china pattern while those paintings with a more minimalist palette show up as smaller sparse rings."",
                                                             caption = ""Data: c/o @geokaramanis | Graphic: @jakekaupp"",
                                                             theme = theme_jk())


ggsave(filename = here(""2019"", ""week32"", ""tw32_plot.png""), plot = all_seasons, width = 30, height = 15, type = ""cairo"")

twitter <- map(plots, ~.x + theme(strip.text = element_blank()))


all_seasons_twitter <- wrap_plots(twitter, nrow = 1) + plot_annotation(title = ""Happy Accidents with 1960s Toys: Sprirographs of Palette Colors of Bob Ross Paintings for 31 Seaons"",
                                                             subtitle = str_wrap(""Illustrated below is a spirograph tracing of the 15 distinct un-mixed palette colours used in each of Bob Ross' paintings.  The more colours used in a painting, the larger the spirograph and it appears similar to china pattern while those paintings with a more minimalist palette show up as smaller sparse rings."", 265),
                                                             caption = ""Data: c/o @geokaramanis | Graphic: @jakekaupp"",
                                                             theme = theme_jk())


ggsave(filename = here(""2019"", ""week32"", ""tw32_plot_for_twitter.png""), plot = all_seasons_twitter, width = 20, height = 7)


",2019,32
100,100,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week33/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(lubridate)
library(here)
library(patchwork)


emperors <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv"") 

ad_births <- c(""Augustus"", ""Tiberius"", ""Claudius"", ""Galba"")

emp_numeric_years <- emperors %>% 
  mutate_if(is.Date, list(year = year)) %>% 
  mutate(birth_year = if_else(name %in% ad_births, -birth_year, birth_year),
         reign_start_year = if_else(name == ""Augustus"", -reign_start_year, reign_start_year))


missing_birth_estimates <- emp_numeric_years %>% 
  filter(is.na(birth_year)) %>% 
  mutate(birth_year = case_when(name == ""Florian"" ~ 202,
                                name == ""Numerian"" ~ 248,
                                name == ""Carinus"" ~ 245,
                                name == ""Severus II"" ~ 260,
                                name == ""Vetranio"" ~ 325))


plot_data <- emp_numeric_years %>% 
  filter(!is.na(birth_year)) %>% 
  bind_rows(missing_birth_estimates)


dynasties <- plot_data %>% 
  group_by(dynasty) %>% 
  summarize(reign_start_year = min(reign_start_year),
               reign_end_year = max(reign_end_year))

roman_palette <- set_names(colorRampPalette(c(""#191970"", ""#FF7F50""))(8), unique(plot_data$dynasty))


overall <- ggplot(plot_data, aes(y = 0)) +
  geom_segment(aes(x = reign_start_year, xend = reign_end_year, yend = 0, color = dynasty), size = 4) +
  scale_color_manual(""Dynasty"", values = roman_palette, breaks = names(roman_palette)) +
  scale_x_continuous(breaks = c(-62, 0, 100, 200, 300, 400), labels = c(""62 BC"", ""1 AD"", ""100 AD"", ""200 AD"", ""300 AD"", ""400 AD"")) +
  expand_limits(x = c(-62, 450)) +
  labs(x = NULL, y = NULL,
       caption = ""Data: Wikipedia via @geokaramanis | Graphic: @jakekaupp"") +
  theme_jk(grid = ""X"") +
  theme(axis.text.y = element_blank(),
        legend.position = ""none"")

bars <- ggplot(plot_data, aes(y = reorder(name, reign_start_year))) +
  geom_segment(aes(x = birth_year, xend = death_year, yend = name), size = 2, color = ""grey90"") +
  geom_segment(aes(x = reign_start_year, xend = reign_end_year, yend = name, color = dynasty), size = 2) +
  geom_segment(data = filter(plot_data, reign_start_year == reign_end_year), aes(x = reign_start_year - 0.5, xend = reign_start_year + 0.5, y = name, yend = name, color = dynasty), size = 2) +
  geom_text(aes(x = death_year, label = name), hjust = 0, family = ""Scope One"", size = 2, nudge_x = 3) +
  scale_color_manual(""Dynasty"", values = roman_palette, breaks = names(roman_palette)) +
  scale_x_continuous(breaks = c(-62, 0, 100, 200, 300, 400), labels = c(""62 BC"", ""1 AD"", ""100 AD"", ""200 AD"", ""300 AD"", ""400 AD"")) +
  expand_limits(x = c(-62, 450)) +
  labs(x = NULL, 
       y = NULL,
       title = str_to_title(""When in Rome: The Game of Imperial Thrones. You Win or You Die.""),
       subtitle = str_wrap(""Illustrated below is a timeline of the life and reigns of Roman Emperors from 62 BC to 395 AD.  The light grey bar depicts the liftime of the emperor, the colored bar (by dynasty) indicates the duration of their reign. An overall timeline by dynasty is shown near the horizontal axis. Unsurprisingly, the majority of emperors reign ending also coincides with the end of their life."", 100)) +
  theme_jk(grid = ""X"") +
  theme(axis.text = element_blank(),
        legend.position = c(0.2, 0.7),
        legend.background = element_rect(fill = ""white"", size = 0))


out <- wrap_plots(bars, overall, ncol = 1, heights = c(0.9, 0.1))

ggsave(here(""2019"", ""week33"", ""tw33_plot.png""), out, width = 7.5, height = 8)
",2019,33
101,101,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week34/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(ggtext)
library(jkmisc)
library(waffle)
library(ggforce)
library(glue)
library(ragg)

nuclear_explosions <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-20/nuclear_explosions.csv"")




plot_data <- nuclear_explosions %>% 
  mutate(date_long = ymd(date_long)) %>% 
  group_by(country, date_long) %>% 
  summarize(n = n(),
            total_yield = sum(yield_upper, na.rm = TRUE)) %>% 
  group_by(country) %>% 
  mutate(c_sum = cumsum(n),
         c_yield = cumsum(total_yield)/1000) %>% 
  filter(country %in% c(""USSR"", ""USA""))

items <- nuclear_explosions %>% 
  mutate(date_long = ymd(date_long)) %>% 
  filter(country %in% c(""USSR"", ""USA"")) %>% 
  group_by(country) %>% 
  top_n(1, yield_upper) %>% 
  slice(1) %>%
  semi_join(plot_data, .) %>% 
  mutate(date_long = ymd(date_long) - 1) %>% 
  mutate(name = if_else(country == ""USA"", ""March 1954: Castle Bravo"", ""October 1961: Tsar Bomba""),
         description  = if_else(country == ""USA"", ""2nd most powerful nuclear test explosion, 3 times over the predicted 5 MT yield."", ""Most powerful nuclear test explosion, twice the predicted yield of 25 MT.""))

explosions <- ggplot(plot_data, aes(x = c_sum, y = c_yield, color = country)) +
  geom_step(linetype = ""solid"", size = 1, direction = ""hv"") +
  geom_point(data = filter(plot_data, date_long %in% range(date_long))) +
  geom_text(data = filter(plot_data, date_long == last(date_long)), aes(label = year(ymd(date_long))), family = ""Oswald"", hjust = -0.5) +
  geom_text(data = filter(plot_data, date_long == first(date_long)), aes(label = year(ymd(date_long))), family = ""Oswald"", nudge_y = c(10, -10)) +
  geom_mark_circle(data = items, aes(color = country, label = name, description = description), expand = unit(3, ""mm""), label.margin = margin(5, 5, 5, 5, ""mm""), con.colour = c(""#0052A5"", ""#FF2400""), label.family = c(""Oswald"",""Scope One""), label.fill = NA, label.minwidth = unit(50, ""mm""), label.fontsize = 10, con.type = ""straight"") +
  scale_color_manual(values = c('USSR' = ""#FF2400"", ""USA"" = ""#0052A5"")) +
  scale_fill_manual(values = c('USSR' = ""#FF2400"", ""USA"" = ""#0052A5"")) +
  scale_y_continuous(labels = function(x) scales::comma(x, suffix = "" MT"")) +
  labs(x = 'Cumulative Number of Explosions',
       y = ""Cumulative Yield (MT)"",
       title = ""Nuclear Weapons Research Race During And After The Cold War"",
       subtitle = ""Illustrated below is a step chart showing the number and yield of nuclear explosions for weapons research for <span style='color:#0052A5'>**USA**</span> and <span style='color:#FF2400'>**USSR**</span>.  During this race nearly<br>500 MT of nuclear explosions and accompanying fallout blanketed the world. The effects are still being dealt with to this date."",
       caption = ""Data: Our World in Data | Graphic: @jakekaupp"") +
  theme_jk(subtitle_family = ""PT Serif"",
           caption_family = ""PT Serif"") +
  theme(plot.title = element_markdown(), 
        plot.subtitle = element_markdown(),
        legend.position = ""none"")
  
ggsave(here(""2019"", ""week34"", ""tw34_plot_2.png""), plot = explosions, width = 12, height = 6, device = agg_png())


# Waffle ----

waffle_data <- nuclear_explosions %>% 
  mutate(purpose = case_when(grepl(""WR"", purpose) ~ ""WR"",
                             grepl(""WE"", purpose) ~ ""WE"",
                             grepl(""PNE"", purpose) ~ ""PNE"",
                             TRUE ~ purpose)) %>% 
  filter(country %in% c(""USSR"", ""USA"")) %>% 
  count(year, purpose) %>% 
  group_by(purpose) %>% 
  mutate(c_sum = cumsum(n))


pal <- set_names(sample(grey.colors(50),10), unique(waffle_data$purpose))

pal[""WR""] <- ""#8A0303""

waffle <- ggplot(waffle_data, aes(fill = purpose, values = c_sum)) +
  geom_waffle(color = ""white"", size = .25, n_rows = 10, flip = TRUE, show.legend = FALSE) +
  facet_wrap(~year, strip.position = ""bottom"", nrow = 1) +
  scale_x_discrete() + 
  scale_y_continuous(labels = function(x) x * 10,
                     expand = c(0,0)) +
  coord_equal() +
  scale_fill_manual(values = pal) +
  labs(y = ""Cumulative Number of Explosions"",
       title = ""Nuclear Weapons Research Testing During the Cold War Was the Primary Driver of Controlled  Nuclear Explosions"",
       subtitle = ""Illustrated below is a timeline of waffle charts showing the distribution of cumulative explosions from <span style='color:#8A0303'>**weapons research**</span> or <span style='color:#4D4D4D'>**other purposes**</span>. Widescale Nuclear testing ceased in the mid-'90's. The only active country<br>conducting nuclear testing in this era is North Korea, weathering the disapproval and ire of the global community."",
       caption = ""Data: Our World in Data | Graphic: @jakekaupp"") +
  theme_jk(strip_text_size = 10,
           subtitle_family = ""PT Serif"",
           caption_family = ""PT Serif"") +
  theme(panel.grid = element_blank(), 
        axis.ticks.y = element_line(),
        strip.text = element_text(hjust = 0.5),
        plot.title = element_markdown(), 
        plot.subtitle = element_markdown())
  
ggsave(here(""2019"", ""week34"", ""tw34_plot.png""), plot = waffle, width = 18, height = 6, device = agg_png())
",2019,34
102,102,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week35/R/analysis.R,"library(tidyverse)
library(tidygraph)
library(ggraph)
library(colorspace)
library(glue)
library(jkmisc)
library(ggtext)
library(ragg)
library(here)


html_string <- glue(""Shown below is a co-occurance network of guest stars in The Simpsons, best explained as a 'Who co-stars together?'. {subtitle_names} are the most frequent guest stars in the series."")

str_break <- function (html_string, width = 80, indent = 0, exdent = 0) {

tags <- str_extract_all(html_string, ""<.*?>"") %>% 
  flatten_chr() 

index <- sprintf(""tag_%s"", seq_along(tags))

string <- str_replace_all(html_string, set_names(index, tags))

  if (width <= 0) 
    width <- 1
  
  out <- stringi::stri_wrap(string, width = width, indent = indent, 
                   exdent = exdent, simplify = FALSE)
  
  broken <- vapply(out, str_c, collapse = ""<br>"", character(1))
  
  str_replace_all(broken, set_names(tags, index))
  
}

text_bc <- function(text, color) {
  
  glue(""<span style = color:{color}>**{text}**</span>"")
  
}


simpsons <- read_delim(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-27/simpsons-guests.csv"", delim = ""|"", quote = """")

nodes <- count(simpsons, guest_star) 

top5 <- top_n(nodes, 5, n) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column(var = ""color"") %>% 
  select(-n)

nodes <- nodes %>% 
  left_join(top5) %>% 
  replace_na(list(color = 7)) %>% 
  mutate(guest_star = str_remove(guest_star, ""'"")) %>% 
  mutate(alpha = if_else(color < 7, 1, 0.5)) 


edges <- simpsons %>% 
  mutate(guest_star = str_remove(guest_star, ""'"")) %>% 
  group_by(production_code) %>% 
  mutate(co_stars = map(guest_star, ~str_subset(guest_star, .x, negate = TRUE))) %>%
  ungroup() %>% 
  mutate(co_stars = map(co_stars, ~ifelse(length(.x) == 0, NA_character_,.x))) %>% 
  unnest(co_stars) %>% 
  count(guest_star, co_stars) %>% 
  filter(!is.na(n), !is.na(co_stars)) %>% 
  set_names(c(""from"", ""to"", ""n"")) %>% 
  left_join(top5, by = c(""from"" = ""guest_star"")) %>% 
  left_join(top5, by = c(""to"" = ""guest_star"")) %>% 
  mutate(color = coalesce(color.x, color.y)) %>% 
  replace_na(list(color = ""grey80"")) 

  
colors <- set_names(tol6qualitative, top5$guest_star)  

subtitle_names <- imap(colors[1:5], ~text_bc(.y, .x)) %>% 
  glue_collapse(sep = ', ') %>% 
  glue("" and {imap(colors[6], ~text_bc(.y, .x))}"")

co_star_graph <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)

co_star_plot <- co_star_graph %>% 
  activate(nodes) %>% 
  arrange(n) %>% 
  mutate(degree = centrality_degree()) %>% 
  filter(degree > 1) %>%  
  ggraph(layout = ""fr"") + 
  geom_edge_arc(edge_width = 0.5, curvature = 0.2, aes(alpha = stat(index), edge_colour = color)) +
  geom_node_point(aes(size = n, color = color, alpha = alpha, fill = color), shape = 21) +
  scale_color_manual(values = c(darken(tol6qualitative), ""grey80"")) + 
  scale_alpha_identity() +
  scale_edge_color_manual(values = c(tol6qualitative, ""grey85"")) +
  scale_fill_manual(values = c(tol6qualitative, ""grey80"")) + 
  scale_size(range = c(2,6)) +
  labs(x = NULL,
       y = NULL,
       title = ""The Guest Star Backbone Of A Simpsons Co-Star Network"",
       subtitle = glue(""Shown below is a co-occurance network of guest stars in The Simpsons, best explained as a 'Who co-stars together?'.<br> {subtitle_names} <br>are the most frequent guest stars in the series.""),
       caption = ""**Data**: Wikipedia via @datawookie | **Graphic**: @jakekaupp"") +
  theme_jk(grid = FALSE,
           subtitle_family = ""Lora"",
           caption_family = ""Lora"",
           markdown = TRUE) +
  theme(legend.position = ""none"",
        axis.text = element_blank())

ggsave(here(""2019"", ""week35"", ""tw35_plot.png""), plot = co_star_plot, device = agg_png(), width = 9, height = 8)

ggplot(mtcars, aes(x = mpg, y = disp)) +
  geom_point() +
  labs(title = paste0(highlight_text(""This is bold"", style = ""bi""), ""This isn't""),
       subtitle = paste0(highlight_text(""This is bold"", style = ""bi""), ""This isn't"")) +
  theme_jk() +
  theme(plot.title = ggtext::element_markdown(),
        plot.subtitle = ggtext::element_markdown())
",2019,35
103,103,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week36/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(nord)
library(glue)
library(here)
library(ragg)

cpu <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/cpu.csv"")

gpu <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/gpu.csv"")

ram <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-03/ram.csv"")


plot_data <- list(cpu = cpu, gpu = gpu) %>% 
  imap_dfr(~select(.x, date_of_introduction, transistor_count, area, process) %>% 
             mutate(type = .y)) %>% 
  group_by(date_of_introduction, type) %>% 
  summarize_at(vars(transistor_count, area, process), mean, na.rm = TRUE) %>% 
  arrange(date_of_introduction, type)

strip_labels <- tibble(type = c(""cpu"", ""gpu""))

plot <- ggplot(plot_data, aes(x = area, y = transistor_count)) +
  geom_text(data = strip_labels, aes(label = toupper(type)), x = 400, y = 4, family = ""Oswald Bold"", size = 18, color = ""grey90"") +
  geom_smooth(method = ""auto"", formula = y ~ log10(x), se = FALSE, size = 0.5,  color = nth(nord_palettes$victory_bonds, 3)) +
  geom_hline(aes(yintercept = 10^10), linetype = ""dotted"", color = first(nord_palettes$victory_bonds)) +
  geom_point(aes(color = log10(process)), size = 3) +
  scale_color_nord(name = ""Process Size"",
                        discrete = FALSE,
                        palette = ""lumina"",
                        reverse = TRUE,
                        labels = function(x) glue(""{scales::comma(10^x)} nm""),
                        breaks = c(1, 2, 3, 4)) +
  scale_y_log10(breaks = c(1, 10^4, 10^6, 10^8, 10^10),
                labels = c(""1"", ""10K"", ""1M"", ""100M"", ""10B"")) +
  scale_x_continuous(labels = function(x) glue(""{x} {expression(mm^2)}"")) +
  facet_wrap(~type) +
  labs(x = NULL, 
       y = NULL,
       title = ""Moore's Law May Be Dead, Killed By The Tension Between Manufacturing and Transistor Density"",
       subtitle = ""*Moore's law*, the observation that the **number of transistors** on integrated circuits **doubles every two years**<br>
       hasn't held.  Transistor density is reaching a plateau, requiring manufacturing changes of an increase in available<br>
       chip size or a decrease in process size."",
       caption = ""**Data:** Wikipedia | **Graphic:** @jakekaupp"") +
  theme_jk(grid = ""XY"",
          markdown = TRUE) +
  theme(strip.text = element_blank())

ggsave(here(""2019"", ""week36"", ""tw36_plot.png""), plot = plot, device = agg_png(), width = 10, height = 6)
",2019,36
104,104,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week37/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(tidygraph)
library(ggraph)
library(ggforce)
library(janitor)
library(jkmisc)
library(glue)
library(ggtext)
library(colorspace)
library(ragg)

legacy_data <- here(""2019"", ""week37"", ""data"", ""Saferparks-dataset-legacy.csv"") %>% 
  read_csv() %>% 
  mutate(year = year(mdy(acc_date))) %>% 
  filter(between(year, 1999, 2007))

device_type <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, device_type, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ lighten(""#251351"", .75),
                           industry_sector == ""recreation"" ~ lighten(""#7d2e68"", .75),
                           industry_sector == ""water park"" ~ lighten(""#41658a"", .75))) %>% 
  select(name = device_type, size, color)

device_category <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ lighten(""#251351"", .25),
                           industry_sector == ""recreation"" ~ lighten(""#7d2e68"", .25),
                           industry_sector == ""water park"" ~ lighten(""#41658a"", .25))) %>% 
  select(name = device_category, size, color) 

sector <-  legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, name = ""size"") %>% 
  mutate(color = case_when(industry_sector == ""amusement ride"" ~ ""#251351"",
                           industry_sector == ""recreation"" ~ ""#7d2e68"",
                           industry_sector == ""water park"" ~""#41658a"")) %>% 
  select(name = industry_sector, size, color)

nodes <- bind_rows(sector, device_category, device_type)

edge_one <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>%
  select(industry_sector, device_category) %>% 
  mutate_all(~as.numeric(factor(., nodes$name))) %>% 
  set_names(c(""from"", ""to""))

edge_two <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>%
  select(device_category, device_type) %>% 
  mutate_all(~as.numeric(factor(., nodes$name))) %>% 
  set_names(c(""from"", ""to""))

edges <- bind_rows(edge_one, edge_two)

graph <- tbl_graph(nodes = nodes, edges = edges) 

labels <- legacy_data %>% 
  filter(industry_sector != ""unknown"") %>% 
  count(industry_sector, device_category, device_type, name = ""size"") %>% 
  group_by(industry_sector) %>% 
  top_n(1 , size) %>% 
  filter(size > 1) %>% 
  pull(device_type) 
 
text_bc <- function(text, color) {
  
  glue(""<span style = color:{color}>**{text}**</span>"")
  
}


plot <- ggraph(graph, 'circlepack', weight = size) + 
  geom_node_circle(aes(fill = color)) + 
  geom_node_text(aes(label = glue(""{str_remove(name, ' - undefined')}:\n{size}""), filter = name %in% labels, family = ""Oswald"")) +
  scale_fill_identity() +
  labs(x = NULL,
       y = NULL,
       title = ""Attractions With The Most Reported Injuries from 1999-2007"",
       caption = ""Data: **SaferParks** | Graphic: **@jakekaupp**"",
       subtitle = glue(""Shown below is a packed circle representation of reported accidents in the SaferParks database from 1999-2007.<br>Circles are organized by {highlight_text('Amusement rides', '#251351', 'b')}, {highlight_text('Recreation', '#7d2e68', 'b')} and {highlight_text('Water Park', '#41658a', 'b')}. Device category and device type are the<br>middle and lightest hues, respectively."")) +
  theme_jk(grid = FALSE) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        plot.subtitle = element_markdown(),
        plot.caption = element_markdown()) 


ggsave(here(""2019"", ""week37"", ""tw_37plot.png""), plot, width = 9, height = 10, dev = agg_png())


  ",2019,37
105,105,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week38/R/analysis.R,"library(tidyverse)
library(rvest)
library(janitor)
library(here)
library(fuzzyjoin)
library(jkmisc)
library(ragg)

# Get park fees
fees_page <- ""https://www.nps.gov/aboutus/entrance-fee-prices.htm""

parks <- read_html(fees_page) %>% 
  html_nodes(""h3"") %>% 
  html_text() %>% 
  .[-1:-2]

park_fees <- read_html(fees_page) %>% 
  html_nodes("".table-wrapper > table"") %>% 
  html_table() %>% 
  map(~set_names(.x, c(""date"", ""park_specific_annual_pass"", ""per_vehicle"", ""per_person"", 
                       ""per_motorcycle""))) %>% 
  map2(parks, ~mutate(.x, park = .y)) %>% 
  bind_rows() %>% 
  filter(date == ""Current"") %>% 
  rename(park_name = park) %>% 
  mutate(park_name = stringi::stri_trans_general(park_name, id = ""Latin-ASCII""),
         park_name = str_replace(park_name, ""Hawai'i"", ""Hawaii""))



#udpated data
summary_report <- here(""2019"", ""week38"", ""data"", ""annual_summary_report.csv"") %>% 
  read_csv() %>% 
  clean_names()

plot_data <- summary_report %>% 
  filter(year == 2018) %>% 
  mutate(visitors = recreation_visitors + non_recreation_visitors) %>% 
  select(year, park_name, visitors) %>% 
  mutate(park_name = str_remove(park_name, ""[A-Z]{2,}""),
         park_name = str_remove(park_name, ""& PRES""),
         park_name = trimws(park_name)) %>% 
  regex_left_join(park_fees, ., ignore_case = TRUE) %>% 
  distinct(year, park_name.x, .keep_all = TRUE) %>% 
  filter(str_detect(park_name.x, ""Park""), !str_detect(park_name.x, ""Great Falls"")) %>% 
  mutate(revenue = visitors * parse_number(per_person)) %>% 
  rename(park_name = park_name.x) %>% 
  select(-park_name.y)
  

plot <- ggplot(plot_data, aes(x = fct_reorder(park_name, revenue), y = revenue)) +
  geom_col(fill = ""#5e81ac"", size = 0.1) +
  coord_flip() +
  scale_y_continuous(labels = scales::dollar, expand = c(0.01,0)) +
  labs(title = ""Estimated National Park Revenue from Fees for 2018"",
       subtitle = str_wrap(""Illustrated below is a bar chart of fee revenue from US National Parks in 2018.  Estimated Revenue calculated using per person admittance rates and total park visitors."", 95),
       caption = ""Data: www.nps.gov | Graphic: @jakekaupp"",
       x = NULL,
       y = NULL) +
  theme_jk(grid = ""X"") +
  theme(plot.background = element_rect(fill = ""#2e3440""),
        text = element_text(color = ""#eceff4""),
        panel.grid = element_line(color = ""#e5e9f0""),
        axis.text.x = element_text(color = ""#eceff4""),
        axis.text.y = element_text(color = ""#eceff4""))

ggsave(here(""2019"", ""week38"", ""tw_38plot.png""), plot, width = 10, height = 8, device = agg_png())

",2019,38
106,106,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week39/R/analysis.R,"library(tidyverse)
library(janitor)
library(tidycensus)
library(glue)
library(here)
library(sf)
library(tigris)
library(jkmisc)
library(ggtext)

school_diversity <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-24/school_diversity.csv"")

## Getting the ACS Survey data ----
acs_var <- load_variables(2017, ""acs1"", cache = TRUE)

race_vars <- filter(acs_var, concept == ""RACE"") %>% 
  select(name, label) %>% 
  separate(label, c(""estimate"", ""total"", ""type""), sep = ""!!"") %>% 
  mutate(type = coalesce(type, total)) %>% 
  select(name, label = type)

if (!file.exists(here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))) {
  
  acs_race <- map_df(state.abb, ~get_acs(geography = ""school district (unified)"", 
                                         variables = race_vars$name,
                                         state = .x))
  
  saveRDS(acs_race, here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))
  
} else {
  
  acs_race <- readRDS(here(""2019"", ""week39"", ""data"", ""acs_race.RDS""))
}





# Recoding ACS and aggregating data, sadly not easy to determine Hispanic origin ----
# Following methodology from WaPo repo, recoding Native Hawaiian and Pacifici Islander into Asian.
diversity_data <- acs_race %>% 
  left_join(race_vars, by = c(""variable"" = ""name"")) %>% 
  mutate(label = case_when(label == ""White alone"" ~ ""White"",
                           label == ""Black or African American alone"" ~ ""Black"",
                           label == ""American Indian and Alaska Native alone"" ~ ""AIAN"",
                           label == ""Native Hawaiian and Other Pacific Islander alone"" ~ ""Asian"",
                           label == ""Asian alone"" ~ ""Asian"",
                           label == ""Two or more races"" ~ ""Multi"",
                           label == ""Some other race alone"" ~ ""Other"",
                           TRUE ~ label)) %>% 
  group_by(GEOID, NAME, label) %>% 
  summarize_at(vars(estimate), sum) 


# Using Simpson's Diversity Index instead of max race metrics for diversity----
totals <- diversity_data %>% 
  summarize(total = sum(estimate)*(sum(estimate)-1))

dvs_score <- diversity_data %>% 
  filter(label != ""Total"") %>% 
  mutate(es_minus = estimate-1) %>% 
  summarize(numerator = sum(estimate*es_minus)) %>% 
  left_join(totals) %>% 
  mutate(diversity = 1 - numerator/total) %>% 
  select(GEOID, NAME, diversity) 
  
acs_diversity <- diversity_data %>% 
  spread(label, estimate) %>% 
  select(-Total) %>% 
  left_join(dvs_score) %>% 
  rename(acs_diversity = diversity) %>% 
  ungroup() %>% 
  mutate(NAME = tolower(NAME),
         NAME = str_remove(NAME, ""\\(.+\\)""),
         NAME = str_replace_all(NAME, "";"", "","")) %>% 
  separate(NAME, c(""NAME"", ""state""), sep = "","") %>% 
  mutate(NAME = str_remove_all(NAME, ""school district*+"")) %>% 
  select(GEOID, acs_diversity)

# Use WaPo data and calculate Simpson's Diversity Index
upd_school <- school_diversity %>% 
  filter(SCHOOL_YEAR == ""2016-2017"") %>% 
  select(LEAID, LEA_NAME, ST, SCHOOL_YEAR, AIAN:Total) %>% 
  pivot_longer(AIAN:Multi, ""race"", ""value"") %>% 
  mutate(n = floor(Total * value))

school_totals <- upd_school %>% 
  group_by(LEAID, LEA_NAME, ST, SCHOOL_YEAR) %>% 
  summarize(total = sum(n)*(sum(n)-1))
  
upd_school_dvs <- upd_school %>% 
  group_by(LEAID, LEA_NAME, ST, SCHOOL_YEAR) %>% 
  mutate(n_minus = n-1) %>% 
  summarize(numerator = sum(n*n_minus)) %>% 
  left_join(school_totals) %>% 
  mutate(diversity = 1 - numerator/total) %>% 
  select(GEOID = LEAID, NAME = LEA_NAME, ST, school_diversity = diversity) 

# Environment Cleanup---
remove(list = ls()[str_which(ls(), ""upd_school_dvs|acs_diversity"", negate = TRUE)])

# Comparing the two diversity measures and creating the ratio ----
overall_diversity <- upd_school_dvs %>% 
  left_join(acs_diversity, by = ""GEOID"") %>% 
  mutate(ratio = school_diversity/acs_diversity) %>% 
  filter(!is.na(acs_diversity))

# Get School District maps ----

if (!file.exists(here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))) {
  
  district_maps <- map(state.abb, ~school_districts(.x, class = ""sf""))
  
  saveRDS(district_maps, here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))
  
} else {
  
  district_maps <- readRDS(here(""2019"", ""week39"", ""data"", ""district_maps.RDS""))
}



# One do.call to keep them all, and in the shallows bind them----
maps <- do.call(sf:::rbind.sf, district_maps)

plot <- overall_diversity %>%
  right_join(maps, ., by = ""GEOID"") %>%
  filter(!ST %in% c(""HI"", ""AK"")) %>%
  ggplot() +
  geom_sf(aes(fill = ratio), color = 'white', size = 0.01) +
  scale_fill_viridis_c(""Alignment Ratio"", option = ""cividis"", limits = c(0, 1), labels = scales::percent, na.value = ""white"") +
  coord_sf(crs = 26915) +
  labs(title = ""Is Diversity In School Districts Reflected In The Diversity Of The General Population?"",
       subtitle = glue(""Shown below is a choropleth map illustrating the ratio between the Diversity Index of a School Population and the Diversity Index of the General Population in that School District in 2017.<br>
       The more {highlight_text('yellow', '#FFEA46', 'b')} an area, the greater alignment between diversity indices.  The more {highlight_text('blue', '#00204D', 'b')} an area, the greater the difference between the diversity of the school and the general populace.<br> 
       This analysis focused on unified school districts and available data on race from the ACS Survey.  Diversity was calculated using Simpson's Diversity Index.""),
       caption = ""Data: **Washington Post via @dataKateR & American Community Survey** | Graphic: **@jakekaupp**"") +
  theme_jk(grid = FALSE,
          markdown = TRUE) +
  theme(axis.text = element_blank())

ggsave(here(""2019"", ""week39"", ""tw39_plot.png""), width = 16, height = 10, device = ragg::agg_png())
",2019,39
107,107,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week4/R/analysis.R,"library(tidyverse)
library(readxl)
library(here)
library(janitor)
library(albersusa)
library(sf)
library(sp)
library(rgeos)
library(maptools)
library(ggthemes)
library(viridis)
library(scales)
library(glue)
library(jkmisc)



incar_data <- here(""2019"",""week4"",""data"") %>% 
  dir(full.names = TRUE, pattern = ""incarceration"") %>% 
  read_excel()

fix_null <- function(x) if_else(is.nan(x), NA_real_, x)

# Mapping things
us <- counties_composite()
us_map <- fortify(us, region=""fips"") %>% 
  mutate_at(""id"", as.numeric)

ratio_data <- incar_data %>% 
  group_by(year, fips, state, county_name) %>% 
  transmute(black_pop_ratio = black_pop_15to64/total_pop_15to64,
         black_prison_ratio = black_prison_pop/total_prison_pop,
         asian_pop_ratio = asian_pop_15to64/total_pop_15to64,
         asian_prison_ratio = asian_prison_pop/total_prison_pop,
         latino_pop_ratio = latino_pop_15to64/total_pop_15to64,
         latino_prison_ratio = latino_prison_pop/total_prison_pop,
         native_pop_ratio = native_pop_15to64/total_pop_15to64,
         native_prison_ratio = native_prison_pop/total_prison_pop,
         white_pop_ratio = white_pop_15to64/total_pop_15to64,
         white_prison_ratio = white_prison_pop/total_prison_pop) %>% 
  group_by(fips, state, county_name) %>% 
  summarize_at(vars(contains(""ratio"")), mean, na.rm = TRUE) %>% 
  mutate_at(vars(contains(""ratio"")), fix_null)


map_data <- left_join(us_map, ratio_data, by = c(""id"" = ""fips""))  %>% 
  ungroup() %>% 
  gather(""variable"",""percentage"", contains(""ratio"")) %>% 
  separate(variable, c(""ethnicity"", ""category""), sep = ""_"")

plot <- ggplot() +
  geom_map(data = us_map, map = us_map,
           aes(x = long, y = lat, map_id = id),
           color =""#2b2b2b"", size = 0.05, fill = NA) +
  geom_map(data = map_data, map = us_map,
           aes(x = long, y = lat, map_id = id, fill = percentage),
           color =""#2b2b2b"", size = 0.05) + 
  scale_fill_viridis_c("""", na.value = ""white"", option = 'cividis', labels = scales::percent) +
  coord_map() +
  labs(title = ""Differences between the General and Prison Population by County and Ethnic Group from 1970 to 2016"",
       subtitle = str_wrap(""Non-white and non-Asian ethnic groups in the South-Eastern United States have a higher average representation in prison than in the overall population.  Missing data indicated by no fill color."",  90),
       caption = ""Data: Vera Institute of Justice | Graphic: @jakekaupp"") +
  facet_grid(category ~ ethnicity , labeller = labeller(category = c(""pop"" = ""Total\nPopulation"", ""prison"" = ""Prison\nPopulation""),
                                                       ethnicity = str_to_title)) +
  theme_map(base_family = ""Scope One"", 
            base_size = 16) +
  theme(plot.caption = element_text(size = 10),
        plot.title = element_text(family = ""Oswald""),
        legend.background = element_rect(fill = NA),
        legend.text = element_text(size = 10),
        legend.title = element_text(size = 12),
        legend.position = ""bottom"",
        legend.justification = ""center"",
        legend.direction = ""horizontal"",
        legend.key.height = unit(0.2, ""cm""),
        legend.key.width = unit(1, ""cm""),
        strip.background = element_blank(),
        strip.text.y = element_text(angle = 0))

ggsave(here(""2019"",""week4"",""tw4_choro.png""), width = 11, height = 5)

",2019,4
108,108,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week40/R/analysis.R,"library(tidyverse)
library(sf)
library(tigris)
library(glue)
library(colorspace)
library(jkmisc)
library(ggforce)
library(ragg)
library(here)

# Get TidyTuesday data
pizza_datafiniti <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv"") %>% 
  filter(province == ""NY"") %>% 
  distinct(name, latitude, longitude)

pizza_barstool <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv"") %>% 
  distinct(name, latitude, longitude) %>% 
  filter(!is.na(latitude))

# Get all New York County road maps
counties <- c(""New York County"", ""Kings County"", ""Bronx County"", ""Richmond County"",  ""Queens County"")

roads_data <- map(counties, ~roads(""NY"", .x, class = ""sf"")) %>% 
  do.call(sf:::rbind.sf, .)

# Build plot colors as a named vector and as a tibble
plotcolors <- c('Other' = '#cccccc',
                'Ave' = '#59c8e5',
                'St' = '#fed032',
                'Tunl' = '#fed032',
                'Brg' = '#fed032',
                'N' = '#fed032',
                'S' = '#fed032',
                'E' = '#fed032',
                'W' = '#fed032',
                'Rd' = '#4cb580',
                'Dr' = '#0a7abf', 
                'Hwy' = '#ff9223', 
                'Plz' = '#ff9223',
                'Viaduct' = '#ff9223', 
                'Expy' = '#ff9223', 
                'Pkwy' = '#ff9223',
                'Thruway' = '#ff9223',
                'State Hwy' = '#ff9223',
                'State' = '#ff9223',
                'US Hwy' = '#ff9223',
                'Blvd'= '#2e968c')

pc_tibble <- tibble(street_type = names(plotcolors),
                    color = plotcolors)

# Assign street types to roads
roads <- roads_data %>% 
  filter(!is.na(RTTYP)) %>% 
  mutate(street_type = map_chr(FULLNAME, ~first(names(plotcolors)[str_which(.x, glue(""{names(plotcolors)}\\b""))]))) %>% 
  mutate(street_type = if_else(str_detect(FULLNAME, ""I-""), 'I-', street_type)) %>% 
  mutate(street_type = case_when(is.na(street_type) & MTFCC == ""S1100"" ~ 'Expy',
                                 is.na(street_type) & MTFCC == ""S1200"" ~ 'St',
                                 is.na(street_type) & !MTFCC %in% c(""S1100"", ""S1200"") ~ ""Other"",
                                 TRUE ~ street_type)) %>% 
  left_join(pc_tibble, by = ""street_type"")

# Get Counties shapefiles to determine which pizza places are in the areas I want
counties_sf <- counties(""NY"", class = ""sf"") %>% 
  filter(NAMELSAD %in% counties)

# Use st_intersects and filter to remove out of bounds pizza places
pizza_sf_df <- st_as_sf(pizza_datafiniti, coords = c(""longitude"", ""latitude""), crs = st_crs(roads)) 
pizza_sf_bs <- st_as_sf(pizza_barstool, coords = c(""longitude"", ""latitude""), crs = st_crs(roads)) 
  
  
# Filter to pizza places in the five boroughs
ny_pizza_df <-  filter(pizza_sf_df, map_lgl(st_intersects(pizza_sf_df, counties_sf), ~!is_empty(.x)))

ny_pizza_bs <-  filter(pizza_sf_bs, map_lgl(st_intersects(pizza_sf_bs, counties_sf), ~!is_empty(.x)))

ny_pizza <- sf:::rbind.sf(ny_pizza_bs, ny_pizza_df) %>% 
  distinct(geometry)

# Construct the color legend
legend <- pc_tibble %>% 
  filter(street_type %in% c(""Other"",""Ave"",""St"", ""Rd"", ""Dr"", ""Hwy"", ""Blvd"")) %>% 
  mutate(street_type = factor(street_type, levels = c(""Other"", ""Ave"", ""Dr"", ""Rd"", ""Blvd"", ""St"", ""Hwy""), labels = c(""Other"", ""Avenue"", ""Drive"", ""Road"", ""Boulevard "", ""Street"", ""Highway""))) %>%
  arrange(street_type) %>% 
  mutate(x0 = seq(3, by = 4.5, length.out = 7),
         r = 1.75,
         y0 = 0) %>% 
  ggplot(aes(x0 = x0, y0 = y0, r = r)) +
  geom_circle(aes(fill = color, color = darken(color))) +
  geom_text(aes(label = street_type, x = x0, y = 0), family = ""Lora"", size = 3) +
  annotate(""text"", family = ""Oswald"", x = -2, y = 0, label = ""Legend"", size = 6) +
  scale_fill_identity() +
  scale_color_identity() +
  expand_limits(y = c(-0.5, 4),
                x = c(-4, 24)) +
  labs(x = NULL,
       y = NULL) +
  coord_equal(clip = ""off"") +
  theme_jk(grid = FALSE, plot_title_size = 30) +
  theme(panel.grid.major = element_line(colour = ""transparent""),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()) 

legend_grob <- ggplotGrob(legend)

subtitle <- ""Shown on this map are the roads and the pizza places of the Five Boroughs of New York City.  
Pizza places are distinct locations almagamated from the DataFiniti and Barstool datasets, and a represented by purple dots.  Manhattan is the most represented borough in the dataset, unsurprising given the relative population, and it being the home of the Teenage Mutant Ninja Turtles.
The map style of plotting the colored roads were inspired by Erin Davis (erdavis1 on github), and her series of circular maps of World Cities.""

caption <- ""Data: DataFiniti, Barstool, US Census Shapefiles\nGraphic: @jakekaupp""


# Plot the Street maps and Pizza place data
pizza_map <- ggplot() +
  geom_sf(data = filter(roads, street_type != ""Other""), aes(color = color), size = 0.25) + 
  geom_sf(data = filter(roads, street_type == ""Other""), aes(color = color), size = 0.35) + 
  geom_sf(data = ny_pizza, color = darken(""#963484""), fill = ""#963484"", shape = 21, size = 2, alpha = 0.5) +
  annotate(""text"", label = ""Pizza Places of the Five Boroughs"", family = ""Oswald"", x = -74.3, y = 40.91, size = 6, hjust = 0) +
  annotate(""text"", family = ""Lato"", label = str_wrap(subtitle, 60), x = -74.3, y = 40.89, hjust = 0, vjust = 1) +
  annotate(""text"", family = ""Lato"", label = caption, x = -74.3, y = 40.78, hjust = 0, vjust = 1) +
  annotation_custom(legend_grob, xmin = -74.2, xmax = Inf, ymin = 40.49, ymax = 40.55) +
  scale_color_identity() +
  scale_size_identity() +
  coord_sf(clip = ""off"") +
  labs(x = NULL, y = NULL) +
  theme_jk(grid = FALSE) +
  theme(panel.grid.major = element_line(colour = ""transparent""),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()) 


ggsave(here('2019', 'week40', 'tw40_plot.png'), width = 12, height = 9, dev = agg_png())",2019,40
109,109,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week41/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(janitor)
library(jkmisc)
library(ggforce)
library(ggtext)
library(nord)
library(glue)
library(ragg)

# It's 250mb, can't put it into git, you're going to have to go get it
# from https://openpowerlifting.org/data and stick it in data.
pl_data <- here(""2019"", ""week41"", ""data"") %>% 
  dir(pattern = ""openpowerlifting"", full.names = TRUE) %>% 
  read_csv() %>% 
  clean_names()

plot_data <- pl_data %>%
  filter_at(vars(starts_with(""best"")), all_vars(. > 0)) %>% 
  filter(str_detect(place, ""1"")) %>% 
  mutate(year = year(date)) %>%
  select(-date) %>%
  mutate_at(vars(starts_with(""best"")), ~./bodyweight_kg) %>% 
  pivot_longer(starts_with(""best""), names_to = ""lift"") %>% 
  group_by(year, sex, lift) %>%
  filter(value == max(value, na.rm = TRUE)) %>% 
  arrange(lift, sex, year)

labels <- tibble(label = c(""Bench Press"", ""Deadlift"", ""Squat""),
                 lift = c(""best3bench_kg"", ""best3deadlift_kg"", ""best3squat_kg""),
                 year = 2020,
                 value = 1)

annotations <- plot_data %>% 
  group_by(sex, lift) %>% 
  filter(value == max(value, na.rm = TRUE)) %>% 
  mutate(description = glue(""Weight: {bodyweight_kg} kg\nLifted: {bodyweight_kg*value} kg\n{federation}: {meet_name}""),
         name = str_remove(name, ""\\#[0-9]""))

plot <- ggplot(plot_data, aes(x = year, y = value)) +
  geom_path(aes(color = sex)) +
  geom_point(aes(fill = sex), shape = 21, color = ""#2E3440"") +
  geom_text(data = labels, aes(label = label), color = ""#E5E9F0"", family = ""Oswald"", fontface = ""bold"", size = 10, hjust = 1) +
  geom_mark_circle(data = filter(annotations, sex == ""M""), aes(color = sex, label = name, description = description), expand = unit(2, ""mm""), label.family = c(""Oswald"", ""Lato""), label.fill = ""#4C566A"", label.colour = ""#E5E9F0"", con.colour = ""#D8DEE9"", label.margin = margin(2, 3, 2, 3, ""mm"")) +
  geom_mark_circle(data = filter(annotations, sex == ""F""), aes(color = sex, label = name, description = description), expand = unit(2, ""mm""), label.family = c(""Oswald"", ""Lato""), label.fill = ""#4C566A"", label.colour = ""#E5E9F0"", con.colour = ""#D8DEE9"", label.margin = margin(2, 3, 2, 3, ""mm"")) +
  facet_wrap(~lift) +
  scale_color_manual(values = set_names(c(""#314cb6"",""#DD2A7B""), c(""M"",""F""))) +
  scale_fill_manual(values = set_names(c(""#314cb6"",""#DD2A7B""), c(""M"",""F""))) +
  scale_x_continuous(breaks = seq(1970, 2020, 10)) +
  theme_jk(dark = TRUE, 
           grid = ""XY"",
           markdown = TRUE) +
  labs(x = NULL,
       y = NULL,
       title = ""Evolution of Power: How the Ratio of Bodyweight to Lifted Weight Has Progressed"",
       subtitle = glue(""Illustrated below is the maximum of the ratio of bodyweight to lifted weight for winning lifts in each year and event for both {highlight_text('Men', '#314cb6', 'b')} and {highlight_text('Women', '#DD2A7B', 'b')} for all meets recorded by Open Powerlifting.""),
       caption = ""Data: **openpowerlifting.org** | Graphic: **@jakekaupp**"") +
  theme(legend.position = ""none"",
        panel.grid.major = element_line(size = 0.01),
        strip.text = element_blank())

ggsave(here('2019', 'week41', 'tw41_plot.png'), plot, width = 15, height = 8, device = agg_png())",2019,41
110,110,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week42/R/analysis.R,"library(tidyverse)
library(here)
library(janitor)
library(jkmisc)
library(ggalt)
library(ggtext)
library(glue)
library(ragg)

big_epa_cars <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-15/big_epa_cars.csv"")

top <- big_epa_cars %>% 
  clean_names() %>% 
  count(make, year) %>% 
  count(make) %>% 
  filter(n == 37)

plot_data <- big_epa_cars %>% 
  clean_names() %>% 
  select(make, v_class, year, you_save_spend) %>% 
  semi_join(top) %>% 
  group_by(year, make) %>% 
  summarize(total_save_spend = mean(you_save_spend)) %>%
  group_by(year) %>% 
  mutate(rank = min_rank(desc(total_save_spend))) %>% 
  ungroup() %>% 
  mutate(size = if_else(make == ""Ford"", 1, 0.5),
         make = factor(make, pull(top, make)),
         make = fct_relevel(make, ""Ford"", after = Inf),
         make = fct_recode(make, ""**Ford**"" = ""Ford""))


grid <- tibble(rank = 1:22)

colors <- set_names(grey.colors(22), pull(top, make) %>%
                      factor() %>%
                      fct_recode(""**Ford**"" = ""Ford""))

colors[[""**Ford**""]] <- ""#DD2A7B""


plot <- ggplot(plot_data, aes(x = year, y = rank)) +
  geom_segment(data = grid, aes(x = 1983, xend = 2021, y = rank, yend = rank), color = ""#cccccc"", alpha = 0.5, size = 0.1) +
  geom_xspline(aes(color = make, size = size), show.legend = FALSE) +
  geom_point(aes(fill = make), shape = 21, color = ""white"", show.legend = FALSE) +
  geom_richtext(data = filter(plot_data, year == 2020), aes(label = as.character(make), x = 2021, color = make), hjust = 0, family = ""Lora"", size = 4, show.legend = FALSE,  fill = NA, label.color = NA, 
                label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_text(data = filter(plot_data, year == 1984), aes(label = rank, x = 1983), hjust = 1, family = ""Oswald"", size = 4) +
  labs(x = NULL,
       y = NULL,
       title = ""From Chugging to Sipping: Fuel Cost Savings of Major Automakers since 1984"",
       subtitle = glue(""Shown below is a rankings chart of average fuel cost savings, measured over 5 years, from 1984 to 2020.  {highlight_text('Ford','#DD2A7B', 'b')} has had quite the journey, battling from the bottom<br>of the list to the second-best North American manufacturer."")) +
  scale_color_manual(values = colors) +
  scale_fill_manual(values = colors) +
  scale_size_identity() +
  scale_x_continuous(breaks = 1984:2020) +
  scale_y_continuous(trans = ""reverse"", breaks = NULL) +
  expand_limits(x = 2025) +
  theme_jk(grid = ""X"", 
           markdown = TRUE)

ggsave(here(""2019"", ""week42"", ""tw42_plot.png""), plot = plot, width = 13, height = 6)
",2019,42
111,111,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week43/R/analysis.R,"library(tidyverse)
library(lubridate)
library(here)
library(ggraph)
library(tidygraph)
library(glue)
library(jkmisc)
library(colorspace)
library(ggforce)
library(ggtext)

horror_movies <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv"")

movie_cast <- distinct(horror_movies, title, release_date, review_rating, .keep_all = TRUE) %>% 
  mutate(year = str_extract(title, ""\\((\\d{4})\\)""),
         year = parse_number(year),
         title = str_remove(title, ""(\\s\\(\\d{4}\\))""),
         date = dmy(release_date)) %>% 
  arrange(title) %>% 
  separate_rows(cast, sep = ""\\|"") %>% 
  mutate(cast = trimws(cast)) %>% 
  select(title, year, review_rating, cast)

cast_df <- left_join(movie_cast, movie_cast, by = c(""title"", ""year"", ""review_rating"")) %>% 
  rename(from = cast.x,
         to = cast.y) %>% 
  filter(from != to) 

nodes <- cast_df %>% 
  group_by(from) %>% 
  summarize(node_size = n_distinct(title)) %>% 
  distinct(from, .keep_all = TRUE) 

focus <- ""Eric Roberts""

edges <- cast_df %>% 
  count(from, to, sort = TRUE, name = ""edge_size"") %>% 
  distinct(from, to, .keep_all = TRUE) %>% 
  mutate(color = if_else(from == focus | to == focus, ""#bb0a1e"", ""#373e40""),
         alpha = if_else(from == focus | to == focus, 1, 0.2),
         size = if_else(from == focus | to == focus, 1, 0.1))

connected <- filter(edges, from == focus) %>% 
  distinct() %>% 
  pull(to) 

cast_network <- tbl_graph(nodes = nodes, edges = edges, directed = FALSE) %>% 
  activate(nodes) %>% 
  mutate(degree = centrality_eigen(),
         alpha = if_else(from %in% c(focus, connected),  1, 0.2)) %>% 
  top_n(500, degree) %>% 
  mutate(fill = if_else(from %in% c(focus, connected), ""#bb0a1e"", ""#373e40""),
         color = if_else(from %in% c(focus, connected), darken(""#bb0a1e""), darken(""#373e40"")))

plot <- ggraph(cast_network, layout = ""graphopt"") + 
  geom_edge_link(aes(alpha = stat(index), edge_colour = color, edge_width = size), show.legend = FALSE) + 
  geom_node_point(aes(size = node_size, fill = fill, color = color), shape = 21, show.legend = FALSE) +
  #geom_mark_circle(aes(x, y, filter = from == focus, label = from, description = ""Legendary B-Movie Actor""), expand = unit(0, ""mm""), label.family = c(""Oswald"", ""Lora"")) +
  scale_edge_color_identity() +
  scale_alpha_identity() +
  scale_fill_identity() +
  scale_edge_width_identity() +
  scale_color_identity() +
  labs(x = NULL,
       y = NULL,
       title = ""Horror Movie Co-Star Networks of the Top 500 Prolific Performers"",
       subtitle = glue(""The reach of B-movie legend {highlight_text('Eric Roberts', '#bb0a1e', 'b')} is featured below across his 27 films. Prolific performers determined by the top 500<br>actors by eigenvalue centrality.""),
       caption = ""Data: **IMDB** | Graphic: **@jakekaupp**"") +
  theme_jk(grid = FALSE,
           markdown = TRUE) +
  theme(axis.text = element_blank())

ggsave(here(""2019"", ""week43"", ""tw43_plot.png""), plot, width = 10, height = 6, device = ragg::agg_png())

  
",2019,43
112,112,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week44/R/analysis.R,"library(tidyverse)
library(waffle)
library(lubridate)
library(jkmisc)
library(scales)
library(colorspace)
library(patchwork)
library(ggtext)
library(glue)
library(here)

#Get the data ----
nyc_squirrels <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-29/nyc_squirrels.csv"")

# Process the data ----
activity_data <- nyc_squirrels %>% 
  mutate(date = mdy(date)) %>% 
  pivot_longer(names_to = ""activity"", running:foraging) %>%
  group_by(date, activity) %>% 
  summarize(value = sum(as.numeric(value))) %>% 
  mutate(activity = factor(activity, labels = unique(activity)),
         activity = fct_reorder(activity, value, .fun = sum)) %>% 
  arrange(date, desc(activity))

# Make my palette

slate_ramp <- colorRampPalette(c(""#3B454A"", lighten(""#3B454A"", 0.8)))(5) 

grey_ramp <- grey.colors(5, 0.5, 0.9)

pal <- set_names(slate_ramp, unique(activity_data$activity))

pal[""foraging""] <- ""#DD2A7B""

partition_waffle <- function(x, start, nrows, flip = FALSE) {
  
   offset <- start - x
  
   offset_rows <- offset %/% nrows
   
   offset_blocks <- offset %% nrows
   
  
   comp_blocks <- nrows - offset_blocks
   
   if (comp_blocks != nrows) {
     
     rows <- (x - comp_blocks) %/% nrows
     
     blocks <- (x - comp_blocks) %% nrows
     
     start_row <- offset_rows + rows + 1
     
     if (blocks == 0) {
       
       end_row <- start_row
       
     } else {
       
       end_row <- start_row + 1 
       
     }
     
   } else {
     
     rows <- x %/% nrows
     
     blocks <- x %% nrows
     
     start_row <- rows + offset_rows
     
     end_row <- rows + offset_rows + 1
     
   }
   
   
   if (flip) {
     
     tibble(y = c(start_row, start_row, end_row),
            yend = c(start_row, end_row, end_row),
            x = c(blocks, blocks, 0),
            xend = c(nrows, blocks, blocks))
     
     
   } else {
     
     tibble(x = c(start_row, start_row, end_row),
            xend = c(start_row, end_row, end_row),
            y = c(blocks, blocks, 0 -1),
            yend = c(nrows + 1, blocks, blocks))
   }
     
     

   
  
} 

# Waffles by activity ----

activity_outlines <- activity_data %>% 
  ungroup() %>% 
  arrange(desc(activity), date) %>% 
  mutate(cum_sum = cumsum(value)) %>% 
  mutate(lines = map2(value, cum_sum, partition_waffle, flip = FALSE, nrows = 20)) %>% 
  unnest(lines) %>% 
  filter(date == last(date))

activity_labels <- activity_data %>% 
  group_by(activity) %>% 
  summarize(total = sum(value)) %>%
  left_join(filter(activity_outlines, yend == 21), by = ""activity"")

waffle_by_activity <- activity_data %>% 
  arrange(desc(activity), date) %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = activity_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""black"", size = 0.5) +
  geom_richtext(data = activity_labels, aes(label = glue(""<b>{str_to_title(activity)}</b>""), x = x, y = yend, color = activity), fill = ""white"", hjust = 1, vjust = 0, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_richtext(data = activity_labels, aes(label = glue(""{total}""), x = x + 1, y = -1), fill = ""white"", color = ""black"", hjust = 0, vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  coord_equal() +
  labs(x = NULL, 
       y = NULL) +
  theme_jk(grid = FALSE) +
  scale_x_continuous(expand = c(0,0)) +
  expand_limits(y = c(-5, 25), x = c(0, 200)) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

# Waffles by day ----

by_day_outlines <- activity_data %>% 
  ungroup() %>% 
  mutate(cum_sum = cumsum(value)) %>% 
  mutate(lines = map2(value, cum_sum, partition_waffle, nrows = 20, flip = FALSE)) %>% 
  unnest(lines) %>% 
  filter(activity == ""chasing"")

by_day_labels <- activity_data %>% 
  group_by(date) %>% 
  summarize(total = sum(value)) %>% 
  left_join(filter(by_day_outlines, yend == 21))

waffle_by_day <- activity_data %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = by_day_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""grey20"", size = 0.5) +
  geom_richtext(data = by_day_labels, aes(label = glue(""{month.abb[month(date)]} {day(date)}""), x = x, y = yend), fill = ""white"", color = ""black"", hjust = c(rep(1,10), 0), vjust = 0, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  geom_richtext(data = by_day_labels, aes(label = glue(""{total}""), x = x, y = -1), fill = ""white"", color = ""black"", hjust = c(rep(0,9),1,0), vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  scale_x_continuous(expand = c(0,0)) +
  coord_equal(clip = ""off"") +
  labs(x = NULL, 
       y = NULL) +
  expand_limits(y = c(-5, 25), x = c(0, 205)) +
  theme_jk(grid = FALSE) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

# Waffle bars by day ----

waffle_day_outlines  <- activity_data %>% 
  arrange(desc(date)) %>% 
  filter(activity == ""foraging"") %>% 
  ungroup() %>%
  group_split(date) %>% 
  map(~mutate(.x, cum_sum = cumsum(value))) %>%
  map_dfr(~mutate(.x, lines = map2(value, cum_sum, partition_waffle, nrows = 20, flip = FALSE))) %>%
  unnest(lines) 

waffle_labels <- waffle_day_outlines %>% 
  filter(y == -1)

mday_label <- function(x) {
  
  glue(""{month.abb[month(x)]} {day(x)}"")
  
}

split_waffle_by_day <- activity_data %>% 
  arrange(desc(date)) %>% 
  ggplot(aes(fill = activity, values = value)) +
  geom_waffle(color = ""white"", size = 0.25, n_rows = 20, flip = FALSE, show.legend = FALSE) +
  geom_segment(data = waffle_day_outlines, aes(x = x, xend = xend, y = y, yend = yend), color = ""grey20"", size = 0.5) +
  geom_richtext(data = waffle_labels, aes(label = glue(""{value}""), x = x + 1, y = -1), fill = ""white"", color = ""black"", hjust = 0, vjust = 1, family = ""Oswald"", show.legend = FALSE, label.color = NA, label.padding = grid::unit(rep(0, 4), ""pt"")) +
  scale_fill_manual(values = pal) +
  coord_equal() +
  scale_color_manual(values = pal) +
  labs(x = NULL,
       y = NULL) +
  theme_jk(grid = FALSE) +
  facet_wrap(~date, nrow = 1, as.table = FALSE, strip.position = ""top"", labeller = labeller(date = mday_label)) +
  expand_limits(y = c(-5, 20)) +
  theme(axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        panel.spacing.y = unit(0.1, ""lines""),
        panel.spacing.x = unit(0.1, ""lines"")) 

out <- wrap_plots(waffle_by_activity, waffle_by_day, split_waffle_by_day, ncol = 1) +
  plot_annotation(title = ""Breakdown of Observed Squirrel Activity from the 2018 NYC Squirrel Census"",
                  subtitle = glue(""Below are  waffle charts of activity totals (I), daily totals (II) and exploded daily activity (III) views of observed squirrel activity.<br>{highlight_text('Foraging', '#DD2A7B', 'b')} is the most frequently observed activity recorded in the census, not surprising for squirrels in the fall.""),
                  caption = ""Data: **NYC Data Portal** | Graphic: **@jakekaupp**"",
                  tag_levels = ""I"",
                  theme = theme_jk(markdown = TRUE))

ggsave(here(""2019"", ""week44"", ""tw44_plot.png""), out, width = 11, height = 8, dev = ragg::agg_png(), dpi = ""print"")

",2019,44
113,113,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week45/R/analysis.R,"library(tidyverse)
library(jkmisc)
library(glue)

commute_mode <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-05/commute.csv"")

total_avg <- commute_mode %>% 
  group_by(city, mode) %>% 
  filter(n() > 1) %>% 
  group_by(mode) %>% 
  summarize(avg = mean(percent/100)) %>% 
  mutate(state = ""US"",
         state_abb = ""US"")

slope_data <- commute_mode %>% 
  group_by(city, mode) %>% 
  filter(n() > 1) %>% 
  group_by(state, state_abb, mode) %>% 
  summarize(avg = mean(percent/100)) %>% 
  ungroup() %>% 
  mutate(state_abb = ifelse(is.na(state_abb), ""DC"", state_abb))

direct_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  top_n(8, avg) %>% 
  arrange(-avg) %>% 
  slice(-3:-6)

direct_labels_bike <- slope_data %>% 
  filter(mode == ""Bike"") %>% 
  top_n(5, avg)

mid_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  top_n(8, avg) %>% 
  arrange(-avg) %>% 
  slice(3:6) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
    avg = mean(.$avg),
    y = min(.$avg),
    yend = max(.$avg))
  
lower_labels <- slope_data %>% 
  filter(mode == ""Walk"") %>% 
  arrange(-avg) %>% 
  slice(9:nrow(.)) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
            avg = mean(avg),
            y = min(.$avg),
            yend = max(.$avg))

lower_bike_labels <- slope_data %>% 
  filter(mode == ""Bike"") %>% 
  arrange(-avg) %>% 
  slice(6:nrow(.)) %>% 
  group_by(mode) %>% 
  summarize(state_abb = toString(state_abb),
            avg = mean(avg),
            y = min(.$avg),
            yend = max(.$avg))


ggplot(slope_data, aes(x = mode, y = avg)) +
  geom_line(aes(group = state), size = 0.2) +
  geom_line(data = total_avg, aes(group = state), color = ""#DD2A7B"", size = 1) +
  geom_point(shape = 21, color = ""white"", stroke = 0.2, fill = ""black"", size = 2) +
  geom_text(data = direct_labels, aes(label = state_abb),  nudge_x = 0.05, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = direct_labels_bike, aes(label = state_abb),  nudge_x = -0.05, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = mid_labels, aes(label = state_abb),  nudge_x = 0.1, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = lower_labels, aes(label = str_wrap(state_abb, 30)), nudge_x = 0.1, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_text(data = lower_bike_labels, aes(label = str_wrap(state_abb, 30)), nudge_x = -0.5, family = ""Lora"", size = 3, hjust = 0, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.06, xend = 2.06, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.01, xend = 2.06, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.01, xend = 2.06, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_labels, aes(x = 2.06, xend = 2.09, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.06, xend = 2.06, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.01, xend = 2.06, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.01, xend = 2.06, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = mid_labels, aes(x = 2.06, xend = 2.09, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.95, xend = 0.95, y = y, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.98, xend = 0.95, y = y, yend = y), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.98, xend = 0.95, y = yend, yend = yend), size = 0.2, color = ""grey50"") +
  geom_segment(data = lower_bike_labels, aes(x = 0.95, xend = 0.91, y = avg, yend = avg), size = 0.2, color = ""grey50"") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = NULL,
       y = NULL,
       title = ""Bicycling and Walking to Work in the United States: 2008-2012"",
       subtitle = glue(""Illustrated below is a slopegraph contrasting the percentage of population that bikes to work and the percentage<br>that bikes to work as well as {highlight_text('the US average', '#DD2A7B', 'b')}"")) +
  scale_x_discrete(labels = c(""Bike to Work"", ""Walk to Work"")) +
  theme_jk(grid = ""XY"",
           markdown = TRUE) +
  theme(panel.grid.major = element_line(linetype = ""dashed""))
  
",2019,45
114,114,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week5/R/analysis.R,"library(tidyverse)
library(ggalt)
library(jkmisc)
library(here)
library(scales)

milk_cow_data <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-29/milkcow_facts.csv"")

milk_product_facts <- read_csv(""https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-29/milk_products_facts.csv"")

us_pop <- read_csv(here(""2019"", ""week5"", ""data"",""us-population-1990-to-2016.csv""))

totals <- milk_product_facts %>% 
  mutate(total_consumption = rowSums(select(., -year))) %>% 
  select(year, total_consumption)

full_data <- left_join(milk_cow_data, totals) %>% 
  left_join(us_pop) %>% 
  mutate(total_consumption_lbs = total_consumption * population)


ggplot(full_data, aes(y = total_consumption_lbs, x = milk_production_lbs)) +
  geom_xspline2(aes(s_open = TRUE, s_shape = 0.5)) +
  geom_point(shape = 21, fill = ""black"", color = ""white"", stroke = 1) +
  scale_y_continuous(labels = scales::unit_format(unit = ""B"", scale = 10e-10, sep = """"), breaks = pretty_breaks(6)) +
  scale_x_continuous(labels = scales::unit_format(unit = ""B"", scale = 10e-10, sep = """"), breaks = pretty_breaks(6)) +
  labs(x = ""US Milk Production (lbs)"",
       y = ""US Average Dairy Consumption (lbs)"",
       title = ""100 Slices of American Cheese or, the Fable of Supply Management"",
       subtitle = str_wrap(""The connected scatterplot below illustrates the relationship between total average dairy consumption and total milk production over the past 25 years.
                           US supply far exceeds the demand, highlighting overproduction and a case for supply management."", 120)) +  
  theme_jk()
",2019,5
115,115,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week6/R/analysis.R,"library(tidyverse)
library(ggalt)
library(jkmisc)
library(lubridate)
library(here)
library(scales)
library(janitor)
library(ggrepel)
library(patchwork)

state_hpi <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-05/state_hpi.csv"")

prime_rates <- read_csv(here(""2019"",""week6"",""data"",""MPRIME.csv"")) %>% 
  clean_names() %>% 
  mutate(year = year(date)) %>% 
  select(-date) %>% 
  group_by(year) %>% 
  summarize_all(mean) %>% 
  filter(year %in% min(state_hpi$year):max(state_hpi$year))

highs <- filter(prime_rates, mprime %in% range(mprime)) %>% 
  distinct(mprime, .keep_all = TRUE)

plot_data <- state_hpi %>% 
  group_by(year, state) %>% 
  summarize_all(mean, na.rm = TRUE) 

prime <- ggplot(prime_rates, aes(x = year, y = mprime)) +
  geom_line(color = viridis_pal()(1), size = 0.5) +
  geom_point(data = highs, color = viridis_pal()(1)) +
  geom_text_repel(data = highs, aes(label = paste0(mprime, ""%"")), color = viridis_pal()(1), nudge_x = 2, nudge_y = 2, family = ""Oswald"", segment.size = 0) +
  scale_x_continuous(breaks = c(1975, seq(1980, 2010, 10), 2018)) +
  theme_jk(grid = ""XY"") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Interest Rates Fall, Housing Prices on the Rise"",
       subtitle = str_wrap(""The top chart shows the average prime interest rate by year since 1975.  The bottom heatmap illustrates the yearly average housing price index by State since 1975."", 100))


heatmap <- ggplot(plot_data, aes(x = year, y = fct_reorder(state, price_index, .fun = mean), fill = price_index)) +
  geom_tile(color = ""white"", size = 0.05) +
  scale_x_continuous(breaks = c(1975, seq(1980, 2010, 10), 2018)) +
  scale_fill_viridis_c(""House Price Index"", option = ""viridis"", direction = 1, breaks = pretty_breaks(5)) +
  scale_color_identity() +
  labs(x = NULL, y = NULL, caption = ""Data: FRED | Graphic: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"",
        legend.key.width = unit(1, ""cm""))


out <- patchwork::wrap_plots(prime, heatmap, heights = c(0.2,1), ncol = 1)

ggsave(here(""2019"", ""week6"", ""tw6_plot.png""), out, width = 8, height = 10)
",2019,6
116,116,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week7/R/analysis.R,"library(tidyverse)
library(here)
library(readxl)
library(janitor)
library(jkmisc)
library(nord)

oecd_data <- here(""2019"", ""week7"", ""data"", ""OECD--1.xlsx"") %>% 
  read_excel(skip = 1, na = c(""na"")) %>% 
  clean_names() %>% 
  filter(!is.na(x1995)) %>% 
  rename(country = x1) %>% 
  gather(year, intensity, -country) %>% 
  arrange(country, year) %>% 
  fill(intensity, .direction =  ""down"") %>% 
  mutate(year = parse_number(year)) %>% 
  group_by(year) %>% 
  arrange(year, intensity) %>% 
  mutate(rank = row_number(-intensity)) %>% 
  ungroup() %>% 
  mutate(color = if_else(country == ""Canada"", nord(""victory_bonds"")[2], nord(""snowstorm"", 1)),
         text_color = if_else(country == ""Canada"", nord(""snowstorm"", 1), ""black""))




plot <- ggplot(oecd_data, aes(x = year, y = -rank, group = country)) +
  geom_line(aes(color = color)) +
  geom_point(aes(color = color)) +
  geom_text(data = filter(oecd_data, year == min(year)), aes(label = rank, color = color), x = 1994, hjust = 0, family = ""Oswald"") +
  geom_text(data = filter(oecd_data, year == max(year)), aes(label = country, color = color), x = 2016.5, hjust = 0, family = ""Oswald"") +
  expand_limits(x = c(1994, 2019)) +
  scale_x_continuous(breaks = 1995:2016) +
  scale_color_identity() +
  theme_jk(dark = TRUE, grid = FALSE) +
  theme(axis.text.y = element_blank()) +
  labs(x = NULL, 
       y = NULL,
       title = ""Canada is Losing a Step In the Global Research Race."",
       subtitle = str_wrap(""Shown below is the ranking of research intensity (% of Gross Domestic Product devoted to Research) from 1995-2016. Canada has been on a decline since hitting a peak in 2001.  Most notably is 2009-2016, which coincides with the systematic defunding of Canadian research scientists by the Conservative Harper Government."", 120),
       caption = ""Data: OECD | Graphic: @jakekaupp"")

ggsave(here(""2019"", ""week7"", ""tw7_plot.png""), plot, width = 9, height = 4.5)
",2019,7
117,117,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week8/R/analysis.R,"library(tidyverse)
library(here)
library(fs)
library(readxl)
library(janitor)
library(jkmisc)
library(scales)
library(egg)

parse_table31 <- function(file) {
  
  df <- file %>% 
    read_excel(na = ""na"") %>% 
    clean_names()
  
  start <- min(str_which(df$x3,""\\d{4}""))
  
  end <- pull(df, 1) %>% 
    str_which(""Since"") %>% 
    max()
  
  df <- slice(df, start:end)
  
  field_idx <- select(df, -1) %>% 
    map_df(is.na) %>% 
    pmap_lgl(all)
  
  labels <- select(df, 1) %>%
    filter(field_idx) %>% 
    na.omit() %>% 
    pull() %>% 
    str_remove(""[abcd]$"")
  
  years <- slice(df, 1) %>% 
    select(-1) %>% 
    flatten_chr() %>% 
    str_remove(""\\.0+$"")
  
 rep <- filter(df, !field_idx) %>% 
    slice(-1) %>% 
    select(1) %>% 
    n_distinct()
  
  filter(df, !field_idx) %>% 
    slice(-1) %>% 
    mutate(discipline = rep(labels, each = rep)) %>% 
    set_names(c(""category"", years, ""discipline"")) %>% 
    gather(year, value, matches(""[0-9]{4}"")) %>% 
    mutate_at(vars(-category, -discipline), as.numeric) %>% 
    mutate_at(""category"", function(x) str_remove(x, ""[abcd]$""))
  
  
}


files <- here(""2019"",""week8"",""data"") %>% 
  dir_ls() 

data <- map_df(files, parse_table31) %>% 
  ungroup() %>% 
  distinct()

plot_data <- data %>% 
  filter(discipline != ""Other"", !str_detect(category, ""doctoral"")) %>% 
  filter(!str_detect(discipline, ""and (?!computer)"")) %>% 
  filter(!str_detect(discipline, ""Physical"")) %>% 
  arrange(year) %>% 
  group_by(category, discipline, year) %>% 
  summarize(value = mean(value, na.rm = TRUE)) %>% 
  ungroup()

plot <- ggplot(plot_data, aes(x = year, y = value, color = discipline)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(breaks = pretty_breaks(), limits = c(0, 25)) +
  scale_color_manual(""Discipline"", values = tol7qualitative) +
  scale_x_continuous(limits = c(1985, 2017)) +
  expand_limits(x = c(1985, 2025)) +
  facet_wrap(~category, labeller = as_labeller(str_to_title), nrow = 1) +
  labs(x = NULL,
       y = NULL,
       title = ""Median Completion Time for Doctoral Degrees Are Getting Shorter"",
       subtitle = str_wrap(""Median completion time in years from 1985 to 2017 contrasting selected disciplines for both University and Graduate School experience.  Education, Humanities and Social Sciences doctoral candidates have a higher than average time to completion in both categories compared to other disciplines. "", 120),
       caption = ""Data: NSF | Analysis: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"")

ggsave(here('2019','week8',""tw8_plot.png""), plot, width = 10, height = 7)


bonus_plot_data <- plot_data %>% 
  filter(discipline == ""All fields"") %>% 
  rename(overall = value) %>% 
  select(-discipline) %>% 
  left_join(filter(plot_data, discipline != ""All fields"")) %>% 
  mutate(diff = abs(value - overall)/2) %>%
  filter(between(year, 1990, 2011))


order <- c(""Education"", ""Mathematics and computer sciences"",  ""Engineering"", ""Humanities"",""Life sciences"", 
   ""Social sciences"")

bonus_plot <- ggplot(bonus_plot_data, aes(ymin = -diff, ymax = diff, x = year, fill = fct_relevel(discipline, order))) +
  geom_ribbon(color = ""white"", size = 0.2, alpha = 0.8) +
  geom_segment(data = filter(bonus_plot_data, year == 2000, category == ""Since bachelor's"", discipline == ""Education""), aes(y = -diff, yend = diff, x = year, xend = year), color = ""grey20"", arrow = arrow(length = unit(0.25, ""cm""), ends = ""both"", type = ""closed"")) +
  facet_wrap(~category, labeller = as_labeller(str_to_title)) +
  scale_fill_viridis_d(""Discipline"") +
  scale_y_continuous(breaks = c(-5, -2.5, -1, 0, 1, 2.5, 5), labels = c(""10 Years"", ""5 Years"", ""2 Years"", ""Group Median"", ""2 Years"", ""5 Years"", ""10 Years"")) +
  labs(x = NULL,
       y = NULL,
       title = ""Relative Differences in Median Doctoral Completion Time from the Group Median by Discipline and Interval"",
       subtitle = str_wrap(""The streamgraph below presents the difference between discipline median completion time and the group median completion time (All Fields) as the width of each colored band (discipline) from 1990 to 2011."", 120),
       caption = ""Data: NSF | Analysis: @jakekaupp"") +
  theme_jk(grid = ""XY"") +
  theme(legend.position = ""bottom"")

bonus_plot <- tag_facet(bonus_plot, x = 1996, y = 5.5, open = """", close = """", tag_pool = c(""Difference from Group Median = 9.1 years"", """"),  fontface = 1, family = ""Oswald"") 



ggsave(here('2019','week8',""tw8_bonus_plot.png""), bonus_plot, width = 12, height = 6)
",2019,8
118,118,https://github.com/jkaupp/tidyweek,jkaupp,tidyweek,2019/week9/R/analysis.R,"library(tidyverse)
library(here)
library(ggforce)
library(jkmisc)
library(sf)
library(osmdata)

full_trains <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/full_trains.csv"")

available_tags(""railway"")

railways <- st_read(here(""2019"", ""week9"", ""data"", ""railways.shp""))

q <- getbb(""fr"") %>%
  opq(timeout=25*1000)%>%
  add_osm_feature(""railway"")

stations <- osmdata_sf(q)

ggplot(stations$osm_lines) +
  geom_sf()

railways$geometry[[1]] %>% st_coordinates() %>% as_tibble -> line

ggplot(line, aes(x = X, y = Y)) +
  geom_link2() +
  coord_sf(datum=NA)

nat_trains <- full_trains %>% 
  filter(service == ""National"") %>% 
  group_by(year, departure_station, arrival_station) %>% 
  summarize_at(vars(journey_time_avg, total_num_trips, avg_delay_late_at_departure, avg_delay_late_on_arrival), mean, na.rm = TRUE)


# Orbit test

centre <- ""PARIS LYON""

test_data <- filter(nat_trains, departure_station == centre | arrival_station == centre) %>% 
  arrange(departure_station)



positions <- test_data %>% 
  filter(arrival_station == centre) %>% 
  group_by(arrival_station, departure_station) %>% 
  summarize(dist = mean(journey_time_avg)) 


circles <- test_data %>% 
  group_by(departure_station) %>% 
  summarize(centre_radius = mean(avg_delay_late_at_departure)) %>% 
  left_join(positions)

main <- circles %>% 
  filter(departure_station == centre)

circles <- circles %>% 
  filter(departure_station != centre) %>% 
  mutate(fraction = nrow(.) - (nrow(.) - seq_along(departure_station)),
         delta = 360/nrow(.)*fraction) %>% 
  bind_rows(main) %>% 
  mutate(x0 = if_else(departure_station == centre, 0, dist*cos((delta*pi/180))),
         y0 = if_else(departure_station == centre, 0, dist*sin((delta*pi/180)))) 


link_coords <- function(dept, arr, lnk) {
  
  circles %>% 
    filter(departure_station == dept | departure_station == arr) %>%
    summarise(x = ifelse(lnk == ""from"", x0[x0 != 0], 0),
           xend = ifelse(lnk == ""from"", 0, x0[x0 != 0]),
           y = ifelse(lnk == ""from"", y0[y0 != 0], 0),
           yend = ifelse(lnk == ""from"", 0, y0[y0 != 0]))
  
  
  
}

links <- test_data %>% 
  group_by(departure_station, arrival_station) %>% 
  mutate(total_delay = ((avg_delay_late_at_departure + avg_delay_late_on_arrival)/journey_time_avg),
         total_trips = sum(total_num_trips)) %>% 
  summarize(size = mean(total_delay),
            alpha = mean(total_num_trips)/max(total_num_trips)) %>% 
  mutate(link = if_else(departure_station == centre, ""to"", ""from"")) %>% 
  ungroup() %>% 
  mutate(links = pmap(list(departure_station, arrival_station, link), ~link_coords(..1, ..2, ..3))) %>% 
  unnest() %>% 
  arrange(link, departure_station)




ggplot() +
  geom_curve(data = links, aes(x = x, xend = xend, y = y, yend = yend, size = size, color = link, alpha = alpha), lineend = ""round"", angle = 270) +
  geom_circle(data = circles, aes(x0 = x0, y0 = y0, group = departure_station, r = 5), fill = ""white"", color = ""#2b41a7"") +
  scale_size(range = c(1,6)) +
  scale_color_manual(values = c(""#2b41a7"", ""#c7ad24"")) +
  scale_fill_distiller(palette = ""Greys"")+
  scale_alpha_identity() +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_jk(grid = FALSE) +
  theme(axis.text = element_blank())

",2019,9
119,969,https://github.com/deepshamenghani/tidytuesday/tree/master/tt_07_29_2019_week31_videogames,deepshamenghani,tidytuesday,tt_07_29_2019_week31_videogames/video_games.R,"library(tidyverse)
library(lubridate)
library(plotly)
library(ggrepel)
library(cowplot)
library(ggforce)

vg_df <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-30/video_games.csv"")

# Clean dates
vg_edited <- vg_df %>% 
    mutate(release_date = as.Date(release_date, format = '%b %d, %Y')) %>%
    mutate(release_year  = as_factor(year(release_date))) %>% 
    drop_na(release_year) %>% 
    mutate(label_text = str_glue(""Game: {game}
                                 Year: {release_year}
                                 Price: {price}""))

# Create text for annotations by playtime
annotations_playtime <- vg_edited %>% 
    arrange(desc(median_playtime)) %>% 
    filter(row_number() < 5) %>% 
    mutate(log_price = log(price)) %>% 
    mutate(game_label = str_glue(""{game}\n{release_year}\n{scales::dollar(price)}"")) %>% # Create label text for plot
    select(game_label, median_playtime, log_price) %>% 
    mutate(y = c(median_playtime[1] + 400, median_playtime[2] - 200, median_playtime[3] + 150, median_playtime[4] - 400),  # Define annotations location
           x = c(log_price[1], log_price[2] + 0.85, log_price[3] - 0.85, log_price[4])) %>% 
    mutate(y_arrow = c(median_playtime[1] + 200, median_playtime[2] + 50, median_playtime[3] - 50, median_playtime[4] - 200)) # Define arrow location

# Create text for annotations by price
annotations_price <- vg_edited %>% 
    arrange(desc((price))) %>% 
    filter(row_number() < 3) %>% 
    mutate(log_price = log(price)) %>% 
    mutate(game_label = str_glue(""{game}\n{release_year}\n{scales::dollar(price)}"")) %>%
    select(game_label, median_playtime, log_price) %>% 
    mutate(y = c(median_playtime[1] + 550, median_playtime[2] + 450), # Define annotations location
           x = c(log_price[1] - 0.2, log_price[2] - 0.75)) %>% 
    mutate(y_arrow = c(median_playtime[1] + 300, median_playtime[2] + 200))  # Define arrow location
    

# Plot with annotations and arrows
pc_games_plot <- ggplot(data = vg_edited, 
                        aes(x = log(price), 
                            y = median_playtime)) +
    geom_point(aes(color = median_playtime, 
                   size  = median_playtime), 
                   alpha = 0.8) +
    scale_color_gradient(low  = ""blue2"", 
                         high = ""darkgreen"") +
    annotate(""text"", 
             x        = annotations_playtime$x, 
             y        = annotations_playtime$y, 
             fontface = ""bold"", 
             label    = annotations_playtime$game_label, 
             size     = 5, 
             color    = ""darkgreen"") + 
    geom_curve(data      = annotations_playtime, 
               aes(x     = log_price, 
                   y     = median_playtime, 
                   xend  = x, 
                   yend  = y_arrow),
               arrow     = arrow(length = unit(0.07, ""inch"")), 
               size      = 1,
               color     = ""gray20"", 
               curvature = -0.25)+
    annotate(""text"", 
             x        = annotations_price$x, 
             y        = annotations_price$y, 
             fontface = ""bold"", 
             label    = annotations_price$game_label, 
             size     = 5, color = ""blue2"") + 
    geom_curve(data      = annotations_price, 
               aes(x     = log_price, 
                   y     = median_playtime, 
                   xend  = x, 
                   yend  = y_arrow),
               arrow     = arrow(length = unit(0.07, ""inch"")), 
               size      = 1,
               color     = ""gray20"", 
               curvature = -0.25) + 
    geom_mark_circle(data       = annotations_price, 
                     aes(x      =log_price[1] ,
                         y      =median_playtime[1]), 
                     color      ='blue2', 
                     label.fill = NA, 
                     expand     = unit(3, ""mm"")) + 
    geom_mark_circle(data       = annotations_price, 
                     aes(x      =log_price[2] ,
                         y      =median_playtime[2]), 
                     color      ='blue2', 
                     label.fill = NA, 
                     expand     = unit(3, ""mm"")) +
    labs(
        x     = ""price (logarithmic scale)"",
        y     = ""Median playtime"",
        title = ""Playtime vs Price""
    ) +
    theme_minimal() +
    scale_y_continuous(breaks = seq(from = 0, to = 4000, by = 500))  +
    scale_x_continuous(breaks = seq(from = -1, to = 7, by = 1)) +
    theme(
        axis.title.x     = element_text(size = 20),
        axis.title.y     = element_text(size = 20, vjust = 1.5),
        axis.text.x      = element_text(size = 15),
        axis.text.y      = element_text(size = 15),
        plot.title       = element_text(size = 25),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()
    ) 

ggsave(""PC_Games.png"", plot = pc_games_plot, width = 60, height =25, units = ""cm"")
",2019,31
120,1080,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week21/plastic_waste.R,AmandaRP,tidytuesday,2019/week21/plastic_waste.R,"library(tidyverse)
library(janitor)
library(inspectdf)
library(cowplot)
library(ggrepel)


#Read data and clean with janitor:
coast_vs_waste <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-21/coastal-population-vs-mismanaged-plastic.csv"") %>% clean_names()
waste_vs_gdp <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-21/per-capita-plastic-waste-vs-gdp-per-capita.csv"")  %>% clean_names()
mismanaged_vs_gdp <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-21/per-capita-mismanaged-plastic-waste-vs-gdp-per-capita.csv"")  %>% clean_names()

#Put data together in one df:
waste <- left_join(waste_vs_gdp, coast_vs_waste, by = c(""entity"", ""code"", ""year""))
(waste <- left_join(waste, mismanaged_vs_gdp, by = c(""entity"", ""code"", ""year"")))

View(waste)

#Clean names a bit more
(waste <- waste %>%
  select(-total_population_gapminder.y, -total_population_gapminder.x, -gdp_per_capita_ppp_constant_2011_international_rate) %>%
  rename(mismng_pl_waste_tons = mismanaged_plastic_waste_tonnes,
        mismng_pl_waste_per_cap = per_capita_mismanaged_plastic_waste_kilograms_per_person_per_day,
        plastic_waste_per_cap = per_capita_plastic_waste_kilograms_per_person_per_day,
        coast_pop = coastal_population,
        gdp_per_cap = gdp_per_capita_ppp_constant_2011_international_constant_2011_international,
        population = total_population_gapminder))
View(waste)


#Look for correlations using the inspectdf package:
inspect_cor(waste, show_plot = TRUE) 
inspect_na(waste, show_plot = TRUE) #Looks like we only have 2010 waste data

#Plots:
data4plot <- waste %>% filter(year == 2010) %>% filter(entity != ""World"")
plt1 <- ggplot(data4plot, aes(x = log(gdp_per_cap), 
             y = log(plastic_waste_per_cap), 
             size = mismng_pl_waste_tons)) +
  geom_smooth(method = ""lm"") +
  geom_point(color = ""#333333"", alpha = 0.7) +
  geom_text_repel(aes(label = entity),
    color         = ""red"",
    size          = 4,
    data          = subset(data4plot, log(plastic_waste_per_cap) > 0 | log(plastic_waste_per_cap) < -4.5 | (log(gdp_per_cap) < 7 & log(plastic_waste_per_cap) < -4) | code %in% c(""USA"",""CHN"")),
    nudge_y       = .8,
    segment.size  = 0.2,
    segment.color = ""grey50"",
    direction     = ""x""
  ) +
  labs(title = ""Richer nations tend to produce more plastic waste"",
       x = ""GDP Per Capita (log scale)"",
       y = ""Per Capita Plastic Waste in kg/day (log scale)"",
       size = ""Mismanagement of \nPlastic Waste (tons)"")  +
  theme(legend.position = ""none"") 


#Richer nations manage their plastic waste better
plt2 <- ggplot(data4plot, aes(x = log(gdp_per_cap), 
             y = mismng_pl_waste_per_cap, 
             size = mismng_pl_waste_tons)) +
  geom_smooth(show.legend = FALSE) + 
  geom_point(color = ""#333333"", alpha = 0.7) + 
  geom_text_repel(aes(label = entity), 
    color         = ""red"",
    size          = 4,
    data          = subset(data4plot, mismng_pl_waste_per_cap > .14 | code %in% c(""USA"",""CHN"")),
    nudge_y       = .025,
    nudge_x       = .1,
    segment.size  = 0.2,
    segment.color = ""grey50"",
    direction     = ""x""
  ) + 
  ylim(0, NA) +
  labs(title = ""Many countries having mid-range GDP are\nbad at management of their plastic waste "",
       x = ""GDP Per Capita (log scale)"",
       y = ""Per Capita Mismanaged Plastic Waste in kg/day"",
       size = ""Mismanaged\nPlastic Waste (tons)"") 

#Get the legend from plot 2 so that I can put it in its own plot grid panel:
l <- get_legend(plt2) 

#Final plot
plot_grid(plt1, 
          plt2 + theme(legend.position = ""none""), 
          l,
          nrow = 1)

",2019,21
121,1081,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week24/meteorites.R,AmandaRP,tidytuesday,2019/week24/meteorites.R,"library(tidyverse)
library(ggdark)
library(gganimate)

#read data:
meteorites <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv"")

#Get rid of outliers:
meteorites %<>% filter(long<180 & year <= 2013)

#map:
mapWorld <- borders(""world"", colour=""gray50"", fill=""gray50"") # create a layer of borders

ggplot(meteorites) +   
  mapWorld + 
  geom_point(aes(x=long, y=lat) ,color=""green"", size=1, alpha = .1) +
  dark_mode() +
  theme(panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank()) + 
  labs(caption = ""Data from the Meteoritical Society (and shared by NASA)"") +
  labs(title = ""Meteorite Crashes on Earth"") 
    
      
# Experimentation with animation:  
#transition_states(year,
#                    transition_length = 2,
#                    state_length = 1) +
#    ggtitle(""Meteorites"", subtitle = ""Year:  {frame_time}"") 
  

#Time series plot:
meteorites %>% 
  filter(year > 1950 & year < 2013) %>%
  group_by(year) %>% 
  count() %>% 
  arrange(desc(n)) %>%
  ggplot(aes(year, n)) +
  geom_line(color = ""green"") + 
  dark_mode() + 
  labs(title = ""Meteorites by Year (1950 - 2012)"",
       x = ""Year"",
       y = ""Count"") +
  geom_curve(aes(x = 1979 + 5, y = 3000, xend = 1979.5, yend = 3046), curvature = 0.3, arrow = arrow(length=unit(2,""mm"")), color = ""white"") +
  annotate(""text"", x = 1979 + 7, y = 2950, label = ""1979"")
  


",2019,24
122,1082,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week19/R/20190507.Rmd,AmandaRP,tidytuesday,2019/week19/R/20190507.Rmd,"---
title: ""Week 19""
output: html_notebook
---

```{r}
library(tidyverse)
library(here)
library(rworldmap)
library(ggthemes)
```

Read data:

```{r}
student_ratio <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/student_teacher_ratio.csv"")

```

Clean data and calculate median:

```{r}

med_ratios <- student_ratio %>% 
  filter(!is.na(student_ratio)) %>%      # remove NAs
  group_by(country_code, indicator) %>%
  filter(year == max(year)) %>%          # grab the most recent data for each country and education level
  ungroup() %>%
  group_by(country_code, country) %>%
  summarise(median_ratio = median(student_ratio)) 

```

Check names in med_ratios compared to map_data(""world"")

```{r}
anti_join(med_ratios, map_data(""world""), by = c(""country"" = ""region"")) %>% View()
```


Need to recode some country names to match map_data:

```{r}
med_ratios_fixed <- med_ratios %>%
  mutate(country = recode(country,
                                ""United States of America"" = ""USA"",
                                ""Viet Nam"" = ""Vietnam"",
                                ""British Virgin Islands"" = ""Virgin Islands"",
                                ""United Republic of Tanzania"" = ""Tanzania"",
                                ""Syrian Arab Republic"" = ""Syria"",
                                ""Russian Federation"" = ""Russia"",
                                ""Democratic People's Republic of Korea"" = ""North Korea"",
                                ""The former Yugoslav Republic of Macedonia"" = ""Macedonia"",
                                ""Republic of Moldova"" = ""Moldova"",
                                ""Republic of Korea"" = ""South Korea"",
                                ""Cte d'Ivoire"" = ""Ivory Coast"",
                                ""Iran (Islamic Republic of)"" = ""Iran"",
                                ""United Kingdom of Great Britain and Northern Ireland"" = ""UK"",
                                ""Micronesia (Federated States of)"" = ""Micronesia"",
                                ""Czechia"" = ""Czech Republic"",
                                ""Cabo Verde"" = ""Cape Verde"",
                                ""Congo"" = ""Democratic Republic of the Congo"",
                                ""Brunei Darussalam"" = ""Brunei"",
                                ""Bolivia (Plurinational State of)"" = ""Bolivia""))
                                
```

Join median student teacher ratio data with map data:

```{r}
my_map_data <- med_ratios_fixed %>%
  full_join(map_data(""world""), by = c(""country"" = ""region"")) %>% 
  filter(!grepl(""Antarctica"",country)) #don't need Antarctica
```


Plot:

```{r}
ggplot(my_map_data, aes(x = long, y = lat)) +
     geom_polygon(aes(fill = median_ratio, group = group), color = ""black"", size = 0.2) +
     scale_fill_gradient2(low = ""blue"", high = ""red"", midpoint = 17.428480) +
     theme_fivethirtyeight() + 
     theme(axis.line=element_blank(),
           axis.text.x=element_blank(),
           axis.text.y=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank()) +
     labs(fill = ""ratio"",
          title = ""Median Student to Teacher Ratios"",
          caption = str_c(str_wrap(""Median student-to-teacher ratio calculated across all education levels using the most recent data for each level. White indicates a median ratio close to the world median. Countries shown in blue have a smaller ratio, while countries shown in red have a larger (less favorable) ratio. Grey indicates missing data."", width = 120),""\nData Source: UNESCO\nVisualization: @AmandaRPlunkett"")) 
  
```



```{r}
ggsave(""studentTeacherRatios.png"", width = 7.4, height = 4.5, dpi = ""retina"")
```

",2019,19
123,1083,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week28/fifa.R,AmandaRP,tidytuesday,2019/week28/fifa.R,"library(tidyverse)
library(ggimage)

#Get the data
wwc_outcomes <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/wwc_outcomes.csv"")
squads <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/squads.csv"")
codes <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/codes.csv"")

#get alternative country codes
more_codes <- countrycode::codelist_panel %>% 
  group_by(country.name.en) %>% 
  top_n(1, year) %>%
  select(country.name.en, ioc, iso2c, iso3c, genc3c, fips)

#join data with codes
wwc_outcomes_wcodes <- dplyr::left_join(wwc_outcomes, codes, by = ""team"") %>%
  select(year, team, score, country) %>%
  left_join(more_codes, by = c(""team"" = ""ioc"")) %>%
  mutate(year = as.character(year))
#Fix England:
wwc_outcomes_wcodes[wwc_outcomes_wcodes$country == ""England"", ""iso2c""] <- ""GB""

#Limit plot to top scoring countries 
top_countries <- wwc_outcomes %>%
  group_by(team) %>%
  summarize(total_score = sum(score)) %>%
  top_n(11, total_score) 

#plot
ggplot(inner_join(wwc_outcomes_wcodes, top_countries), 
       aes(reorder(country, total_score), score, fill = year)) + 
  geom_col(position = position_stack(reverse = TRUE)) + #adjusted position_stack to have years increase from left to right
  ggthemes::scale_fill_tableau(name=""Year"") +           #nice colors
  coord_flip() +
  labs(
    title = ""Womens World Cup Soccer: Total Goals (1991 - 2019)"",
    caption = ""Data from https://data.world/sportsvizsunday/womens-world-cup-data""
  ) +
  geom_flag(y = -5, aes(image = iso2c)) +
  expand_limits(y = -5) +
  geom_image(aes(x = ""France"", y = 120, 
                 image = ""~/tidytuesday/2019/week28/soccer2.png""),
             size = 0.15) +
  theme_minimal() +
  theme(title = element_text(size=14),
        panel.grid = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.title = element_blank(),
        strip.text.y = element_text(angle = 180)) + 
  guides(fill = guide_legend(reverse = TRUE))  #put 2019 at top of legend

",2019,28
124,1084,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week22/wine.R,AmandaRP,tidytuesday,2019/week22/wine.R,"library(tidyverse)
library(showtext)

#read data:
wine_ratings <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv"")

#clean up a bit:
wine_ratings <- wine_ratings %>% 
  select(-X1) %>% #X1 is not informative
  distinct()      #de-dupe based on a tip posted on twitter

#Some countries only have a handful of ratings.
# Select only countries that have atleast 1000 ratings:
large_sample_countries <- wine_ratings %>% 
  group_by(country) %>% 
  summarize(med_cntry_pnts = median(points), cnt = n()) %>% 
  arrange(desc(med_cntry_pnts)) %>%  
  filter(cnt >= 1000) #%>% View()

data4plotting <- inner_join(wine_ratings, large_sample_countries)

#Note: I had to use the showtext package (extrafonts didn't offer very many fonts 
# and newly installed fonts didn't render... may be user error). 
# Required copying and pasting code into terminal window.
# Read more about showtext package here: 
# https://cran.rstudio.com/web/packages/showtext/vignettes/introduction.html
font_add_google(""Satisfy"", ""satisfy"")
showtext_auto()


# Country vs Taster. Do some tasters give higher ratings in general than others?  
p <- data4plotting %>% 
  filter(!is.na(taster_name)) %>%
  group_by(taster_name, country) %>%
  summarize(med_cntry_taster_pnts = median(points), cnt_cntry_taster = n())  %>%
  ungroup() %>%
  mutate(taster_name = fct_reorder(taster_name, med_cntry_taster_pnts)) %>%
  ggplot(aes(x = country, y = taster_name, size = cnt_cntry_taster, color = med_cntry_taster_pnts)) +
  geom_point() +
  scale_colour_gradient(low = ""white"", high = ""dark red"") +
  theme_light() +
  labs(y = ""Taster"", 
       x = ""Wine Country"", 
       title = ""Wine Taster Rating Profile (by Country)"",
       color = ""Median Score"",
       size = ""Number of Reviews"",
       caption = ""Countries limited to those with atleast 1000 rated wines"") +
  theme(axis.text.x = element_text(angle = 45, hjust=1, size = 20),
        axis.text.y = element_text(size = 20),
        title = element_text(family = ""satisfy"", size = 36),
        plot.caption = element_text(hjust = 0.5),
        legend.text = element_text(size = 18))

p

ggsave(""wine_tasting.png"", p, height = 4, width = 7)


### Bonus code:

#The following parallel plot looks cool, but I don't think it's as informative 
# as the bubble plot.
library(ggparallel)  

df <- data4plotting %>% 
    filter(!is.na(taster_name)) %>%
    group_by(taster_name, country) %>%
    summarize(med_cntry_taster_pnts = median(points)) 
  
ggparallel(list(""taster_name"", ""country""), as.data.frame(df))

  ",2019,22
125,1085,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week20/20190514.R,AmandaRP,tidytuesday,2019/week20/20190514.R,"library(tidyverse)
library(tidytext)
library(wordcloud)

#Read data:
nobel_winners <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winners.csv"")

#Get list of words and their counts
words_df <- nobel_winners %>% 
  select(gender, motivation) %>%
  filter(!is.na(motivation) & !is.na(gender)) %>%
  unnest_tokens(word, motivation) %>%
  anti_join(stop_words) %>%
  group_by(gender, word) %>%
  summarize(cnt = n()) %>%
  arrange(desc(cnt)) %>% 
  ungroup()

#Create document term matrix:
dtm <- words_df %>% cast_dtm(gender, word, cnt) 

#Draw word clouds:
comparison.cloud(t(as.matrix(dtm)), max.words=75)
commonality.cloud(t(as.matrix(dtm)), max.words=40)
",2019,20
126,1086,https://github.com/AmandaRP/tidytuesday/blob/master/2019/week26/ufo.R,AmandaRP,tidytuesday,2019/week26/ufo.R,"library(tidyverse)
library(summarytools)
library(tidytext)
library(igraph)
library(ggraph)

#read data:
ufo_sightings <- readr::read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv"")

#Look at the data:
View(ufo_sightings)
view(dfSummary(ufo_sightings))

#Clean and tokenize 
bigram_counts <- ufo_sightings %>%
  select(description) %>%
  mutate(description = str_replace_all(description, ""\\&\\#\\d+"", """")) %>%  # Remove &#digit
  filter(!is.na(description)) %>%
  unnest_tokens(bigram, description, token = ""ngrams"", n = 2) %>%
  separate(bigram, c(""word1"", ""word2""), sep = "" "") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  filter(word1 != ""nuforc"" & word2 != ""nuforc"")

#Create graph structure:
bigram_graph <- bigram_counts %>%
  filter(n > 225) %>%
  graph_from_data_frame()

a <- grid::arrow(type = ""closed"", length = unit(.15, ""inches""))

#plot:
ggraph(bigram_graph, layout = ""fr"") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = ""lightgreen"", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()  +
  theme(plot.margin=unit(c(.5, .5, .5, .5),""cm"")) +
  labs(title = ""Common Bigrams in UFO Sighting Descriptions"",
       subtitle = ""Data provided by NUFORC"")

",2019,26
127,1124,https://github.com/Talitrus/tidytuesday/tree/master/2019/week25,Talitrus,tidytuesday,2019/week25/Jun192019_tidytuesday.R,"####################
# Bryan Nguyen
####################
# Data is from https://www.birdscanada.org/
# Cleaned and tidied by @_sharleen_w on Twitter.
############################################
# This code performs an ordination on the Christmas Bird Count (CBC) data from Hamilton, Ontario, Canada.
# Specifically, it uses a Principle Coordinates Analysis on the Bray-Curtis distance matrix of yearly data.
# The resulting plot is colored by year to show a trend through time.
# Some years were missing CBC data and were removed.
#######################################

library(tidyverse)
library(LaCroixColoR)
library(vegan)
library(ggrepel)
library(extrafont)
bird_counts <- read_csv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-18/bird_counts.csv"")


table <- spread(data = bird_counts %>% select(year, species_latin, how_many_counted_by_hour), key = ""year"", value = ""how_many_counted_by_hour"", fill = 0) %>% 
  column_to_rownames(var = ""species_latin"")
vegan_table <- t(table)

vegan_table <- vegan_table[rowSums(vegan_table) > 0,] # remove birds with no sightings in Hamilton ever.

bc_matrix <- vegdist(as.matrix(vegan_table)) # Calculate (abundance-weighted) Bray-Curtis ecological community distances between years.

PCoA_ord <- cmdscale(bc_matrix, k = 2, eig = TRUE) # Principle Coordinates Analysis
PCoA_df <- tibble(year = as.numeric(rownames(PCoA_ord$points)), x = PCoA_ord$points[,1], y = PCoA_ord$points[,2])
years_to_label <- c(1921, 1939, 2017)
PCoA_df$label <- """" # Years with a blank label are not labelled.
PCoA_df$label[match(years_to_label, PCoA_df$year)] <- years_to_label # Label years to label with the corresponding year information.

PCoA_plot <- ggplot(data = PCoA_df) +
  geom_point(mapping = aes(x = x, y = y, color = year), size = I(5), alpha = 0.70) +
  #viridis::scale_color_viridis() + # sorry, but the LaCroix color palettes are too much fun.
  scale_color_gradientn(colors = lacroix_palette(""PeachPear"")) +
  geom_label_repel(mapping = aes(x = x, y = y, label = label, color = year), force = 5) +
  xlab(paste0(""PCoA axis 1: "", round(PCoA_ord$eig[1] / sum(PCoA_ord$eig) * 100, digits = 1), ""%"")) +
  ylab(paste0(""PCoA axis 2: "", round(PCoA_ord$eig[2] / sum(PCoA_ord$eig) * 100, digits = 1), ""%"")) +
  ggtitle(""Hamilton, Canada CBC bird communities change / time"") +
  theme_minimal() +
  theme(
    text = element_text(
      family = ""Lato"" # This is a freely available Google font that I imported into R with the 'extrafont' library.
        ),
    plot.title = element_text(
      face=""bold"")
  )
PCoA_plot

ggsave(filename = ""PCoA_plot.png"", plot = PCoA_plot, height = 4, width = 6)
",2019,25
128,1126,https://github.com/toscano84/TidyTuesday/blob/master/week4_2019/week4_2019.R,toscano84,TidyTuesday,week4_2019/week4_2019.R,"# week 4 2019 - TidyTuesday

library(tidyverse) # wrangle, visualization of data
library(data.table) # load file
library(albersusa) # map of all 50 states plus DC
library(broom) # in this case to tidy a shape file
library(ggalt) # create coordinate system in maps
library(viridis) # color palette
library(extrafont) # add fonts to R


options(scipen = 999)

# load file
incarceration <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-22/incarceration_trends.csv"")

# tidy the dataframe
murder_crime_tbl <- incarceration %>% 
  mutate(decade = year -year %% 10) %>% # create variable decade)
  group_by(decade, state) %>% # group by decade and state
  mutate(crime = rowSums(cbind(violent_crime, murder_crime),na.rm=TRUE) 
         / total_pop * 100) %>% # create variable crime that corresponds to the sum of violent crime and murder crime
  summarize(crime_rate = mean(crime, na.rm = TRUE)) %>%
  filter(!state %in% c(""AK"", ""AL"") | decade != 1970)

#--- create map of the us states

us <- usa_composite() %>%
  tidy(., region = ""iso_3166_2"") # use tidy function from the broom package to create a dataframe of the us map

#---- create quantiles
number_quantiles <- 5
labels <- c()

quantiles <- quantile(murder_crime_tbl$crime_rate, 
                      probs = seq(0, 1, length.out = no_classes + 1))

# create custom labels for the quantiles
for(i in 1:length(quantiles)){
  labels <- c(labels, paste0(round(quantiles[i], 2), 
                             "" - "", 
                             round(quantiles[i + 1], 2)))
}
# Remove last label
labels <- labels[1:length(labels)-1]

# create new variable based on quantiles
murder_crime_tbl$crime_rate_quantiles <- cut(murder_crime_tbl$crime_rate, 
                                          breaks = quantiles, 
                                          labels = labels, 
                                          include.lowest = T)


#----------build plot: fill varriable correspond to the quantiles-----#
p <-  ggplot() +
  geom_map(data = us, map = us,
           aes(x = long, y = lat,
               map_id = id),
           color = ""grey30"",
           fill = NA) +
  geom_map(data = murder_crime_tbl, map = us,
           aes(fill = crime_rate_quantiles, map_id = state),
           color = ""grey30"") +
  scale_fill_viridis_d(option = ""plasma"",
                     name = ""Violent\nand Murder Crime Rate"",
    direction = -1,
    guide = guide_legend(
      keyheight = unit(5, units = ""mm""),
      title.position = 'top',
      reverse = T
    )) +
  facet_wrap(vars(decade)) +
  coord_proj(us_laea_proj) +
  labs(title = ""Violent and Murder Crime in the USA"",
       subtitle = ""Mean Rate (%) by decade "",
       x = """",
       y ="""") +
  theme_minimal() +
  theme(plot.title = element_text(family = ""Cooper Black"",
                                  size = 30, hjust = 0.5),
        plot.subtitle = element_text(family = ""Cooper Black"",
                                  size = 20, hjust = 0.5),
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(family = ""Cooper Black"", size = 18, 
                                  face = ""bold""),
        legend.text = element_text(family = ""Cooper Black"", size = 14),
        legend.title = element_text(family = ""Cooper Black"", size = 18),
        plot.background = element_rect(fill = ""#e8e8ea""),
        panel.grid = element_line(color = ""grey70""),
        legend.direction = ""vertical"",
        legend.position = c(0.85, 0.25))

ggsave(""crime_usa.jpg"", p, units = ""cm"", 
       height = 25, width = 40, dpi = ""retina"")

",2019,4
129,1129,https://github.com/toscano84/TidyTuesday/blob/master/week2_2019/week2_2019.R,toscano84,TidyTuesday,week2_2019/week2_2019.R,"# week 2 2019 - TidyTuesday

# libraries needed
library(tidyverse)
library(data.table)
library(ggrepel)
library(lubridate)
library(ggdark)
library(ggrepel)
library(extrafont)

# import and load fonts
font_import()
loadfonts(device = ""win"")

# open the file
imdb_ratings <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-08/IMDb_Economist_tv_ratings.csv"")

# glimpse the dataframe
glimpse(imdb_ratings)

# tidy the file
imdb_ratings_tidy <- imdb_ratings %>%
  mutate(year = year(date), # create variable year
         decade = year - year %% 10) # create variable decade


# create variable above_below_median grouped by year
imdb_ratings_tidy <- imdb_ratings_tidy %>%
  group_by(year) %>%
  mutate(above_below_median = ifelse(av_rating > median(av_rating), 
                               1, 0))

# create a dataframe with highest average tv_ratings per decade
imdb_top_perdecade <- imdb_ratings_tidy %>%
  group_by(decade) %>%
  top_n(1, wt = av_rating)

#----plot----#
plot <- imdb_ratings_tidy %>%
  group_by(year) %>%
  ggplot(aes(x = factor(year),
             y= av_rating, label = title)) +
  geom_jitter(aes(color = factor(above_below_median)), size = 3, alpha = 0.3) +
  geom_boxplot(alpha = 0, color = ""grey50"", outlier.colour = ""red"") +
  geom_label_repel(data = imdb_top_perdecade, # label with the top show per decade
                   fontface = ""bold"",
                   color = ""grey50"",
                   size = 6,
                   family = ""Agency FB"",
                   direction = ""x"") +
  stat_summary(fun.y = median, geom = ""line"", aes(group = 1), 
               linetype = 1, colour = ""grey50"", size = 1) # create a line based on the median +
  dark_theme_gray() +
  theme(plot.title = element_text(family = ""Agency FB"", face = ""bold"",
                                  size = 40, hjust = 0.5),
        plot.subtitle = element_text(family = ""Agency FB"", face = ""bold"",
                                     size = 20, hjust = 0.5),
        plot.background = element_rect(fill = ""grey10""),
        panel.background = element_blank(),
        panel.grid.major = element_line(color = ""grey30"", size = 0.2),
        panel.grid.minor = element_line(color = ""grey30"", size = 0.2),
        legend.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_text(family = ""Agency FB"", size = 15),
        axis.text = element_text(family = ""Agency FB"", size = 15),
        legend.key = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(family = ""Agency FB"", size = 15),
        legend.text = element_text(family = ""Agency FB"", size = 15),
        legend.title = element_text(family = ""Agency FB"", size = 15)) +
  scale_color_manual(values = c(""#d35400"", ""#1abc9c""), labels = c(""Below the Median"", 
                                                                  ""Above the Median"")) + 
  labs(title = ""Average Rating of Tv Shows"",
       subtitle = ""Parenthood, Third Watch and Breaking Bad had\nthe highest average rating for each decade"", 
       x = ""Year"", 
       y = ""Average Rating"", 
       color = ""Shows"") +
  facet_wrap(vars(decade), ncol = 3, scales = ""free_x"")

plot

ggsave(""av_ratings_imdb.jpg"", plot, units = ""cm"", 
       height = 25, width = 40, dpi = ""retina"")
",2019,2
130,1130,https://github.com/toscano84/TidyTuesday/blob/master/week5_2019/week5_2019.R,toscano84,TidyTuesday,week5_2019/week5_2019.R,"# week 5 2019 - TidyTuesday

library(tidyverse) # wrangle, visualization of data
library(data.table) # load file
library(extrafont) # add fonts to R
library(gghighlight) # highlight values in a plot

options(scipen = 999) # remove scientific notation


# load data frame
milk_facts <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv"")

# create manual palette
cols <-  c(""#578300"", ""#991200"", ""#1280A1"", ""yellow"", ""Tomato"",
           ""MediumSeaGreen"", ""DodgerBlue"", ""blue"", ""#FA8072"", 
           ""#FFC300"", ""#FF5733"", ""#C70039"", ""#900C3E"", ""#571845"",
           ""#D35400"", ""#000080"") 

# create plot
p <- milk_facts %>%
  gather(key = types_milk, value = avg_consumption, 2:ncol(.)) %>%
  filter(types_milk != ""fluid_milk"") %>% #do not include fluid_milk
  mutate(types_milk2 = recode(types_milk,
                                  ""butter"" = ""Butter"", 
          ""cheese_american"" = ""Cheese American"", 
          ""cheese_cottage"" = ""Cheese Cottage"", 
          ""cheese_other"" = ""Cheese Other"",
          ""dry_buttermilk"" = ""Dry Buttermilk"", 
          ""dry_nonfat_milk"" = ""Dry non-fat Milk"", 
          ""dry_whey"" = ""Dry Whey"",
         ""dry_whole_milk"" = ""Dry Whole Milk"",
         ""evap_cnd_canned_whole_milk"" = ""Evap. and Canned whole Milk"",
         ""evap_cnd_bulk_whole_milk"" = ""Evap. and Canned Bulk whole Milk"",
         ""evap_cnd_bulk_and_can_skim_milk"" = ""Evap. and Canned Bulk & skim Milk"",
         ""fluid_yogurt"" = ""Fluid Yogurt"",
         ""frozen_ice_cream_regular"" = ""Frozen Ice cream Regular"",
         ""frozen_ice_cream_reduced_fat"" = ""Frozen Ice Cream reduced Fat"",
         ""frozen_other"" = ""Frozen Other"",
         ""frozen_sherbet"" = ""Frozen Sherbet"",
         ""fluid_milk"" = ""Fluid Milk"")) %>%
  group_by(year, types_milk2) %>%
  summarize(avg_milk_consumed = mean(avg_consumption, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_milk_consumed)) +
  guides(color = FALSE) +
  geom_line(aes(colour = types_milk2), size = 3.5) +
  gghighlight(use_direct_label = FALSE, use_group_by = FALSE,
              unhighlighted_colour = alpha(""grey20"", 0.3)) +
  facet_wrap(vars(types_milk2)) + 
  labs(title = ""Average Consumption of Milk Products (lbs per person)"",
       subtitle = ""Not including fluid Milk"",
       x = NULL,
       y = NULL) +
  scale_x_continuous(breaks = c(seq(1975, 2015, by = 10)),
                     limits = c(1975, 2017)) +
  ggdark::dark_theme_bw() +
  scale_color_manual(values = cols) +
  theme(plot.title = element_text(family = ""Agency FB"", face = ""bold"",
                                  size = 30, hjust = 0.5),
        plot.subtitle = element_text(family = ""Agency FB"", face = ""bold"",
                                  size = 15, hjust = 0.5),
        plot.background = element_rect(fill = ""black""),
        panel.border = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_text(family = ""Agency FB"", size = 15),
        legend.key = element_blank(),
        axis.title = element_text(family = ""Agency FB"", size = 20, face = ""bold""),
        strip.background = element_blank(),
        strip.text = element_text(family = ""Agency FB"", size = 20, face = ""bold""),
        legend.text = element_text(family = ""Agency FB"", size = 15),
        legend.title = element_text(family = ""Agency FB"", size = 15))


ggsave(""milk_consumption.jpg"", p, units = ""cm"", 
       height = 25, width = 40, dpi = ""retina"")


           


",2019,5
131,1132,https://github.com/toscano84/TidyTuesday/blob/master/week6_2019/week6_2019.R,toscano84,TidyTuesday,week6_2019/week6_2019.R,"# week 6 Tidy Tuesday

library(tidyverse) # wrangle, visualization data
library(data.table) # load file
library(geojsonio) # open json files
library(cartogram) # create cartograms
library(broom) # tidy data frames
library(rgeos) # manipulation of spatial objects
library(viridis) # palette
library(extrafont) # add new fonts to R

# open data frame
hpi_usa <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-05/state_hpi.csv"")


# tidy the data frame
hpi_usa_tbl <- hpi_usa %>% 
  mutate(state_full_name = as.factor(case_when(state == ""AL"" ~ ""Alabama"",
                                               state == ""AK"" ~ ""Alaska"",
                                state == ""AR"" ~ ""Arkansas"",
                                state == ""AZ"" ~ ""Arizona"",
                                state == ""CA"" ~ ""California"",
                                state == ""CO"" ~ ""Colorado"",
                                state == ""CT"" ~ ""Connecticut"",
                                state == ""DC"" ~ ""District of Columbia"",
                                state == ""DE"" ~ ""Delaware"",
                                state == ""FL"" ~ ""Florida"",
                                state == ""GA"" ~ ""Georgia"",
                                state == ""HI"" ~ ""Hawaii"",
                                state == ""IA"" ~ ""Iowa"",
                                state == ""ID"" ~ ""Idaho"",
                                state == ""IL"" ~ ""Illinois"",
                                state == ""IN"" ~ ""Indiana"",
                                state == ""KS"" ~ ""Kansas"",
                                state == ""KY"" ~ ""Kentucky"",
                                state == ""LA"" ~ ""Louisiana"",
                                state == ""MD"" ~ ""Maryland"",
                                state == ""ME"" ~ ""Maine"",
                                state == ""MA"" ~ ""Massachusetts"",
                                state == ""MI"" ~ ""Michigan"",
                                state == ""MN"" ~ ""Minnesota"",
                                state == ""MO"" ~ ""Missouri"",
                                state == ""MS"" ~ ""Mississippi"",
                                state == ""MT"" ~ ""Montana"",
                                state == ""NC"" ~ ""North Carolina"",
                                state == ""NH"" ~ ""New Hampshire"",
                                state == ""ND"" ~ ""North Dakota"",
                                state == ""NE"" ~ ""Nebraska"",
                                state == ""NJ"" ~ ""New Jersey"",
                                state == ""NM"" ~ ""New Mexico"",
                                state == ""NV"" ~ ""Nevada"",
                                state == ""NY"" ~ ""New York"",
                                state == ""OH"" ~ ""Ohio"",
                                state == ""OK"" ~ ""Oklahoma"",
                                state == ""OR"" ~ ""Oregon"",
                                state == ""PA"" ~ ""Pennsylvania"",
                                state == ""RI"" ~ ""Rhode Island"",
                                state == ""SC"" ~ ""South Carolina"",
                                state == ""SD"" ~ ""South Dakota"",
                                state == ""TN"" ~ ""Tennessee"",
                                state == ""TX"" ~ ""Texas"",
                                state == ""UT"" ~ ""Utah"",
                                state == ""VA"" ~ ""Virginia"",
                                state == ""VT"" ~ ""Vermont"",
                                state == ""WA"" ~ ""Washington"",
                                state == ""WI"" ~ ""Wisconsin"",
                                state == ""WV"" ~ ""West Virginia"",
                                state == ""WY"" ~ ""Wyoming""))) %>% # create new variable as factor with the state name in a long format
  group_by(year, state_full_name) %>%
  summarize(price_index_avg = mean(price_index, na.rm = TRUE)) %>% # create variable with mean price index per year and state
  mutate(id_row = 1:n()) %>% # this step is important for the spread function to work
  spread(year, price_index_avg) %>% # from long to wide format
  mutate(change_price_index = (`2018` - `2000`) / `2000` * 100) %>% # create variable based on the changes in the house price index from 2000 to 2018
  select(-id_row)
  
  
#-----manipulate hexbin file of the USA------

# open hexbin file
# Hexbin available in https://team.carto.com/u/andrew/tables/andrew.us_states_hexgrid/public/map.
us_hexagonal <- geojson_read(""us_states_hexgrid.geojson"",  what = ""sp"")

# create variable region
us_hexagonal@data <- us_hexagonal@data %>% 
  mutate(google_name = gsub("" \\(United States\\)"", """", google_name))

# merge both data frames
us_hexagonal@data <- us_hexagonal@data %>% 
  left_join(., hpi_usa_tbl, by=c(""google_name""=""state_full_name""))


# create the cartogram using the change_price_index variable
cartogram <- cartogram(us_hexagonal, 'change_price_index')


# use the broom package to make the spatial object a data frame and then merge it with the cartogram
us_fortified <- tidy(cartogram, region = ""google_name"")
us_fortified <-  us_fortified %>% 
  left_join(. , cartogram@data, by=c(""id""=""google_name"")) 

# Important step to center the state labels
centers <- cbind.data.frame(data.frame(gCentroid(cartogram, byid=TRUE), 
                                       id=cartogram@data$iso3166_2))

#----plot-----#
#---alter the key legend with specific breaks---
# create breaks
new_breaks <- c(50,100,150,200,250,300)
# find the min for the labels
minvalue <- min(us_fortified$change_price_index, na.rm = T)

# create labels
labels <- c()
breaks <- c(minvalue, new_breaks)
# round the labels 
for(i in 1:length(breaks)){
  labels <- c(labels,round(breaks[i + 1], 2))
}

labels <- labels[1:length(labels)-1]
# create a new variable based on breaks
us_fortified$breaks <- cut(us_fortified$change_price_index, 
                     breaks = breaks, 
                     include.lowest = TRUE, 
                     labels = labels)

breaks_scale <- levels(us_fortified$breaks)
labels_scale <- rev(breaks_scale)

p <- ggplot() +
  geom_polygon(data = us_fortified, 
               aes(fill = breaks, 
                   x = long, y = lat, group = group) , 
               size=0.05, color=""grey40"") +
      # create manual scale based on the breaks 
  scale_fill_manual(values = rev(cividis(8)), # reverse cividis scale from the viridis palette
    breaks = rev(breaks_scale),
    name = ""Change of Price Index (%)"",
    drop = FALSE,
    labels = labels_scale,
    guide = guide_legend(
      direction = ""horizontal"",
      keyheight = unit(3.4, units = ""mm""),
      keywidth = unit(18, units = ""mm""),
      title.position = 'top',
      title.hjust = 0.5,
      label.hjust = 1,
      nrow = 1,
      byrow = TRUE,
      reverse = TRUE,
      label.position = ""bottom""
    )
  ) +
  geom_text(data=centers, aes(x=x, y=y, label=id), 
            color=""grey25"", size=5, alpha=0.6, family = ""Cooper Black"") +
  labs(title =  ""House Price Index"",
       subtitle = ""From 2000 to 2018"") +
  theme_void() +
  theme(panel.grid = element_blank(),
    legend.position = c(0.5, 0.87),
    axis.line = element_blank(),
    axis.text = element_blank(),
    plot.background = element_rect(fill = ""#333333"", color = NA), 
    panel.background = element_rect(fill = ""#333333"", color = NA), 
    legend.background = element_rect(fill = ""#333333"", color = NA),
    legend.text = element_text(size= 12, color = ""grey50"", family = ""Cooper Black""),
    plot.title = element_text(size= 28, hjust=0.5, color = ""grey50"", family = ""Cooper Black""),
    plot.subtitle = element_text(size= 14, hjust=0.5, vjust = -5, color = ""grey50"", family = ""Cooper Black""),
    legend.title = element_text(size= 16, hjust=0.5, vjust = -5, color = ""grey50"", family = ""Cooper Black"")) +
  coord_map()

ggsave(""hpi_index_changes.jpg"", p, units = ""cm"", 
       height = 25, width = 40, dpi = ""retina"")

",2019,6
132,1134,https://github.com/toscano84/TidyTuesday/blob/master/week7_2019/week7_2019.R,toscano84,TidyTuesday,week7_2019/week7_2019.R,"# week 7 Tidy Tuesday

library(tidyverse) # wrangle, visualization data
library(data.table) # load file
library(viridis) # palette
library(extrafont) # add new fonts to R
library(waffle) # create waffle plot



options(scipen = 999) # remove scientific notation

# import and load fonts
font_import()
loadfonts(device = ""win"")

# importing fontawesome font
fa_font <- tempfile(fileext = "".ttf"")
download.file(""http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/fonts/fontawesome-webfont.ttf?v=4.3.0"",
              destfile = fa_font, method = ""curl"")

font_import(paths = dirname(fa_font), prompt = FALSE)

fonts()
if (.Platform$OS.type == ""windows"") loadfonts(""win"")


# open data frame
fed_spend <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/fed_r_d_spending.csv"") 


glimpse(fed_spend)


# tidy the data frame
fed_spend_tbl <- fed_spend %>%
  group_by(year) %>%
  mutate(sum_rd = sum(rd_budget),
         perc_rd = round((rd_budget / sum_rd * 100),0)) %>%
  ungroup() %>%
  filter(year > 2014) %>% # only include the last 3 years
  select(year, department, perc_rd) %>%
  group_by(department) %>% # important step to remove NAs after changing the format from long to wide
  mutate(n = row_number()) %>% # this step is important for the spread function to work
  spread(department, perc_rd) %>%
  select(-n) %>%
  select_if(~mean(.) > 4) %>%
  select(year, DOD, HHS,NIH, everything())



# turn each year data into a vector
year_2015 <- unlist(fed_spend_tbl[1, 2:6])
year_2016 <- unlist(fed_spend_tbl[2, 2:6])
year_2017 <- unlist(fed_spend_tbl[3, 2:6])


# scale color manual
cols <- c(""#F8C932"", ""#E55B2F"", ""#A42C60"", ""#61156E"", ""#120A33"")

# set theme
theme_new <- ggdark::dark_theme_gray() +
  theme(plot.title = element_text(family = ""Cooper Black"", face = ""bold"",
                                  size = 18, hjust = 0.5),
        plot.subtitle = element_text(family = ""Cooper Black"", face = ""bold"",
                                     size = 14, hjust = 0.5),
        plot.background = element_rect(fill = ""grey10""),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(family = ""Cooper Black"", size = 12),
        axis.text = element_blank(),
        legend.key = element_blank(),
        legend.text = element_text(family = ""Cooper Black"", size = 10),
        legend.title = element_text(family = ""Cooper Black"", size = 10)) 

# Make waffle graph for each year
fed1 <- waffle(year_2015, rows = 5, size = 0.3, pad = 0.5, colors = cols, 
              use_glyph = ""dollar"", glyph_size = 8) +
  labs(title = ""Top 5 Departments per R&D Budget (%)"",
       subtitle = ""Year 2015"") +
  theme_new
  
fed2 <-  waffle(year_2016, rows = 5, size = 0.3, pad = 1, colors = cols, 
              use_glyph = ""dollar"", glyph_size = 8) +
  labs(subtitle = ""Year 2016"") +
  theme_new

fed3 <-  waffle(year_2017, rows = 5, size = 0.3, pad = 1, colors = cols, 
              use_glyph = ""dollar"", glyph_size = 8) +
  labs(subtitle = ""Year 2017"",
       x = ""Each dollar sign represents ~1% of the RD budget"") +
  theme_new


# iron function to arrange the three plots into one
iron(fed1, fed2, fed3)



",2019,7
133,1138,https://github.com/toscano84/TidyTuesday/blob/master/week1_2019/week1_2019.R,toscano84,TidyTuesday,week1_2019/week1_2019.R,"# week 1 2019 - TidyTuesday

# load needed libraries
library(tidyverse) # wrangle and visualize the data
library(lubridate) # deal with dates
library(viridis) # color palette
library(extrafont) # add fonts to R

# open file
tweets <- readRDS(""rstats_tweets.rds"")


# tidy the dataframe
tweets_tidy <- tweets %>%
  mutate(year = year(created_at), # create variable year 
         weekday = wday(created_at, label = TRUE),
         weekday = fct_relevel(weekday, ""Mon"",
                               ""Tue"",
                               ""Wed"",
                               ""Thu"",
                               ""Fri"",
                               ""Sat"",
                               ""Sun""), # create variable day of the week and relevel it
         month = month(created_at, label = TRUE), # create variable month
         week = week(created_at)) %>% # create variable week of the year
  filter(year > 2013) %>% # only include last 5 years
  count(year, weekday, week, month)

## plot ##
p <- tweets_tidy %>%
  ggplot(aes(week, weekday, fill = n)) +
  geom_tile(colour = ""grey30"") +
  facet_grid(year ~ month, scales = ""free"") + # divide by month and year
  scale_fill_viridis(name = ""Number of Tweets"",
                     option = ""plasma"", guide = guide_colorbar(
    direction = ""horizontal"",
    barheight = unit(3.5, units = ""mm""),
    barwidth = unit(50, units = ""mm""),
    title.position = 'top',
    title.hjust = 0.5,
    label.hjust = 0.5)) + # manipulate dimensions of the legend's scale 
  labs(title = ""#rstats Tweets in the last 5 years"",
       x = """", y = """") +
  theme(plot.title = element_text(family = ""Cooper Black"",
                                  size = 30, hjust = 0.5),
        plot.background = element_rect(fill = ""#d0d3d4""),
        panel.grid = element_blank(),
        panel.background = element_blank(),
        legend.background = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_text(colour = ""black"",
                                   family = ""Cooper Black"", size = 12),
        axis.text.x = element_blank(),
        legend.key = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(family = ""Cooper Black"", size = 18, face = ""bold""),
        legend.text = element_text(family = ""Cooper Black"", size = 15),
        legend.title = element_text(family = ""Cooper Black"", size = 15),
        legend.position = ""bottom"") 

ggsave(""rstats.jpg"", p, units = ""cm"", height = 20, width = 40, dpi = ""retina"")

",2019,1
134,1139,https://github.com/toscano84/TidyTuesday/blob/master/week3_2019/week3_2019.R,toscano84,TidyTuesday,week3_2019/week3_2019.R,"# week 3 tidytuesday 2019

library(tidyverse) # wrangle and visualize data
library(data.table) # in this case to open the file with the function fread
library(lubridate) # manipulate dates and times
library(ggdark) # theme for plots
library(extrafont) # add new fonts to base R

# open file
space_launches <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-15/launches.csv"")


# create new dataframe with new variables
space_launches_us <- space_launches %>%
  filter(state_code == ""US"") %>% #only include usa launches
  mutate(administration = case_when(between(launch_year, 1957, 1960) ~ ""Eisenhower"",
                                    between(launch_date, ""1961-01-20"", ""1963-11-22"") ~ ""Kennedy"",
                                    between(launch_date, ""1963-11-23"", ""1969-01-19"") ~ ""Johnson"",
                                    between(launch_date,""1969-01-20"", ""1974-08-09"") ~ ""Nixon"",
                                    between(launch_date,""1974-08-10"", ""1977-01-19"") ~ ""Ford"",
                                    between(launch_date,""1977-01-20"", ""1981-01-19"") ~ ""Carter"",
                                    between(launch_date,""1981-01-20"", ""1989-01-19"") ~ ""Reagan"",
                                    between(launch_date,""1989-01-20"", ""1993-01-19"") ~ ""Bush I"",
                                    between(launch_date,""1993-01-20"", ""2001-01-19"") ~ ""Clinton"",
                                    between(launch_date,""2001-01-20"", ""2009-01-19"") ~ ""Bush II"",
                                    between(launch_date,""2009-01-20"", ""2017-01-19"") ~ ""Obama"",
                                    between(launch_date,""2017-01-20"", ""2019-01-17"") ~ ""Trump""
                                    ), # create variable based on presidential administrations
         party = case_when(administration %in% c(""Kennedy"", ""Johnson"", ""Carter"", ""Clinton"", ""Obama"") ~ ""Democratic"",
                           administration %in% c(""Eisenhower"", ""Nixon"", ""Ford"", ""Reagan"", ""Bush I"", ""Bush II"",
                                                 ""Trump"") ~ ""Republican"")) %>% # party variable
  drop_na(administration) # delete missing values in the variable administration



##----plot------##
# function created to have separate breaks due to the use of facets in the plot
breaks_created <- function(x) { 
  if (max(x) < 1961) seq(1957, 1960, 1) 
  else if (max(x) < 1964) seq(1960, 1964, 1)
  else if (max(x) < 1969) seq(1963, 1969, 1)
  else if (max(x) < 1975) seq(1969, 1974, 1)
  else if (max(x) < 1977) seq(1974, 1977, 1)
  else if (max(x) < 1981) seq(1977, 1981, 1)
  else if (max(x) < 1989) seq(1981, 1989, 1)
  else if (max(x) < 1994) seq(1989, 1994, 1)
  else if (max(x) < 2001) seq(1993, 2001, 1)
  else if (max(x) < 2010) seq(2001, 2009, 1)
  else if (max(x) < 2018) seq(2009, 2017, 1)
  else (seq(2017, 2018, 1))}

# plot creation
p <- space_launches_us %>%
  group_by(launch_year, party, administration) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = launch_year, y = n, fill = party)) +
  geom_area() +
  dark_theme_grey() +
  labs(title = ""Space Launches during each Presidential Administration"", 
       subtitle =, x = """", y = """") +
  scale_fill_manual(name = ""Party"", values = c(""navyblue"", ""red""), 
                    labels = c(""Democratic"", ""Republican"")) +
  scale_x_continuous(breaks = breaks_created) +
  theme(plot.title = element_text(family = ""Agency FB"", face = ""bold"",
                                  size = 30, hjust = 0.5),
        plot.background = element_rect(fill = ""grey10""),
        panel.background = element_blank(),
        panel.grid.major = element_line(color = ""grey20"", size = 0.2),
        panel.grid.minor = element_line(color = ""grey20"", size = 0.2),
        legend.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_text(family = ""Agency FB"", size = 15),
        legend.key = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(family = ""Agency FB"", size = 20, face = ""bold""),
        legend.text = element_text(family = ""Agency FB"", size = 15),
        legend.title = element_text(family = ""Agency FB"", size = 15)) +
  facet_wrap(vars(fct_reorder(administration, launch_year)), ncol = 3, scales = ""free_x"")

p

ggsave(""plot_us_space.jpg"", p, units = ""cm"", 
       height = 25, width = 40, dpi = ""retina"")




  
 
",2019,3
135,1140,https://github.com/toscano84/TidyTuesday/blob/master/week8_2019/week8_2019.R,toscano84,TidyTuesday,week8_2019/week8_2019.R,"# week 8 Tidy Tuesday

library(tidyverse) # wrangle, visualization data
library(data.table) # load file
library(highcharter) # interactive data visualizations

options(scipen = 999)

# open data
phd <- fread(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv"")




# top 20 fields
phd_per <- phd  %>%
  group_by(field) %>%
  summarize(phd_number = sum(n_phds, na.rm = TRUE)) %>%
  mutate(phd_perc = phd_number / sum(phd_number) * 100) %>%
  top_n(20, phd_perc) %>%
  arrange(desc(phd_perc))


# highcharter plot
hchart(phd_per, ""bar"",
       hcaes(x = fct_reorder(field, phd_perc),
             y = phd_perc)) %>%
  hc_add_theme(hc_theme_chalk()) %>%
  hc_title(text = ""Percentage of PhDs: Top 20 Fields"", margin = 10, 
           fontSize = ""50px"") %>%
  hc_xAxis(title = list(text = NULL)) %>% 
  hc_yAxis(title = list(text = ""Percentage"")) %>% 
  hc_subtitle(text = ""From 2008 to 2017"") %>%
  hc_credits(enabled = TRUE,
             text = ""Tidy Tuesday Week 8"",
             style = list(
               fontSize = ""14px""
             )
  )
  


",2019,8
136,1646,https://github.com/lizwillow/TidyTuesday/blob/master/TT.2019.01.01/week20190101.Rmd,lizwillow,TidyTuesday,TT.2019.01.01/week20190101.Rmd,"---
output: github_document
---

### #First #TidyTuesday of 2019

This is the code behind an analysis of the ""#rstats and #TidyTuesday Tweets from rtweet"" dataset from the [#tidytuesday project](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,error=FALSE)

library(tidyverse)
library(scales)
library(broom)
library(ggthemes)
library(plotly)
library(here)
library(rtweet)

theme_set(theme_light())
```

```{r, include=FALSE}

githubURL <- (""https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-01-01/tidytuesday_tweets.rds?raw=true"")
download.file(githubURL,""tidytuesday_tweets.rds"")
tidytuesday <- read_rds(""tidytuesday_tweets.rds"")


githubURL <- (""https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-01-01/rstats_tweets.rds?raw=true"")
download.file(githubURL,""rstats_tweets.rds"")
rstats <- read_rds(""rstats_tweets.rds"")

```

Whose #rstats tweets were retweeted the most in 2018?

```{r}

library(ghibli)

toppeople <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>% 
  summarise(total_retweets = sum(retweet_count),
            n=n()) %>%
  arrange(desc(total_retweets)) %>%
  head(5)

myColors <- as.list(ghibli_palette(""PonyoMedium"",n=5))
names(myColors) <- toppeople$screen_name

toppeople %>% 
  mutate(screen_name = fct_reorder(screen_name,total_retweets)) %>%
  ggplot(aes(y = total_retweets, x = screen_name, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Who had the most retweeted #rstats tweets in 2018?"",
       y = ""Number of retweets of #rstats tweets in 2018"",
       x = """",
       caption = ""(Data from the rtweet package)"") +
  theme(legend.position = ""none"",
        text = element_text(family = ""Andale Mono""),
        plot.margin = unit(c(.5,1,.5,0),""cm"")) +
  scale_fill_manual(name = ""screen_name"",values = myColors) +
  geom_text(aes(label = paste(n,c(rep(""tweets"",5)))), 
            position = position_stack(vjust = 0.5),
            family = ""Andale Mono"",
            size = 2,
            col=""white"")

#ggsave(filename = ""rstatsretweets.png"", path = here(""Week20190101_files""),width = 7, height = 4)

```

Whose #rstats tweets were liked the most in 2018?

```{r}

library(ghibli)

toppeople <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(total_favs = sum(favorite_count),
            n=n()) %>%
  arrange(desc(total_favs)) %>%
  head(5)

toppeople %>% 
  mutate(screen_name = fct_reorder(screen_name,total_favs)) %>%
  ggplot(aes(y = total_favs, x = screen_name, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Who had the most liked #rstats tweets in 2018?"",
       y = ""Number of favorites of #rstats tweets in 2018"",
       x = """",
       caption = ""(Data from the rtweet package)"") +
  theme(legend.position = ""none"",
        text = element_text(family = ""Andale Mono""),
        plot.margin = unit(c(.5,1,.5,0),""cm"")) +
  scale_fill_manual(name = ""screen_name"",values = myColors) +
  geom_text(aes(label = paste(n,c(rep(""tweets"",5)))), 
            position = position_stack(vjust = 0.5),
            family = ""Andale Mono"",
            size = 2,
            col=""white"")

#ggsave(filename = ""rstatslikes.png"", path = here(""Week20190101_files""),width = 7, height = 4)


```

Who had the most liked and retweeted #rstats tweets of 2018?

```{r}
topliked <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text,favorite_count) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(total_fav_ret = sum(retweet_count,favorite_count)) %>%
  arrange(desc(total_fav_ret)) %>%
  head(10)

topliked %>% 
  mutate(screen_name = fct_reorder(screen_name,total_fav_ret)) %>%
  ggplot(aes(y = total_fav_ret, x = screen_name, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Users with the most retweeted and favorited #rstats tweets in 2018"",
       y = ""Total #rstats retweets and favorites in 2018"",
       x = """") +
  theme(legend.position = ""none"")

#ggsave(filename=""rstats.png"",path=here(""Week20190101_files""),width = 9, height = 6)
```

Who had the most liked and retweeted #tidytuesday tweets of 2018?

```{r}
topliked <- tidytuesday %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text,favorite_count) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(total_fav_ret = sum(retweet_count,favorite_count)) %>%
  arrange(desc(total_fav_ret)) %>%
  head(10)

topliked %>% 
  mutate(screen_name = fct_reorder(screen_name,total_fav_ret)) %>%
  ggplot(aes(y = total_fav_ret, x = screen_name, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Users with the most retweeted and favorited #TidyTuesday tweets in 2018"",
       y = ""Total #TidyTuesday retweets and favorites in 2018"",
       x = """") +
  theme(legend.position = ""none"")

#ggsave(filename=""tidytuesday.png"",path=here(""Week20190101_files""),width = 9, height = 6)
```

What were the most retweeted and favorited #tidytuesday tweets of 2018?

```{r}
# install.packages(""devtools"")
#devtools::install_github(""hadley/emo"")
#devtools::install_github(""GuangchuangYu/emojifont"")

toptweet <- tidytuesday %>% 
  select(screen_name,created_at,is_retweet,favorite_count, retweet_count,text,status_id) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  mutate(fav_ret = retweet_count+favorite_count) %>%
  arrange(desc(fav_ret)) %>%
  head(10)

toptweet %>%
  mutate(tex = paste0(str_sub(text,1,60),"" ... ("",format(created_at,format = ""%b %d %Y""),"")""),
         screen_name = paste0(""@"",screen_name)) %>% 
  mutate(tex = fct_reorder(tex,fav_ret)) %>%
  ggplot(aes(y = fav_ret, x = tex, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = ""The most retweeted and favorited #TidyTuesday tweets of 2018"",
       y = ""Number of retweets and favorites combined"",
       x = """",
       fill="""",
       caption = ""Data from the rtweet package."") +
  theme(legend.position = ""bottom"",
        axis.text=element_text(size=7),
        legend.text=element_text(size=7),
        title=element_text(size=8),
        text=element_text(family = ""Tahoma"")) 

#ggsave(filename=""tidytuesdaytweets.png"",path=here(""Week20190101_files""),width = 8, height = 4)

```



How have the number of tweets changed over time?

```{r}

rstats %>% 
  ts_plot(by = ""weeks"")

tidytuesday %>%
  ts_plot(by = ""days"")

```

Whose #rstats tweets were retweeted the most in 2018 on average?

```{r}

toppeople <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>% 
  summarise(mean_retweets = mean(retweet_count),
            n = n()) %>%
  filter(n>19) %>%
  arrange(desc(mean_retweets)) %>%
  head(7)

toppeople %>% 
  mutate(screen_name = fct_reorder(screen_name,mean_retweets)) %>%
  ggplot(aes(y = mean_retweets, x = screen_name, fill=n)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Whose #rstats tweets were retweeted most on average in 2018?"",
       subtitle = ""(of those with at least 10 tweets)"",
       y = ""Mean number of retweets of #rstats tweets in 2018"",
       x = """",
       caption = ""(Data from the rtweet package)"") +
  theme(legend.position = ""none"",
        text = element_text(family = ""Andale Mono""),
        plot.margin = unit(c(.5,1,.5,0),""cm""),
        axis.text = element_text(size = 10)) +
  geom_text(aes(label = paste(n,c(rep(""tweets"",7)))), 
            position = position_stack(vjust = 0.5),
            family = ""Andale Mono"",
            size = 3,
            col=""white"") +
  scale_fill_gradient(low = myColors[1], high = myColors[5])

#ggsave(filename = ""rstatsavgretweets.png"", path = here(""Week20190101_files""),width = 8.5, height = 4)

```

Whose #rstats tweets were liked the most in 2018?

```{r}

library(ghibli)

toppeople <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(mean_favs = mean(favorite_count),
            n=n()) %>%
  filter(n>19) %>%
  arrange(desc(mean_favs)) %>%
  head(7)

toppeople %>% 
  mutate(screen_name = fct_reorder(screen_name,mean_favs)) %>%
  ggplot(aes(y = mean_favs, x = screen_name, fill=n)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(title = ""Whose #rstats tweets were liked most on average in 2018?"",
       subtitle = ""(of those with at least 10 tweets)"",
       y = ""Mean number of favorites for #rstats tweets in 2018"",
       x = """",
       caption = ""(Data from the rtweet package)"") +
  theme(legend.position = ""none"",
        text = element_text(family = ""Andale Mono""),
        plot.margin = unit(c(.5,1,.5,0),""cm""),
        axis.text = element_text(size = 10)) +
  scale_fill_gradient(low = myColors[1], high = myColors[5]) +
  geom_text(aes(label = paste(n,c(rep(""tweets"",7)))), 
            position = position_stack(vjust = 0.5),
            family = ""Andale Mono"",
            size = 3,
            col=""white"")

#ggsave(filename = ""rstatsavglikes.png"", path = here(""Week20190101_files""),width = 8.5, height = 4)


```

```{r,echo=FALSE}
knitr::knit_exit()
```

What were the most liked tweets of 2018?

```{r}

toptweet <- tidytuesday %>% 
  select(screen_name,created_at,is_retweet,favorite_count, retweet_count,text,status_id) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  arrange(desc(favorite_count)) %>%
  head(10)

toptweet %>%
  mutate(tex = paste0(str_sub(text,1,60),"" ... ("",format(created_at,format = ""%b %d %Y""),"")""),
         screen_name = paste0(""@"",screen_name)) %>% 
  mutate(tex = fct_reorder(tex,favorite_count)) %>%
  ggplot(aes(y = favorite_count, x = tex, fill=screen_name)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = ""The most liked #TidyTuesday tweets of 2018"",
       y = ""Number of likes"",
       x = """",
       fill="""",
       caption = ""Data from the rtweet package."") +
  theme(legend.position = ""bottom"",
        axis.text=element_text(size=7),
        legend.text=element_text(size=7),
        title=element_text(size=8),
        text=element_text(family = ""Tahoma"")) 

#ggsave(filename=""tidytuesdaytweets.png"",path=here(""Week20190101_files""),width = 8, height = 4)

```
",2019,1
137,1649,https://github.com/rakash/TidyTuesday-Social-data-project/blob/master/week1_2019.R,rakash,TidyTuesday-Social-data-project,week1_2019.R,"library(tidyverse)
library(scales)
library(broom)
library(ggthemes)
library(plotly)
library(here)
library(rtweet)

install.packages(""yaml"")
install.packages(""rtweet"")

install.packages(""htmlwidgets"")

trace(utils:::unpackPkgZip, edit=TRUE)

update.packages()

.libPaths()

#githubURL <- (""https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-01-01/tidytuesday_tweets.rds?raw=true"")
#download.file(githubURL,""tidytuesday_tweets.rds"")
tidytuesday <- read_rds(""C:/Users/AKASHR/Documents/tidytuesday_tweets.rds"")


#githubURL <- (""https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-01-01/rstats_tweets.rds?raw=true"")
#download.file(githubURL,""rstats_tweets.rds"")
rstats <- readRDS(""C:/Users/AKASHR/Documents/rstats_tweets.rds"")
rstats

## 1. Whose #rstats tweets were retweeted the most in 2018?

install.packages(""ghibli"")
library(ghibli)

toppeople <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>% 
  summarise(total_retweets = sum(retweet_count),
            n=n()) %>%
  arrange(desc(total_retweets)) %>%
  head(5)

myColors <- as.list(ghibli_palette(""PonyoMedium"",n=5))
names(myColors) <- toppeople$screen_name

View(rstats)
View(toppeople)

library(ggplot2)
theme_set(theme_classic())

# Plot

ggplot(toppeople, aes(x=screen_name, y=n, label=n))+ 
  geom_point(stat='identity', fill=""black"", size=12)  +
  geom_segment(aes(y = 0, 
                   x = `screen_name`, 
                   yend = n, 
                   xend = `screen_name`), 
               color = ""black"") +
  geom_text(color=""white"", size=4) +
  labs(title=""Lollipop Chart"", 
       subtitle=""Whose #rstats tweets were retweeted the most in 2018?"")
       + 
  coord_flip()


## 2. Whose #rstats tweets were liked the most in 2018?


toplikes <- rstats %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(total_favs = sum(favorite_count),
            n=n()) %>%
  arrange(desc(total_favs)) %>%
  head(5)


View(toplikes)

theme_set(theme_classic())

# Plot
ggplot(toplikes, aes(x=screen_name, y=total_favs)) + 
  geom_point(col=""tomato2"", size=3) + # Draw points
  geom_segment(aes(x=screen_name, 
                   xend=screen_name, 
                   y=min(total_favs), 
                   yend=max(total_favs)), 
               linetype=""dashed"", 
               size=0.1) +   # Draw dashed lines
  labs(title=""Dot Plot"", 
       subtitle=""Whose #rstats tweets were liked the most in 2018"", 
       caption=""source: mpg"") +  
  coord_flip()


## 3. Who had the most liked and retweeted #tidytuesday tweets of 2018?

View(tidytuesday)


toplikedtt <- tidytuesday %>% 
  select(screen_name,created_at,followers_count,is_retweet,favorite_count, retweet_count,text,favorite_count) %>%
  filter(created_at >= as.Date(""2018-01-01""),
         is_retweet == FALSE)  %>%
  group_by(screen_name) %>%
  summarise(total_favr = sum(retweet_count,favorite_count)) %>%
  arrange(desc(total_favr)) %>%
  head(10)

View(toplikedtt)

tt5 <- toplikedtt %>% head(5)
View(tt5)

library(ggplot2)
library(gganimate)
library(gapminder)
theme_set(theme_dark())  # pre-set the bw theme.

g <- ggplot(tt5, aes(screen_name, total_favr)) + 
  labs(subtitle=""Who had the most liked and retweeted #tidytuesday tweets of 2018?"",
       title=""Bubble chart"")

g + geom_jitter(aes(col=screen_name, size=total_favr)) + 
  geom_smooth(aes(col=screen_name), method=""lm"", se=F)

## 4. Total #rstats and #tidytuesday tweets over time in 2018 ?


rstats1 <- rstats %>% mutate(tweet_month = as.POSIXct(rstats$created_at))

rstats1$tweet_month <- format(rstats1$tweet_month,""%B"")

View(rstats1)
View(rtotal)

rtotal <- rstats1 %>% 
  select(created_at, tweet_month) %>%
  filter(created_at >= as.Date(""2018-01-01"")) %>%
  group_by(tweet_month) %>%
  summarise(month_tweets=n()) %>%
  arrange(desc(month_tweets))# %>%

theme_set(theme_light())

ggplot(rtotal, aes(tweet_month, month_tweets)) +
  geom_linerange(
    aes(x = tweet_month, ymin = 0, ymax = month_tweets), 
    color = ""lightgray"", size = 1.5
  )+
  geom_point(aes(color = tweet_month), size = 2)
  #ggpubr::color_palette(""jco"")


## TIdy tuesday 

View(tidytuesday)

ttotal <- tidytuesday %>% mutate(tweet_month = as.POSIXct(tidytuesday$created_at))

ttotal$tweet_month <- format(ttotal$tweet_month,""%B"")

View(ttotal)

ttotal <- ttotal %>% 
  select(created_at, tweet_month) %>%
  filter(created_at >= as.Date(""2018-01-01"")) %>%
  group_by(tweet_month) %>%
  summarise(month_tweets=n()) %>%
  arrange(desc(month_tweets))# %>%

View(ttotal)

theme_set(theme_dark())

# plot 
ggplot(ttotal , aes(x = tweet_month, y = month_tweets)) +
  geom_bar(fill = ""#0073C2FF"", stat = ""identity"") +
  geom_text(aes(label = month_tweets), vjust = -0.3) + 
  labs(title=""which month had the most #tidytuesday tweets of 2018?"")
",2019,1
138,1747,https://github.com/tbobin/tidytuesday/blob/master/src/20180514_tidytuseday_week7.R,tbobin,tidytuesday,src/20180514_tidytuseday_week7.R,"

library(tidyverse)
library(purrrlyr)
library(viridis)



df_sw_raw <- read.csv(""./Data/week7_starwars.csv"")

# replace all """" with NA
df_sw_raw[df_sw_raw == """" ] <- NA


# Not Fans #

# if not seen any of the films not a Fan
# if not self considered as fan not a Fan

# Star Wars: Episode I The Phantom Menace
# Star Wars: Episode II Attack of the Clones
# Star Wars: Episode III Revenge of the Sith
# Star Wars: Episode IV A New Hope
# Star Wars: Episode V The Empire Strikes Back
# Star Wars: Episode VI Return of the Jedi


df_sw_not_Fans <- df_sw_raw %>% 
  mutate(Fan = ifelse(
    (Have.you.seen.any.of.the.6.films.in.the.Star.Wars.franchise. == ""No"") |
      (Do.you.consider.yourself.to.be.a.fan.of.the.Star.Wars.film.franchise. == ""No"") |
      is.na(Do.you.consider.yourself.to.be.a.fan.of.the.Star.Wars.film.franchise.), ""No"", ""Yes""
    )) %>% 
  filter(Fan == ""No"") %>% 
  mutate(have.you.seen.Episode.I = ifelse(!is.na(Which.of.the.following.Star.Wars.films.have.you.seen..Please.select.all.that.apply.),
                                          1, 0 ),
         have.you.seen.Episode.II = ifelse(!is.na(X), 1, 0),
         have.you.seen.Episode.III = ifelse(!is.na(X.1), 1, 0),
         have.you.seen.Episode.IV = ifelse(!is.na(X.2), 1, 0),
         have.you.seen.Episode.V = ifelse(!is.na(X.3), 1, 0),
         have.you.seen.Episode.VI = ifelse(!is.na(X.4), 1, 0)) %>% 
  select(RespondentID, Fan:have.you.seen.Episode.VI)



df_Fan_seen_old <- df_sw_not_Fans %>% 
  select(have.you.seen.Episode.I:have.you.seen.Episode.III) %>% 
  by_row(sum, .collate = ""cols"", .to = ""sum_seen_old"")

df_Fan_seen_new <- df_sw_not_Fans %>% 
  select(have.you.seen.Episode.IV:have.you.seen.Episode.VI) %>% 
  by_row(sum, .collate = ""cols"", .to = ""sum_seen_new"")


df_sw_not_Fans <- cbind(df_sw_not_Fans, 
                        new = df_Fan_seen_new$sum_seen_new, 
                        old = df_Fan_seen_old$sum_seen_old) %>% 
  mutate(`one of episodes I - III and one of IV - VI` = ifelse(new > 0 & old > 0 , 1, 0),
         `only one of episodes I - III` = ifelse(new > 0 & old == 0 , 1, 0),
         `only one of episodes IV - VI`= ifelse(new == 0 & old > 0 , 1, 0),
         `no one` = ifelse(new == 0 & old == 0 , 1, 0))

df_sw_not_Fans %>% 
  select(RespondentID, `one of episodes I - III and one of IV - VI`:`no one`) %>% 
  gather(`one of episodes I - III and one of IV - VI`:`no one`,
         key = ""seen"", value = ""count"") %>% 
  mutate(seen = factor(seen, levels = c(""one of episodes I - III and one of IV - VI"", 
                                        ""only one of episodes I - III"", 
                                        ""only one of episodes IV - VI"",
                                        ""no one""), ordered = T)) %>% 
  filter(count > 0) %>% 
  ggplot(aes(x = seen, fill = seen)) + 
  geom_bar(stat = ""count"") +
  geom_text(aes(label = ..count..), stat = ""count"", vjust = -1) +
  viridis::scale_fill_viridis(discrete = T ) +
  scale_y_continuous(limits = c(0, 360)) +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        legend.position = """", 
        axis.text.y = element_blank()) +
  labs(y = """", x = """",
       title = ""Which episodes have people seen, that are not a fan of the franchise?"",
       subtitle = ""A fan is someone who constider himself as a fan, all others are not seen as a fan.
Most of the people, who consider themself not as as Fan and have seen at least one movie, have seen at least one of 
the new and one of the old episodes. The fewest people of this group saw only the old episodes."",
       caption = ""@T_bobin\nsource: https://github.com/rudeboybert/fivethirtyeight"") 
  



",2018,7
139,1748,https://github.com/tbobin/tidytuesday/blob/master/src/20180511_tidytuseday_week6.R,tbobin,tidytuesday,src/20180511_tidytuseday_week6.R,"

library(tidyverse)
library(readxl)
library(scales)
library(geojsonio)
library(broom)
library(viridis)
library(rgeos)
library(skimr)


# read in data
df_cof_raw <- readxl::read_xlsx(paste0(here::here(),""/data/week6_coffee_chains.xlsx""))

# let's start with stores in the US by Brand

df_cof_all <- df_cof_raw %>% 
  filter(Country == ""US"") %>% 
  group_by(`State/Province`, Brand) %>% 
  count()

# let's take a look at the data
df_cof_all %>% 
  ggplot(aes(n)) +
  geom_histogram(binwidth = 100)

df_cof_all %>% ungroup %>% select(-(`State/Province`)) %>% skim(n)

## From r-graph-gallery: https://www.r-graph-gallery.com/328-hexbin-map-of-the-usa/
# Hexbin available in the geojson format here: https://team.carto.com/u/andrew/tables/andrew.us_states_hexgrid/public/map. Download it and then:
spdf <- geojson_read(paste0(here::here(),""/data/us_states_hexgrid.geojson""),  what = ""sp"")

# I need to 'fortify' the data to be able to show it with ggplot2 (we need a data frame format)
# spdf@data = spdf@data %>% mutate(google_name = gsub("" \\(United States\\)"", """", google_name))
spdf_fortified <- tidy(spdf, region = ""iso3166_2"")


# join data with spital data
spdf_fortified <- spdf_fortified %>% 
  left_join(df_cof_all, by=c(""id"" = ""State/Province""))

#
centers <- cbind.data.frame(data.frame(gCentroid(spdf, byid=TRUE), id=spdf@data$iso3166_2))

# Prepare binning
spdf_fortified$bin = cut( spdf_fortified$n , breaks=c(seq(0,500,100), Inf), 
                          labels=c(""0-100"", ""101-200"", ""201-300"", ""301-400"", ""401-500"", ""500+"" ), include.lowest = TRUE )



# lets plot the Starbucks map
spdf_fortified %>% 
  filter(Brand == ""Starbucks"") %>% 
  ggplot(aes()) +
  geom_polygon(aes(fill = bin, x = long, y = lat, group = group) , size=0, alpha=0.9) +
  geom_text(data=centers, aes(x=x, y=y, label=id), color=""white"", size=3, alpha=0.6) +
  scale_fill_viridis(option = ""viridis"", discrete=TRUE,
                     name = """") +
  theme_minimal() +
  labs(title = ""Starbucks Coffee stores per State"",
       caption = ""@T_Bobin \nsource: kaggle.com"")+
  theme(panel.border=element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        legend.position = ""bottom"") +
  coord_map() 

#ggsave(""/graphs/20180512_tidyTuseday_week_6.png"", width = 10, dpi = 600)

",2018,6
