---
title: "TidyTuesday Survey Development"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Packages ----
library(tidyverse)
library(rtweet)
library(gsheet)
library(lubridate)
```

# Data

## Data Source

Data for this survey came from a TAGS archive of tweets with the hashtage #TidyTuesday. Tweets were collected from 2019 to 2020 and represent about 1.5 years of #TidyTuesday tweets. Data was processed in May of 2020.

```{r}
tags_data <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1yucj-9lR-yaac2A2tuUCPcURZ90-adEKl87mlhdzSMk/edit#gid=400689247") 

# Use statuses from TAGS to get full Twitter dataset ----
twitter_data <- lookup_statuses(tags_data$id_str)

```

## Inclusion Criteria

To develop a list of *potential* participants, the Twitter archive was processed to remove any tweets that were retweets and that did not have any media (e.g. #TidyTuesday contribitions) associated with it. The data was then organized by screen name and filtered to contain only those who contributed 7 or more times. A total of 64 individuals were identified.

```{r}
# use data to develop a list of possible participants to contact ----
possible_participants <- twitter_data %>%
  filter(is_retweet == FALSE) %>% #select initial tweets not retweets
  filter(!is.na(ext_media_url)) %>% #select individuals who contributed an image
  mutate(year = year(created_at)) %>%
  group_by(year) %>%
  mutate(week = week(created_at),
         year_week = paste0(year, " ", week)) %>%
  ungroup() %>%
  group_by(screen_name) %>%
  summarize(n = n()) %>%
    arrange(desc(n)) %>%
  filter(n > 6)
```


Initial contributions of #TidyTuesday participants were then organized into a small database and analyzed by the authors. The authors looked at the initial contributions and decided to include or exclude participants based on whether contributions were relevant (include) and whether they showed already-established strong data visualization skills (exclude). The purpose of this exercise was to establish a participant pool of contributors from which to develop survey items that could possibly show the development of data visualization skills across time. Therefore, contributions that showed little skill (in terms of clarity, complexity, and customization in the visualizations) served as a baseline of growth.

For calibration purposes, the authors rated the first three (?) contributions together. They then independenly rated 1/3 of the remaining contributions each. Finally, any contributions marked "unsure" were discussed and re-rated as a group.

```{r}
# create list of potential participants for rating
filtered_twitter_data <- twitter_data %>%
  filter(is_retweet == FALSE) %>% #select initial tweets not retweets
  filter(!is.na(ext_media_url)) %>%
  filter(screen_name %in% possible_participants$screen_name) %>%
  mutate(year = year(created_at)) %>%
  group_by(year) %>%
  mutate(week = week(created_at),
         year_week = paste0(year, " ", week)) %>%
  ungroup() %>%
  unnest_wider(ext_media_url, names_sep = "_") %>%
  janitor::clean_names() %>%
  mutate(profile = paste0("http://www.twitter.com/", screen_name)) %>%
  group_by(screen_name) %>%
  mutate(tweet_number = row_number()) %>%
  select(screen_name, profile, tweet_number, created_at, year, week, year_week, text, ext_media_url_1, ext_media_url_2, ext_media_url_3, ext_media_url_4, status_id)
  
write.csv(filtered_twitter_data, file="Updated analysis for ICLS/Tidy Tuesday Participants.csv")
```

Finally, participants chosen for inclusion were contacted for permission to use their contributions in our survey. Twenty-one participants agreed to allow us to include their contributions in the survey. For those who agreed, the tweet archive was filtered to contain only those users. The first two, middle two, and final two tweets in the archive for each user were then downloaded to include in the survey items. Any non-relevant image were deleted.


```{r}
# get list of participants we chose for inclusion
selected_participants <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1uEbxH83Ww-AJtZl4M_XJubbqg9P_nRyhlVOwf5bcJOQ/edit#gid=1191274417") %>%
  filter(`Include?` == "Include") %>%
  filter(`Agree to Participate` == "Agree") %>%
  select(screen_name)

# get files ready for dl
participant_images <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1uEbxH83Ww-AJtZl4M_XJubbqg9P_nRyhlVOwf5bcJOQ/edit#gid=821913014") %>%
  filter(screen_name %in% selected_participants$screen_name) %>%
  select(screen_name, tweet_number:week, ext_media_url_1:status_id) %>%
  group_by(screen_name) %>%
  pivot_longer(ext_media_url_1:ext_media_url_4,
               names_to="media", values_to="url") %>%
  drop_na(url) %>%
  mutate(
    filename=paste0("survey/images/", screen_name, "-", tweet_number, "-", basename(url)),
    stat = case_when(
      tweet_number == 1 ~"1",
      tweet_number == 2 ~"2",
      tweet_number == round(median(tweet_number), digits=0)-1 ~"3",
      tweet_number == round(median(tweet_number), digits=0) ~"4",
      tweet_number == max(tweet_number)-1 ~ "5",
      tweet_number == max(tweet_number) ~ "6"
    )) %>%
  drop_na(stat)

# # download images
# for (i in 1:length(participant_images$url)){
#   
#   skip_to_next <- FALSE
#   
#   tryCatch(
#   download.file(participant_images$url[i], destfile =  participant_images$filename[i], mode = 'wb'), error = function(e) { skip_to_next <<- TRUE})
#   
#   if(skip_to_next) { next }     
# }
```


# Survey Development

## Calibration Items

In addition to #TidyTuesday participants' contributions, two calibration items were developed. The items were developed to help survey takers implicitly recognize a "poor" data visualization and an "excellent" data visualization. In addition, these images served as a means to measure the convergent and divergent validity of the survey. Items rated poorly should correlate with low rating of the "poor" calibration image. Conversely, items rated highly should not correlate with this image. Similar relationships should also exist between the "excellent" calibration image and other survey items.

The poor data visualization was developed to use very little analysis (a basic count of data points), default colors, no customization of labels or theme, and, in general, serve as a poor communicator of data. The "excellent" visualization, on the other hand, used some analysis (distribution of data points and demarcation of median points), customized colors of shapes and text, and clear communication through the use of text and axis labels. Both images were developed based on the Palmer Penguins dataset (Horst, 2020). The "excellent" calibration image was adapted from a well-liked #TidyTuesday contribution by Cedric Sherer (2020).

```{r}
library(palmerpenguins)
library(colorspace)

# bad plot
penguins %>%
  ggplot()+
  geom_col(aes(y=island, x=body_mass_g, fill=sex))+
  ggtitle("Penguin Sex and Bodymass by Island")

# good plot 
df_penguins <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv') %>% 
  mutate(species = if_else(species == "Adelie", "Ad√©lie", species))


df_rect <-
  tibble(
    xmin = c(-Inf, 2.46, 3.27),
    xmax = c(Inf, Inf, Inf),
    ymin = c(3, 2, 1),
    ymax = c(Inf, Inf, Inf)
  )
df_peng_iqr <- 
  df_penguins %>% 
  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %>% 
  filter(!is.na(bill_ratio)) %>% 
  group_by(species) %>% 
  mutate(
    median = median(bill_ratio),
    q25 = quantile(bill_ratio, probs = .25),
    q75 = quantile(bill_ratio, probs = .75),
    n = n()
  ) %>% 
  ungroup() %>% 
  mutate(species_num = as.numeric(fct_rev(species))) 
url <- "https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/lter_penguins.png"
img <- png::readPNG(RCurl::getURLContent(url))
i2 <- grid::rasterGrob(img, interpolate = T)

pal <- c("#FF8C00", "#A034F0", "#159090")


ggplot(df_peng_iqr, aes(bill_ratio, species_num - .2)) +
  geom_rect(
    data = df_rect,
    aes(
      xmin = xmin, xmax = xmax,
      ymin = ymin, ymax = ymax
    ),
    inherit.aes = F,
    fill = "white"
  ) +
  geom_linerange(
    data = df_peng_iqr %>% 
      group_by(species, species_num) %>% 
      summarize(m = unique(median)),
    aes(
      xmin = -Inf, 
      xmax = m, 
      y = species_num,
      color = species
    ),
    inherit.aes = F,
    linetype = "dotted",
    size = .7
  ) +
  geom_boxplot(
    aes(
      color = species,
      color = after_scale(darken(color, .1, space = "HLS"))
    ),
    width = 0,
    size = .9
  ) +
  geom_rect(
    aes(
      xmin = q25,
      xmax = median,
      ymin = species_num - .05,
      ymax = species_num - .35
    ),
    fill = "grey89"
  ) +
  geom_rect(
    aes(
      xmin = q75,
      xmax = median,
      ymin = species_num - .05,
      ymax = species_num - .35
    ),
    fill = "grey79"
  ) +
  geom_segment(
    aes(
      x = q25, 
      xend = q25,
      y = species_num - .05,
      yend = species_num - .35,
      color = species,
      color = after_scale(darken(color, .05, space = "HLS"))
    ),
    size = .25
  ) +
  geom_segment(
    aes(
      x = q75, 
      xend = q75,
      y = species_num - .05,
      yend = species_num - .35,
      color = species,
      color = after_scale(darken(color, .05, space = "HLS"))
    ),
    size = .25
  ) +
  geom_point(
    aes(
      color = species
    ), 
    shape = "|",
    size = 5,
    alpha = .33
  ) +
  ggdist::stat_halfeye(
    aes(
      y = species_num,
      color = species,
      fill = after_scale(lighten(color, .5))
    ),
    shape = 18,
    point_size = 3,
    interval_size = 1.8,
    adjust = .5,
    .width = c(0, 1)
  ) +
  geom_text(
    data = df_peng_iqr %>% 
      group_by(species, species_num) %>% 
      summarize(m = unique(median)),
    aes(
      x = m, 
      y = species_num + .12,
      label = format(round(m, 2), nsmall = 2)
    ),
    inherit.aes = F,
    color = "white",
    family = "Neutraface Slab Display TT Titl",
    size = 3.5
  ) +
  geom_text(
    data = df_peng_iqr %>% 
      group_by(species, species_num) %>% 
      summarize(n = unique(n), max = max(bill_ratio, na.rm = T)),
    aes(
      x = max + .01, 
      y = species_num + .02,
      label = glue::glue("n = {n}"),
      color = species
    ),
    inherit.aes = F,
    family = "Neutraface Slab Display TT Bold",
    size = 3.5,
    hjust = 0
  ) +
  annotation_custom(i2, ymin = 2.5, ymax = 3.6, xmin = 3, xmax = 3.7) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(
    limits = c(1.57, 3.7),
    breaks = seq(1.6, 3.6, by = .2),
    expand = c(.001, .001)
  ) +
  scale_y_continuous(
    limits = c(.55, NA),
    breaks = 1:3,
    labels = c("Gentoo", "Chinstrap", "Ad√©lie"),
    expand = c(0, 0)
  ) +
  scale_color_manual(
    values = pal,
    guide = F
  ) +
  scale_fill_manual(
    values = pal,
    guide = F
  ) +
  labs(
    x = "Bill ratio",
    y = NULL,
    title = "Distribution of the bill ratio of Palmer penguin species",
    subtitle = "The following graph shows how bill ratio is distributed among Palmer penguin species.\nYou can see the overall distribution above the colored line and individuals below the colored line.\nDots represent outliers.",
    caption = 'Note: Bill ratio is estimated as bill length divided by bill depth.\nVisualization: Adapted from C√©dric Scherer\nData: 10.1371/journal.pone.0090081\nIllustrations: Allison Horst'
  )+
  theme_minimal()+
  theme(
    panel.grid.major.y = element_blank(),
    axis.text.y = element_text(family = "Arial", 
                               color = rev(pal), size = 14, lineheight = .9),
    axis.ticks.length = unit(0, "lines"),
    plot.title = element_text(color="black", size=14, face = "bold"),
    plot.subtitle = element_text(color="grey50", size=12)
  )
```

## Survey Items


Survey items consisted of first, middle, and final contributions of #TidyTuesday participants who agreed for us to use their graphics. Any graphics that were not relevant to #TidyTuesday were removed. In addition, when more than one graphic existed for any of the three time points, graphics were chosen based on those that required the least context to understand (e.g. did not use advanced statistical techniques such as odd ratios or sentiment analysis, did not contain undefined acronyms, etc.)

## Survey Questions

A number of assessments have been used to understand learners‚Äô data visualization interpretation abilities (e.g. ,McKenzie & Padilla, 1986; Boote, 2014; Lai et al., 2016). Here, our focus differs, in that we sought to measure not interpreters' ability, but, rather, data visualizations themselves. Accordingly, we focused on literature on the quality of visualizations and infographics, research more in the realm of Human-Computer Interaction (HCI) literature than in STEM education.

There are three main survey questions per image. Each question is meant to measure a different data visualizationn construct. Initial considerations for constructs came from Berland et al.'s (2015) epistemologies in practice framework, which was designed to serve as a measure of students' thinking about the nature of knowledge-related ideas that guided their science inquiry. This framework included dimensions for attention to audience, the inclusion of evidence, and the extent to which the model communicated how and why something in the world worked.

Further refinement of the constructs was also based on prior research. For example, Quispel and Maes (2014) had graphic design students rate visualizations on 1) attractiveness, 2) information retrieval, 3) clarity, and 4) an overall rating in addition to a selection task where participants selected three visualizations they most appreciated and then explained their selections. Quispel et al. (2016) surveyed design experts and "laymen" (non-design experts) on attractiveness, familiarity, and perceived ease of use. Locoro et al. (2017) include a higher number of measures in their survey of the perceptions and interaction with infographics. They measured sinteticity (informational quality), clarity, informativity, intuitivity, elegance, attractiveness, usability, and ease of use.

In developing the survey questions, we considered the overlapping constructs of these examples from prior visualization work and how they were related to the above epistemologies in practice (modeling) framework. For example, informativity was common in all of the studies, and was also a part of the modeling framework (for how and why something occurs). Clarity was a part of all of the studies, and was relfected in the modeling framework in the audience dimension. Finally, attractiveness was a part of all of the frameworks, and also pertained to the audience dimension of the modeling framework. In addition, we considered the need to create succinct survey questions in order to reduce survey fatigue and minimize unplanned missing data. 

Thus, we developed he following three survey constructs, each of which is rated on a 5-point scale from very poor (1) to very good (5):

  - Informativity - This element focuses on how well the graphic describes or explains something that is happening or has happened.
  - Clarity - This element focuses on how easy it is to read, understand, and interpret the visualization.
 - Attractiveness - This element focuses on how appealing and engaging the graphic is in terms of its overall use of shapes, color, theme, and layout.
 
# Survey Validation

## Content Validity

To test the validity of our survey, we conducted a content validity study (Rubio et al, 2003). We asked a group of 7 judges to rate the clarity and potential reliability of each survey construct as it was portrayed on the survey (see Appendix X). We also asked judges to give a holistic rating on the survey's ability to measure data visualization development. In addition, we collected open-ended feedback for each construct rating as well as the holistic rating. We then computed a content validity index (CVI) for each item, ranging from 0 to 1. The item CVI was calculated as the total number of raters who rated each item in its highest and second highest category (e.g. "very clear" and "needs minor revisions to be clear") divided by the total number of raters. Th

Only two of the seven items were rated less than 1. The reliability of the informativity construct was rated as .66, indicating major revisions were needed. The holistic rating of the survey was .83. Both construct definitions and rating scale category names were revised based on feedback.

## Convergent/Divergent Validity

## Reliability

### Item Analysis

### Factor Analysis

### D Study

# References

Berland, L. K., Schwarz, C. V., Krist, C., Kenyon, L., Lo, A. S., & Reiser, B. J. (2016). Epistemologies in practice: Making scientific practices meaningful for students. Journal of Research in Science Teaching, 53(7), 1082-1112.
Lai, K., Cabrera, J., Vitale, J. M., Madhok, J., Tinker, R., & Linn, M. C. (2016). Measuring graph comprehension, critique, and construction in science. Journal of Science Education and Technology, 25(4), 665-681.
Locoro, A., Cabitza, F., Actis-Grosso, R., & Batini, C. (2017). Static and interactive infographics in daily tasks: A value-in-use and quality of interaction user study. Computers in Human Behavior, 71, 240-257.
Quispel, A., & Maes, A. (2014). Would you prefer pie or cupcakes? Preferences for data visualization designs of professionals and laypeople in graphic design. Journal of Visual Languages & Computing, 25(2), 107-116.


